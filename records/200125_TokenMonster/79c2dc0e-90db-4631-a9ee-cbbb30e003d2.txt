import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)
flex_kernel_options = None
if torch.cuda.get_device_name(0).endswith(("3090", "4090")):
    flex_kernel_options = {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_M1": 32, "BLOCK_N1": 64, "BLOCK_M2": 64, "BLOCK_N2": 32}

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale, kernel_options=flex_kernel_options)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor = None, sliding_window_num_blocks: Tensor = 0):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 28415).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x) if not self.training else lm_head_fp8(x, self.lm_head.weight)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)

        if target_seq is None:
            return logits

        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb-tokmon-10B/english-28416-balanced/fineweb-tokmon_train_*.bin" # input .bin to train on
    val_files = "data/fineweb-tokmon-10B/english-28416-balanced/fineweb-tokmon_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # fewer tokens but equivalent text for validation, snapped to nearest seq_len
    val_ratio = 0.99011 # equivalent token density on validation tokens to that of GPT-2
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()


def main():
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

    model = GPT(vocab_size=28416, num_layers=12, num_heads=6, model_dim=768).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    k = 1.08
    adam_params = [dict(params=head_params, lr=0.008*k), dict(params=embed_params, lr=0.6*k), dict(params=scalar_params, lr=0.04*k)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05*k, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model)
    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss = (val_loss * args.val_ratio) / val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
    )
    dist.destroy_process_group()


if __name__ == "__main__":
    main()
====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu124 compiled for CUDA 12.4
Mon Jan 20 17:23:43 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   38C    P0            128W /  700W |    7714MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   41C    P0            121W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   35C    P0            116W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   43C    P0            127W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   40C    P0            122W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    3212MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     41419      C   /usr/bin/python3                             3394MiB |
|    0   N/A  N/A     41420      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     41421      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     41422      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     41423      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     41424      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     41425      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     41426      C   /usr/bin/python3                              610MiB |
|    1   N/A  N/A     41420      C   /usr/bin/python3                             3442MiB |
|    2   N/A  N/A     41421      C   /usr/bin/python3                             3442MiB |
|    3   N/A  N/A     41422      C   /usr/bin/python3                             3442MiB |
|    4   N/A  N/A     41423      C   /usr/bin/python3                             3442MiB |
|    5   N/A  N/A     41424      C   /usr/bin/python3                             3442MiB |
|    6   N/A  N/A     41425      C   /usr/bin/python3                             3442MiB |
|    7   N/A  N/A     41426      C   /usr/bin/python3                             3202MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.1533 train_time:0ms step_avg:nanms
step:1/1393 train_time:19841ms step_avg:nanms
step:2/1393 train_time:19878ms step_avg:nanms
step:3/1393 train_time:20366ms step_avg:nanms
step:4/1393 train_time:20477ms step_avg:nanms
step:5/1393 train_time:20590ms step_avg:nanms
step:6/1393 train_time:20702ms step_avg:nanms
step:7/1393 train_time:20815ms step_avg:nanms
step:8/1393 train_time:20927ms step_avg:nanms
step:9/1393 train_time:21040ms step_avg:nanms
step:10/1393 train_time:21153ms step_avg:nanms
step:11/1393 train_time:112ms step_avg:nanms
step:12/1393 train_time:226ms step_avg:nanms
step:13/1393 train_time:339ms step_avg:113.01ms
step:14/1393 train_time:452ms step_avg:113.04ms
step:15/1393 train_time:565ms step_avg:113.05ms
step:16/1393 train_time:679ms step_avg:113.10ms
step:17/1393 train_time:791ms step_avg:113.01ms
step:18/1393 train_time:904ms step_avg:112.96ms
step:19/1393 train_time:1016ms step_avg:112.92ms
step:20/1393 train_time:1129ms step_avg:112.93ms
step:21/1393 train_time:1243ms step_avg:113.00ms
step:22/1393 train_time:1356ms step_avg:113.04ms
step:23/1393 train_time:1470ms step_avg:113.04ms
step:24/1393 train_time:1584ms step_avg:113.13ms
step:25/1393 train_time:1698ms step_avg:113.18ms
step:26/1393 train_time:1812ms step_avg:113.24ms
step:27/1393 train_time:1925ms step_avg:113.26ms
step:28/1393 train_time:2039ms step_avg:113.26ms
step:29/1393 train_time:2152ms step_avg:113.24ms
step:30/1393 train_time:2264ms step_avg:113.20ms
step:31/1393 train_time:2377ms step_avg:113.20ms
step:32/1393 train_time:2490ms step_avg:113.18ms
step:33/1393 train_time:2604ms step_avg:113.20ms
step:34/1393 train_time:2717ms step_avg:113.19ms
step:35/1393 train_time:2829ms step_avg:113.16ms
step:36/1393 train_time:2942ms step_avg:113.15ms
step:37/1393 train_time:3055ms step_avg:113.13ms
step:38/1393 train_time:3169ms step_avg:113.16ms
step:39/1393 train_time:3283ms step_avg:113.21ms
step:40/1393 train_time:3396ms step_avg:113.18ms
step:41/1393 train_time:3509ms step_avg:113.18ms
step:42/1393 train_time:3621ms step_avg:113.16ms
step:43/1393 train_time:3734ms step_avg:113.15ms
step:44/1393 train_time:3847ms step_avg:113.15ms
step:45/1393 train_time:3961ms step_avg:113.16ms
step:46/1393 train_time:4074ms step_avg:113.18ms
step:47/1393 train_time:4187ms step_avg:113.17ms
step:48/1393 train_time:4301ms step_avg:113.19ms
step:49/1393 train_time:4415ms step_avg:113.20ms
step:50/1393 train_time:4528ms step_avg:113.20ms
step:51/1393 train_time:4642ms step_avg:113.21ms
step:52/1393 train_time:4754ms step_avg:113.20ms
step:53/1393 train_time:4868ms step_avg:113.21ms
step:54/1393 train_time:4981ms step_avg:113.22ms
step:55/1393 train_time:5094ms step_avg:113.21ms
step:56/1393 train_time:5207ms step_avg:113.20ms
step:57/1393 train_time:5321ms step_avg:113.21ms
step:58/1393 train_time:5434ms step_avg:113.20ms
step:59/1393 train_time:5547ms step_avg:113.21ms
step:60/1393 train_time:5660ms step_avg:113.21ms
step:61/1393 train_time:5774ms step_avg:113.21ms
step:62/1393 train_time:5887ms step_avg:113.20ms
step:63/1393 train_time:6000ms step_avg:113.20ms
step:64/1393 train_time:6113ms step_avg:113.20ms
step:65/1393 train_time:6226ms step_avg:113.21ms
step:66/1393 train_time:6339ms step_avg:113.20ms
step:67/1393 train_time:6452ms step_avg:113.20ms
step:68/1393 train_time:6566ms step_avg:113.20ms
step:69/1393 train_time:6679ms step_avg:113.20ms
step:70/1393 train_time:6793ms step_avg:113.22ms
step:71/1393 train_time:6906ms step_avg:113.22ms
step:72/1393 train_time:7019ms step_avg:113.21ms
step:73/1393 train_time:7132ms step_avg:113.20ms
step:74/1393 train_time:7245ms step_avg:113.21ms
step:75/1393 train_time:7359ms step_avg:113.21ms
step:76/1393 train_time:7472ms step_avg:113.21ms
step:77/1393 train_time:7584ms step_avg:113.20ms
step:78/1393 train_time:7697ms step_avg:113.19ms
step:79/1393 train_time:7810ms step_avg:113.19ms
step:80/1393 train_time:7924ms step_avg:113.19ms
step:81/1393 train_time:8036ms step_avg:113.19ms
step:82/1393 train_time:8149ms step_avg:113.19ms
step:83/1393 train_time:8263ms step_avg:113.19ms
step:84/1393 train_time:8376ms step_avg:113.19ms
step:85/1393 train_time:8489ms step_avg:113.19ms
step:86/1393 train_time:8602ms step_avg:113.18ms
step:87/1393 train_time:8715ms step_avg:113.18ms
step:88/1393 train_time:8828ms step_avg:113.18ms
step:89/1393 train_time:8941ms step_avg:113.18ms
step:90/1393 train_time:9055ms step_avg:113.18ms
step:91/1393 train_time:9168ms step_avg:113.18ms
step:92/1393 train_time:9281ms step_avg:113.19ms
step:93/1393 train_time:9395ms step_avg:113.20ms
step:94/1393 train_time:9508ms step_avg:113.19ms
step:95/1393 train_time:9621ms step_avg:113.19ms
step:96/1393 train_time:9734ms step_avg:113.19ms
step:97/1393 train_time:9847ms step_avg:113.19ms
step:98/1393 train_time:9960ms step_avg:113.19ms
step:99/1393 train_time:10073ms step_avg:113.18ms
step:100/1393 train_time:10186ms step_avg:113.18ms
step:101/1393 train_time:10299ms step_avg:113.17ms
step:102/1393 train_time:10412ms step_avg:113.17ms
step:103/1393 train_time:10526ms step_avg:113.19ms
step:104/1393 train_time:10639ms step_avg:113.18ms
step:105/1393 train_time:10752ms step_avg:113.18ms
step:106/1393 train_time:10866ms step_avg:113.19ms
step:107/1393 train_time:10980ms step_avg:113.19ms
step:108/1393 train_time:11093ms step_avg:113.20ms
step:109/1393 train_time:11207ms step_avg:113.21ms
step:110/1393 train_time:11321ms step_avg:113.21ms
step:111/1393 train_time:11434ms step_avg:113.21ms
step:112/1393 train_time:11549ms step_avg:113.22ms
step:113/1393 train_time:11662ms step_avg:113.22ms
step:114/1393 train_time:11775ms step_avg:113.22ms
step:115/1393 train_time:11889ms step_avg:113.23ms
step:116/1393 train_time:12003ms step_avg:113.23ms
step:117/1393 train_time:12116ms step_avg:113.24ms
step:118/1393 train_time:12230ms step_avg:113.24ms
step:119/1393 train_time:12344ms step_avg:113.24ms
step:120/1393 train_time:12457ms step_avg:113.25ms
step:121/1393 train_time:12571ms step_avg:113.25ms
step:122/1393 train_time:12684ms step_avg:113.25ms
step:123/1393 train_time:12798ms step_avg:113.25ms
step:124/1393 train_time:12911ms step_avg:113.25ms
step:125/1393 train_time:13024ms step_avg:113.26ms
step:125/1393 val_loss:4.3324 train_time:13137ms step_avg:114.24ms
step:126/1393 train_time:13162ms step_avg:113.46ms
step:127/1393 train_time:13253ms step_avg:113.28ms
step:128/1393 train_time:13374ms step_avg:113.34ms
step:129/1393 train_time:13490ms step_avg:113.37ms
step:130/1393 train_time:13604ms step_avg:113.37ms
step:131/1393 train_time:13717ms step_avg:113.37ms
step:132/1393 train_time:13831ms step_avg:113.37ms
step:133/1393 train_time:13944ms step_avg:113.37ms
step:134/1393 train_time:14057ms step_avg:113.37ms
step:135/1393 train_time:14171ms step_avg:113.37ms
step:136/1393 train_time:14286ms step_avg:113.38ms
step:137/1393 train_time:14399ms step_avg:113.38ms
step:138/1393 train_time:14513ms step_avg:113.38ms
step:139/1393 train_time:14627ms step_avg:113.38ms
step:140/1393 train_time:14741ms step_avg:113.39ms
step:141/1393 train_time:14855ms step_avg:113.39ms
step:142/1393 train_time:14968ms step_avg:113.39ms
step:143/1393 train_time:15082ms step_avg:113.40ms
step:144/1393 train_time:15196ms step_avg:113.40ms
step:145/1393 train_time:15309ms step_avg:113.40ms
step:146/1393 train_time:15423ms step_avg:113.41ms
step:147/1393 train_time:15537ms step_avg:113.41ms
step:148/1393 train_time:15650ms step_avg:113.41ms
step:149/1393 train_time:15764ms step_avg:113.41ms
step:150/1393 train_time:15878ms step_avg:113.41ms
step:151/1393 train_time:15992ms step_avg:113.42ms
step:152/1393 train_time:16106ms step_avg:113.43ms
step:153/1393 train_time:16221ms step_avg:113.43ms
step:154/1393 train_time:16334ms step_avg:113.43ms
step:155/1393 train_time:16447ms step_avg:113.43ms
step:156/1393 train_time:16562ms step_avg:113.44ms
step:157/1393 train_time:16675ms step_avg:113.44ms
step:158/1393 train_time:16790ms step_avg:113.44ms
step:159/1393 train_time:16904ms step_avg:113.45ms
step:160/1393 train_time:17018ms step_avg:113.45ms
step:161/1393 train_time:17131ms step_avg:113.45ms
step:162/1393 train_time:17245ms step_avg:113.45ms
step:163/1393 train_time:17358ms step_avg:113.45ms
step:164/1393 train_time:17473ms step_avg:113.46ms
step:165/1393 train_time:17587ms step_avg:113.47ms
step:166/1393 train_time:17701ms step_avg:113.47ms
step:167/1393 train_time:17814ms step_avg:113.47ms
step:168/1393 train_time:17927ms step_avg:113.47ms
step:169/1393 train_time:18042ms step_avg:113.47ms
step:170/1393 train_time:18156ms step_avg:113.48ms
step:171/1393 train_time:18270ms step_avg:113.48ms
step:172/1393 train_time:18384ms step_avg:113.48ms
step:173/1393 train_time:18497ms step_avg:113.48ms
step:174/1393 train_time:18611ms step_avg:113.48ms
step:175/1393 train_time:18726ms step_avg:113.49ms
step:176/1393 train_time:18840ms step_avg:113.49ms
step:177/1393 train_time:18953ms step_avg:113.49ms
step:178/1393 train_time:19067ms step_avg:113.49ms
step:179/1393 train_time:19180ms step_avg:113.49ms
step:180/1393 train_time:19294ms step_avg:113.49ms
step:181/1393 train_time:19407ms step_avg:113.49ms
step:182/1393 train_time:19523ms step_avg:113.50ms
step:183/1393 train_time:19637ms step_avg:113.51ms
step:184/1393 train_time:19750ms step_avg:113.51ms
step:185/1393 train_time:19863ms step_avg:113.51ms
step:186/1393 train_time:19977ms step_avg:113.51ms
step:187/1393 train_time:20092ms step_avg:113.51ms
step:188/1393 train_time:20205ms step_avg:113.51ms
step:189/1393 train_time:20320ms step_avg:113.52ms
step:190/1393 train_time:20434ms step_avg:113.52ms
step:191/1393 train_time:20548ms step_avg:113.53ms
step:192/1393 train_time:20662ms step_avg:113.53ms
step:193/1393 train_time:20776ms step_avg:113.53ms
step:194/1393 train_time:20890ms step_avg:113.53ms
step:195/1393 train_time:21004ms step_avg:113.53ms
step:196/1393 train_time:21119ms step_avg:113.54ms
step:197/1393 train_time:21233ms step_avg:113.54ms
step:198/1393 train_time:21347ms step_avg:113.55ms
step:199/1393 train_time:21460ms step_avg:113.55ms
step:200/1393 train_time:21574ms step_avg:113.55ms
step:201/1393 train_time:21688ms step_avg:113.55ms
step:202/1393 train_time:21801ms step_avg:113.55ms
step:203/1393 train_time:21915ms step_avg:113.55ms
step:204/1393 train_time:22029ms step_avg:113.55ms
step:205/1393 train_time:22143ms step_avg:113.55ms
step:206/1393 train_time:22258ms step_avg:113.56ms
step:207/1393 train_time:22371ms step_avg:113.56ms
step:208/1393 train_time:22485ms step_avg:113.56ms
step:209/1393 train_time:22599ms step_avg:113.56ms
step:210/1393 train_time:22713ms step_avg:113.57ms
step:211/1393 train_time:22827ms step_avg:113.57ms
step:212/1393 train_time:22941ms step_avg:113.57ms
step:213/1393 train_time:23055ms step_avg:113.57ms
step:214/1393 train_time:23170ms step_avg:113.58ms
step:215/1393 train_time:23285ms step_avg:113.59ms
step:216/1393 train_time:23400ms step_avg:113.59ms
step:217/1393 train_time:23514ms step_avg:113.59ms
step:218/1393 train_time:23628ms step_avg:113.59ms
step:219/1393 train_time:23742ms step_avg:113.60ms
step:220/1393 train_time:23857ms step_avg:113.60ms
step:221/1393 train_time:23971ms step_avg:113.61ms
step:222/1393 train_time:24086ms step_avg:113.61ms
step:223/1393 train_time:24201ms step_avg:113.62ms
step:224/1393 train_time:24315ms step_avg:113.62ms
step:225/1393 train_time:24429ms step_avg:113.62ms
step:226/1393 train_time:24543ms step_avg:113.63ms
step:227/1393 train_time:24657ms step_avg:113.63ms
step:228/1393 train_time:24771ms step_avg:113.63ms
step:229/1393 train_time:24885ms step_avg:113.63ms
step:230/1393 train_time:25000ms step_avg:113.64ms
step:231/1393 train_time:25114ms step_avg:113.64ms
step:232/1393 train_time:25228ms step_avg:113.64ms
step:233/1393 train_time:25342ms step_avg:113.64ms
step:234/1393 train_time:25457ms step_avg:113.65ms
step:235/1393 train_time:25571ms step_avg:113.65ms
step:236/1393 train_time:25687ms step_avg:113.66ms
step:237/1393 train_time:25801ms step_avg:113.66ms
step:238/1393 train_time:25915ms step_avg:113.66ms
step:239/1393 train_time:26029ms step_avg:113.67ms
step:240/1393 train_time:26144ms step_avg:113.67ms
step:241/1393 train_time:26258ms step_avg:113.67ms
step:242/1393 train_time:26374ms step_avg:113.68ms
step:243/1393 train_time:26488ms step_avg:113.68ms
step:244/1393 train_time:26603ms step_avg:113.69ms
step:245/1393 train_time:26716ms step_avg:113.69ms
step:246/1393 train_time:26830ms step_avg:113.69ms
step:247/1393 train_time:26944ms step_avg:113.69ms
step:248/1393 train_time:27058ms step_avg:113.69ms
step:249/1393 train_time:27172ms step_avg:113.69ms
step:250/1393 train_time:27287ms step_avg:113.70ms
step:250/1393 val_loss:3.9601 train_time:27401ms step_avg:114.17ms
step:251/1393 train_time:27425ms step_avg:113.80ms
step:252/1393 train_time:27519ms step_avg:113.72ms
step:253/1393 train_time:27643ms step_avg:113.76ms
step:254/1393 train_time:27760ms step_avg:113.77ms
step:255/1393 train_time:27875ms step_avg:113.77ms
step:256/1393 train_time:27989ms step_avg:113.78ms
step:257/1393 train_time:28104ms step_avg:113.78ms
step:258/1393 train_time:28217ms step_avg:113.78ms
step:259/1393 train_time:28332ms step_avg:113.78ms
step:260/1393 train_time:28446ms step_avg:113.78ms
step:261/1393 train_time:28560ms step_avg:113.78ms
step:262/1393 train_time:28674ms step_avg:113.79ms
step:263/1393 train_time:28790ms step_avg:113.80ms
step:264/1393 train_time:28906ms step_avg:113.80ms
step:265/1393 train_time:29020ms step_avg:113.80ms
step:266/1393 train_time:29135ms step_avg:113.81ms
step:267/1393 train_time:29249ms step_avg:113.81ms
step:268/1393 train_time:29363ms step_avg:113.81ms
step:269/1393 train_time:29477ms step_avg:113.81ms
step:270/1393 train_time:29591ms step_avg:113.81ms
step:271/1393 train_time:29705ms step_avg:113.81ms
step:272/1393 train_time:29819ms step_avg:113.81ms
step:273/1393 train_time:29934ms step_avg:113.82ms
step:274/1393 train_time:30048ms step_avg:113.82ms
step:275/1393 train_time:30162ms step_avg:113.82ms
step:276/1393 train_time:30276ms step_avg:113.82ms
step:277/1393 train_time:30390ms step_avg:113.82ms
step:278/1393 train_time:30504ms step_avg:113.82ms
step:279/1393 train_time:30619ms step_avg:113.82ms
step:280/1393 train_time:30733ms step_avg:113.83ms
step:281/1393 train_time:30847ms step_avg:113.83ms
step:282/1393 train_time:30961ms step_avg:113.83ms
step:283/1393 train_time:31075ms step_avg:113.83ms
step:284/1393 train_time:31189ms step_avg:113.83ms
step:285/1393 train_time:31303ms step_avg:113.83ms
step:286/1393 train_time:31418ms step_avg:113.83ms
step:287/1393 train_time:31531ms step_avg:113.83ms
step:288/1393 train_time:31645ms step_avg:113.83ms
step:289/1393 train_time:31759ms step_avg:113.83ms
step:290/1393 train_time:31874ms step_avg:113.83ms
step:291/1393 train_time:31988ms step_avg:113.84ms
step:292/1393 train_time:32103ms step_avg:113.84ms
step:293/1393 train_time:32217ms step_avg:113.84ms
step:294/1393 train_time:32332ms step_avg:113.84ms
step:295/1393 train_time:32446ms step_avg:113.84ms
step:296/1393 train_time:32559ms step_avg:113.84ms
step:297/1393 train_time:32674ms step_avg:113.85ms
step:298/1393 train_time:32789ms step_avg:113.85ms
step:299/1393 train_time:32903ms step_avg:113.85ms
step:300/1393 train_time:33017ms step_avg:113.85ms
step:301/1393 train_time:33131ms step_avg:113.85ms
step:302/1393 train_time:33246ms step_avg:113.85ms
step:303/1393 train_time:33359ms step_avg:113.85ms
step:304/1393 train_time:33474ms step_avg:113.86ms
step:305/1393 train_time:33588ms step_avg:113.86ms
step:306/1393 train_time:33702ms step_avg:113.86ms
step:307/1393 train_time:33817ms step_avg:113.86ms
step:308/1393 train_time:33931ms step_avg:113.86ms
step:309/1393 train_time:34045ms step_avg:113.86ms
step:310/1393 train_time:34159ms step_avg:113.86ms
step:311/1393 train_time:34273ms step_avg:113.86ms
step:312/1393 train_time:34390ms step_avg:113.87ms
step:313/1393 train_time:34507ms step_avg:113.88ms
step:314/1393 train_time:34624ms step_avg:113.89ms
step:315/1393 train_time:34740ms step_avg:113.90ms
step:316/1393 train_time:34856ms step_avg:113.91ms
step:317/1393 train_time:34974ms step_avg:113.92ms
step:318/1393 train_time:35092ms step_avg:113.93ms
step:319/1393 train_time:35208ms step_avg:113.94ms
step:320/1393 train_time:35325ms step_avg:113.95ms
step:321/1393 train_time:35441ms step_avg:113.96ms
step:322/1393 train_time:35558ms step_avg:113.97ms
step:323/1393 train_time:35675ms step_avg:113.98ms
step:324/1393 train_time:35791ms step_avg:113.98ms
step:325/1393 train_time:35908ms step_avg:113.99ms
step:326/1393 train_time:36025ms step_avg:114.00ms
step:327/1393 train_time:36142ms step_avg:114.01ms
step:328/1393 train_time:36260ms step_avg:114.03ms
step:329/1393 train_time:36377ms step_avg:114.03ms
step:330/1393 train_time:36494ms step_avg:114.05ms
step:331/1393 train_time:36611ms step_avg:114.05ms
step:332/1393 train_time:36728ms step_avg:114.06ms
step:333/1393 train_time:36844ms step_avg:114.07ms
step:334/1393 train_time:36962ms step_avg:114.08ms
step:335/1393 train_time:37078ms step_avg:114.09ms
step:336/1393 train_time:37196ms step_avg:114.10ms
step:337/1393 train_time:37313ms step_avg:114.11ms
step:338/1393 train_time:37429ms step_avg:114.11ms
step:339/1393 train_time:37546ms step_avg:114.12ms
step:340/1393 train_time:37663ms step_avg:114.13ms
step:341/1393 train_time:37779ms step_avg:114.14ms
step:342/1393 train_time:37896ms step_avg:114.14ms
step:343/1393 train_time:38013ms step_avg:114.15ms
step:344/1393 train_time:38130ms step_avg:114.16ms
step:345/1393 train_time:38248ms step_avg:114.17ms
step:346/1393 train_time:38365ms step_avg:114.18ms
step:347/1393 train_time:38482ms step_avg:114.19ms
step:348/1393 train_time:38599ms step_avg:114.20ms
step:349/1393 train_time:38716ms step_avg:114.21ms
step:350/1393 train_time:38833ms step_avg:114.21ms
step:351/1393 train_time:38950ms step_avg:114.22ms
step:352/1393 train_time:39066ms step_avg:114.23ms
step:353/1393 train_time:39184ms step_avg:114.24ms
step:354/1393 train_time:39300ms step_avg:114.24ms
step:355/1393 train_time:39417ms step_avg:114.25ms
step:356/1393 train_time:39535ms step_avg:114.26ms
step:357/1393 train_time:39653ms step_avg:114.27ms
step:358/1393 train_time:39770ms step_avg:114.28ms
step:359/1393 train_time:39887ms step_avg:114.29ms
step:360/1393 train_time:40004ms step_avg:114.30ms
step:361/1393 train_time:40121ms step_avg:114.30ms
step:362/1393 train_time:40237ms step_avg:114.31ms
step:363/1393 train_time:40354ms step_avg:114.32ms
step:364/1393 train_time:40470ms step_avg:114.32ms
step:365/1393 train_time:40588ms step_avg:114.33ms
step:366/1393 train_time:40704ms step_avg:114.34ms
step:367/1393 train_time:40821ms step_avg:114.35ms
step:368/1393 train_time:40939ms step_avg:114.36ms
step:369/1393 train_time:41056ms step_avg:114.36ms
step:370/1393 train_time:41173ms step_avg:114.37ms
step:371/1393 train_time:41291ms step_avg:114.38ms
step:372/1393 train_time:41407ms step_avg:114.38ms
step:373/1393 train_time:41525ms step_avg:114.39ms
step:374/1393 train_time:41641ms step_avg:114.40ms
step:375/1393 train_time:41758ms step_avg:114.41ms
step:375/1393 val_loss:3.7609 train_time:41874ms step_avg:114.72ms
step:376/1393 train_time:41897ms step_avg:114.47ms
step:377/1393 train_time:41993ms step_avg:114.42ms
step:378/1393 train_time:42121ms step_avg:114.46ms
step:379/1393 train_time:42240ms step_avg:114.47ms
step:380/1393 train_time:42357ms step_avg:114.48ms
step:381/1393 train_time:42474ms step_avg:114.49ms
step:382/1393 train_time:42590ms step_avg:114.49ms
step:383/1393 train_time:42707ms step_avg:114.50ms
step:384/1393 train_time:42824ms step_avg:114.50ms
step:385/1393 train_time:42941ms step_avg:114.51ms
step:386/1393 train_time:43058ms step_avg:114.52ms
step:387/1393 train_time:43175ms step_avg:114.52ms
step:388/1393 train_time:43292ms step_avg:114.53ms
step:389/1393 train_time:43409ms step_avg:114.54ms
step:390/1393 train_time:43527ms step_avg:114.54ms
step:391/1393 train_time:43644ms step_avg:114.55ms
step:392/1393 train_time:43761ms step_avg:114.56ms
step:393/1393 train_time:43878ms step_avg:114.56ms
step:394/1393 train_time:43994ms step_avg:114.57ms
step:395/1393 train_time:44111ms step_avg:114.57ms
step:396/1393 train_time:44228ms step_avg:114.58ms
step:397/1393 train_time:44346ms step_avg:114.59ms
step:398/1393 train_time:44463ms step_avg:114.60ms
step:399/1393 train_time:44580ms step_avg:114.60ms
step:400/1393 train_time:44697ms step_avg:114.61ms
step:401/1393 train_time:44814ms step_avg:114.61ms
step:402/1393 train_time:44931ms step_avg:114.62ms
step:403/1393 train_time:45048ms step_avg:114.63ms
step:404/1393 train_time:45164ms step_avg:114.63ms
step:405/1393 train_time:45282ms step_avg:114.64ms
step:406/1393 train_time:45399ms step_avg:114.64ms
step:407/1393 train_time:45516ms step_avg:114.65ms
step:408/1393 train_time:45633ms step_avg:114.66ms
step:409/1393 train_time:45750ms step_avg:114.66ms
step:410/1393 train_time:45866ms step_avg:114.67ms
step:411/1393 train_time:45983ms step_avg:114.67ms
step:412/1393 train_time:46101ms step_avg:114.68ms
step:413/1393 train_time:46218ms step_avg:114.69ms
step:414/1393 train_time:46335ms step_avg:114.69ms
step:415/1393 train_time:46452ms step_avg:114.70ms
step:416/1393 train_time:46570ms step_avg:114.70ms
step:417/1393 train_time:46687ms step_avg:114.71ms
step:418/1393 train_time:46804ms step_avg:114.72ms
step:419/1393 train_time:46922ms step_avg:114.72ms
step:420/1393 train_time:47039ms step_avg:114.73ms
step:421/1393 train_time:47156ms step_avg:114.73ms
step:422/1393 train_time:47273ms step_avg:114.74ms
step:423/1393 train_time:47390ms step_avg:114.75ms
step:424/1393 train_time:47508ms step_avg:114.75ms
step:425/1393 train_time:47626ms step_avg:114.76ms
step:426/1393 train_time:47743ms step_avg:114.77ms
step:427/1393 train_time:47860ms step_avg:114.77ms
step:428/1393 train_time:47978ms step_avg:114.78ms
step:429/1393 train_time:48095ms step_avg:114.79ms
step:430/1393 train_time:48212ms step_avg:114.79ms
step:431/1393 train_time:48330ms step_avg:114.80ms
step:432/1393 train_time:48447ms step_avg:114.80ms
step:433/1393 train_time:48566ms step_avg:114.81ms
step:434/1393 train_time:48683ms step_avg:114.82ms
step:435/1393 train_time:48801ms step_avg:114.83ms
step:436/1393 train_time:48918ms step_avg:114.83ms
step:437/1393 train_time:49035ms step_avg:114.84ms
step:438/1393 train_time:49152ms step_avg:114.84ms
step:439/1393 train_time:49270ms step_avg:114.85ms
step:440/1393 train_time:49387ms step_avg:114.85ms
step:441/1393 train_time:49505ms step_avg:114.86ms
step:442/1393 train_time:49623ms step_avg:114.87ms
step:443/1393 train_time:49740ms step_avg:114.87ms
step:444/1393 train_time:49858ms step_avg:114.88ms
step:445/1393 train_time:49975ms step_avg:114.89ms
step:446/1393 train_time:50092ms step_avg:114.89ms
step:447/1393 train_time:50209ms step_avg:114.89ms
step:448/1393 train_time:50327ms step_avg:114.90ms
step:449/1393 train_time:50445ms step_avg:114.91ms
step:450/1393 train_time:50562ms step_avg:114.91ms
step:451/1393 train_time:50680ms step_avg:114.92ms
step:452/1393 train_time:50798ms step_avg:114.93ms
step:453/1393 train_time:50916ms step_avg:114.93ms
step:454/1393 train_time:51033ms step_avg:114.94ms
step:455/1393 train_time:51150ms step_avg:114.94ms
step:456/1393 train_time:51267ms step_avg:114.95ms
step:457/1393 train_time:51385ms step_avg:114.96ms
step:458/1393 train_time:51503ms step_avg:114.96ms
step:459/1393 train_time:51621ms step_avg:114.97ms
step:460/1393 train_time:51738ms step_avg:114.97ms
step:461/1393 train_time:51856ms step_avg:114.98ms
step:462/1393 train_time:51973ms step_avg:114.98ms
step:463/1393 train_time:52090ms step_avg:114.99ms
step:464/1393 train_time:52209ms step_avg:115.00ms
step:465/1393 train_time:52326ms step_avg:115.00ms
step:466/1393 train_time:52444ms step_avg:115.01ms
step:467/1393 train_time:52562ms step_avg:115.02ms
step:468/1393 train_time:52679ms step_avg:115.02ms
step:469/1393 train_time:52796ms step_avg:115.02ms
step:470/1393 train_time:52913ms step_avg:115.03ms
step:471/1393 train_time:53031ms step_avg:115.03ms
step:472/1393 train_time:53149ms step_avg:115.04ms
step:473/1393 train_time:53266ms step_avg:115.05ms
step:474/1393 train_time:53385ms step_avg:115.05ms
step:475/1393 train_time:53503ms step_avg:115.06ms
step:476/1393 train_time:53621ms step_avg:115.07ms
step:477/1393 train_time:53739ms step_avg:115.07ms
step:478/1393 train_time:53856ms step_avg:115.08ms
step:479/1393 train_time:53974ms step_avg:115.08ms
step:480/1393 train_time:54091ms step_avg:115.09ms
step:481/1393 train_time:54209ms step_avg:115.09ms
step:482/1393 train_time:54327ms step_avg:115.10ms
step:483/1393 train_time:54444ms step_avg:115.10ms
step:484/1393 train_time:54563ms step_avg:115.11ms
step:485/1393 train_time:54681ms step_avg:115.12ms
step:486/1393 train_time:54798ms step_avg:115.12ms
step:487/1393 train_time:54916ms step_avg:115.13ms
step:488/1393 train_time:55033ms step_avg:115.13ms
step:489/1393 train_time:55150ms step_avg:115.14ms
step:490/1393 train_time:55268ms step_avg:115.14ms
step:491/1393 train_time:55384ms step_avg:115.14ms
step:492/1393 train_time:55503ms step_avg:115.15ms
step:493/1393 train_time:55621ms step_avg:115.16ms
step:494/1393 train_time:55738ms step_avg:115.16ms
step:495/1393 train_time:55856ms step_avg:115.17ms
step:496/1393 train_time:55974ms step_avg:115.17ms
step:497/1393 train_time:56091ms step_avg:115.18ms
step:498/1393 train_time:56209ms step_avg:115.18ms
step:499/1393 train_time:56327ms step_avg:115.19ms
step:500/1393 train_time:56445ms step_avg:115.19ms
step:500/1393 val_loss:3.6488 train_time:56561ms step_avg:115.43ms
step:501/1393 train_time:56585ms step_avg:115.24ms
step:502/1393 train_time:56681ms step_avg:115.21ms
step:503/1393 train_time:56806ms step_avg:115.23ms
step:504/1393 train_time:56927ms step_avg:115.24ms
step:505/1393 train_time:57045ms step_avg:115.24ms
step:506/1393 train_time:57162ms step_avg:115.25ms
step:507/1393 train_time:57280ms step_avg:115.25ms
step:508/1393 train_time:57397ms step_avg:115.25ms
step:509/1393 train_time:57514ms step_avg:115.26ms
step:510/1393 train_time:57631ms step_avg:115.26ms
step:511/1393 train_time:57749ms step_avg:115.27ms
step:512/1393 train_time:57866ms step_avg:115.27ms
step:513/1393 train_time:57983ms step_avg:115.27ms
step:514/1393 train_time:58100ms step_avg:115.28ms
step:515/1393 train_time:58217ms step_avg:115.28ms
step:516/1393 train_time:58334ms step_avg:115.29ms
step:517/1393 train_time:58452ms step_avg:115.29ms
step:518/1393 train_time:58572ms step_avg:115.30ms
step:519/1393 train_time:58692ms step_avg:115.31ms
step:520/1393 train_time:58812ms step_avg:115.32ms
step:521/1393 train_time:58931ms step_avg:115.32ms
step:522/1393 train_time:59051ms step_avg:115.33ms
step:523/1393 train_time:59170ms step_avg:115.34ms
step:524/1393 train_time:59289ms step_avg:115.35ms
step:525/1393 train_time:59408ms step_avg:115.35ms
step:526/1393 train_time:59527ms step_avg:115.36ms
step:527/1393 train_time:59646ms step_avg:115.37ms
step:528/1393 train_time:59765ms step_avg:115.38ms
step:529/1393 train_time:59885ms step_avg:115.39ms
step:530/1393 train_time:60004ms step_avg:115.39ms
step:531/1393 train_time:60124ms step_avg:115.40ms
step:532/1393 train_time:60244ms step_avg:115.41ms
step:533/1393 train_time:60364ms step_avg:115.42ms
step:534/1393 train_time:60483ms step_avg:115.43ms
step:535/1393 train_time:60604ms step_avg:115.44ms
step:536/1393 train_time:60723ms step_avg:115.44ms
step:537/1393 train_time:60843ms step_avg:115.45ms
step:538/1393 train_time:60963ms step_avg:115.46ms
step:539/1393 train_time:61083ms step_avg:115.47ms
step:540/1393 train_time:61202ms step_avg:115.48ms
step:541/1393 train_time:61321ms step_avg:115.48ms
step:542/1393 train_time:61441ms step_avg:115.49ms
step:543/1393 train_time:61562ms step_avg:115.50ms
step:544/1393 train_time:61680ms step_avg:115.51ms
step:545/1393 train_time:61800ms step_avg:115.51ms
step:546/1393 train_time:61919ms step_avg:115.52ms
step:547/1393 train_time:62038ms step_avg:115.53ms
step:548/1393 train_time:62158ms step_avg:115.53ms
step:549/1393 train_time:62277ms step_avg:115.54ms
step:550/1393 train_time:62397ms step_avg:115.55ms
step:551/1393 train_time:62516ms step_avg:115.56ms
step:552/1393 train_time:62636ms step_avg:115.56ms
step:553/1393 train_time:62755ms step_avg:115.57ms
step:554/1393 train_time:62875ms step_avg:115.58ms
step:555/1393 train_time:62995ms step_avg:115.59ms
step:556/1393 train_time:63115ms step_avg:115.60ms
step:557/1393 train_time:63234ms step_avg:115.60ms
step:558/1393 train_time:63353ms step_avg:115.61ms
step:559/1393 train_time:63473ms step_avg:115.62ms
step:560/1393 train_time:63592ms step_avg:115.62ms
step:561/1393 train_time:63711ms step_avg:115.63ms
step:562/1393 train_time:63830ms step_avg:115.63ms
step:563/1393 train_time:63951ms step_avg:115.64ms
step:564/1393 train_time:64071ms step_avg:115.65ms
step:565/1393 train_time:64190ms step_avg:115.66ms
step:566/1393 train_time:64310ms step_avg:115.66ms
step:567/1393 train_time:64429ms step_avg:115.67ms
step:568/1393 train_time:64548ms step_avg:115.68ms
step:569/1393 train_time:64667ms step_avg:115.68ms
step:570/1393 train_time:64786ms step_avg:115.69ms
step:571/1393 train_time:64905ms step_avg:115.70ms
step:572/1393 train_time:65025ms step_avg:115.70ms
step:573/1393 train_time:65145ms step_avg:115.71ms
step:574/1393 train_time:65265ms step_avg:115.72ms
step:575/1393 train_time:65385ms step_avg:115.73ms
step:576/1393 train_time:65504ms step_avg:115.73ms
step:577/1393 train_time:65624ms step_avg:115.74ms
step:578/1393 train_time:65744ms step_avg:115.75ms
step:579/1393 train_time:65863ms step_avg:115.75ms
step:580/1393 train_time:65982ms step_avg:115.76ms
step:581/1393 train_time:66102ms step_avg:115.77ms
step:582/1393 train_time:66223ms step_avg:115.78ms
step:583/1393 train_time:66344ms step_avg:115.78ms
step:584/1393 train_time:66464ms step_avg:115.79ms
step:585/1393 train_time:66584ms step_avg:115.80ms
step:586/1393 train_time:66704ms step_avg:115.81ms
step:587/1393 train_time:66824ms step_avg:115.81ms
step:588/1393 train_time:66943ms step_avg:115.82ms
step:589/1393 train_time:67062ms step_avg:115.82ms
step:590/1393 train_time:67181ms step_avg:115.83ms
step:591/1393 train_time:67300ms step_avg:115.83ms
step:592/1393 train_time:67419ms step_avg:115.84ms
step:593/1393 train_time:67540ms step_avg:115.85ms
step:594/1393 train_time:67661ms step_avg:115.86ms
step:595/1393 train_time:67780ms step_avg:115.86ms
step:596/1393 train_time:67902ms step_avg:115.87ms
step:597/1393 train_time:68021ms step_avg:115.88ms
step:598/1393 train_time:68142ms step_avg:115.89ms
step:599/1393 train_time:68261ms step_avg:115.89ms
step:600/1393 train_time:68380ms step_avg:115.90ms
step:601/1393 train_time:68499ms step_avg:115.90ms
step:602/1393 train_time:68618ms step_avg:115.91ms
step:603/1393 train_time:68738ms step_avg:115.92ms
step:604/1393 train_time:68857ms step_avg:115.92ms
step:605/1393 train_time:68977ms step_avg:115.93ms
step:606/1393 train_time:69096ms step_avg:115.93ms
step:607/1393 train_time:69217ms step_avg:115.94ms
step:608/1393 train_time:69337ms step_avg:115.95ms
step:609/1393 train_time:69456ms step_avg:115.95ms
step:610/1393 train_time:69576ms step_avg:115.96ms
step:611/1393 train_time:69696ms step_avg:115.97ms
step:612/1393 train_time:69815ms step_avg:115.97ms
step:613/1393 train_time:69935ms step_avg:115.98ms
step:614/1393 train_time:70055ms step_avg:115.98ms
step:615/1393 train_time:70176ms step_avg:115.99ms
step:616/1393 train_time:70296ms step_avg:116.00ms
step:617/1393 train_time:70415ms step_avg:116.01ms
step:618/1393 train_time:70535ms step_avg:116.01ms
step:619/1393 train_time:70655ms step_avg:116.02ms
step:620/1393 train_time:70774ms step_avg:116.02ms
step:621/1393 train_time:70894ms step_avg:116.03ms
step:622/1393 train_time:71013ms step_avg:116.03ms
step:623/1393 train_time:71133ms step_avg:116.04ms
step:624/1393 train_time:71252ms step_avg:116.05ms
step:625/1393 train_time:71372ms step_avg:116.05ms
step:625/1393 val_loss:3.5657 train_time:71491ms step_avg:116.25ms
step:626/1393 train_time:71516ms step_avg:116.10ms
step:627/1393 train_time:71614ms step_avg:116.07ms
step:628/1393 train_time:71744ms step_avg:116.09ms
step:629/1393 train_time:71865ms step_avg:116.10ms
step:630/1393 train_time:71985ms step_avg:116.11ms
step:631/1393 train_time:72105ms step_avg:116.11ms
step:632/1393 train_time:72224ms step_avg:116.12ms
step:633/1393 train_time:72343ms step_avg:116.12ms
step:634/1393 train_time:72463ms step_avg:116.13ms
step:635/1393 train_time:72582ms step_avg:116.13ms
step:636/1393 train_time:72702ms step_avg:116.14ms
step:637/1393 train_time:72822ms step_avg:116.14ms
step:638/1393 train_time:72942ms step_avg:116.15ms
step:639/1393 train_time:73062ms step_avg:116.16ms
step:640/1393 train_time:73182ms step_avg:116.16ms
step:641/1393 train_time:73303ms step_avg:116.17ms
step:642/1393 train_time:73423ms step_avg:116.18ms
step:643/1393 train_time:73543ms step_avg:116.18ms
step:644/1393 train_time:73662ms step_avg:116.19ms
step:645/1393 train_time:73783ms step_avg:116.19ms
step:646/1393 train_time:73902ms step_avg:116.20ms
step:647/1393 train_time:74023ms step_avg:116.21ms
step:648/1393 train_time:74143ms step_avg:116.21ms
step:649/1393 train_time:74263ms step_avg:116.22ms
step:650/1393 train_time:74383ms step_avg:116.22ms
step:651/1393 train_time:74503ms step_avg:116.23ms
step:652/1393 train_time:74622ms step_avg:116.23ms
step:653/1393 train_time:74742ms step_avg:116.24ms
step:654/1393 train_time:74861ms step_avg:116.24ms
step:655/1393 train_time:74981ms step_avg:116.25ms
step:656/1393 train_time:75101ms step_avg:116.26ms
step:657/1393 train_time:75221ms step_avg:116.26ms
step:658/1393 train_time:75342ms step_avg:116.27ms
step:659/1393 train_time:75462ms step_avg:116.27ms
step:660/1393 train_time:75582ms step_avg:116.28ms
step:661/1393 train_time:75702ms step_avg:116.28ms
step:662/1393 train_time:75822ms step_avg:116.29ms
step:663/1393 train_time:75942ms step_avg:116.30ms
step:664/1393 train_time:76062ms step_avg:116.30ms
step:665/1393 train_time:76181ms step_avg:116.31ms
step:666/1393 train_time:76302ms step_avg:116.31ms
step:667/1393 train_time:76422ms step_avg:116.32ms
step:668/1393 train_time:76542ms step_avg:116.32ms
step:669/1393 train_time:76662ms step_avg:116.33ms
step:670/1393 train_time:76782ms step_avg:116.34ms
step:671/1393 train_time:76902ms step_avg:116.34ms
step:672/1393 train_time:77021ms step_avg:116.35ms
step:673/1393 train_time:77142ms step_avg:116.35ms
step:674/1393 train_time:77262ms step_avg:116.36ms
step:675/1393 train_time:77382ms step_avg:116.36ms
step:676/1393 train_time:77501ms step_avg:116.37ms
step:677/1393 train_time:77622ms step_avg:116.38ms
step:678/1393 train_time:77742ms step_avg:116.38ms
step:679/1393 train_time:77862ms step_avg:116.39ms
step:680/1393 train_time:77982ms step_avg:116.39ms
step:681/1393 train_time:78102ms step_avg:116.40ms
step:682/1393 train_time:78222ms step_avg:116.40ms
step:683/1393 train_time:78342ms step_avg:116.41ms
step:684/1393 train_time:78462ms step_avg:116.41ms
step:685/1393 train_time:78582ms step_avg:116.42ms
step:686/1393 train_time:78702ms step_avg:116.42ms
step:687/1393 train_time:78822ms step_avg:116.43ms
step:688/1393 train_time:78942ms step_avg:116.43ms
step:689/1393 train_time:79062ms step_avg:116.44ms
step:690/1393 train_time:79181ms step_avg:116.44ms
step:691/1393 train_time:79301ms step_avg:116.45ms
step:692/1393 train_time:79421ms step_avg:116.45ms
step:693/1393 train_time:79541ms step_avg:116.46ms
step:694/1393 train_time:79662ms step_avg:116.47ms
step:695/1393 train_time:79781ms step_avg:116.47ms
step:696/1393 train_time:79902ms step_avg:116.48ms
step:697/1393 train_time:80024ms step_avg:116.48ms
step:698/1393 train_time:80144ms step_avg:116.49ms
step:699/1393 train_time:80264ms step_avg:116.49ms
step:700/1393 train_time:80384ms step_avg:116.50ms
step:701/1393 train_time:80504ms step_avg:116.50ms
step:702/1393 train_time:80623ms step_avg:116.51ms
step:703/1393 train_time:80744ms step_avg:116.51ms
step:704/1393 train_time:80863ms step_avg:116.52ms
step:705/1393 train_time:80983ms step_avg:116.52ms
step:706/1393 train_time:81102ms step_avg:116.53ms
step:707/1393 train_time:81222ms step_avg:116.53ms
step:708/1393 train_time:81342ms step_avg:116.54ms
step:709/1393 train_time:81462ms step_avg:116.54ms
step:710/1393 train_time:81582ms step_avg:116.55ms
step:711/1393 train_time:81701ms step_avg:116.55ms
step:712/1393 train_time:81821ms step_avg:116.55ms
step:713/1393 train_time:81941ms step_avg:116.56ms
step:714/1393 train_time:82061ms step_avg:116.56ms
step:715/1393 train_time:82181ms step_avg:116.57ms
step:716/1393 train_time:82300ms step_avg:116.57ms
step:717/1393 train_time:82421ms step_avg:116.58ms
step:718/1393 train_time:82540ms step_avg:116.58ms
step:719/1393 train_time:82661ms step_avg:116.59ms
step:720/1393 train_time:82782ms step_avg:116.59ms
step:721/1393 train_time:82902ms step_avg:116.60ms
step:722/1393 train_time:83021ms step_avg:116.60ms
step:723/1393 train_time:83140ms step_avg:116.61ms
step:724/1393 train_time:83261ms step_avg:116.61ms
step:725/1393 train_time:83383ms step_avg:116.62ms
step:726/1393 train_time:83504ms step_avg:116.63ms
step:727/1393 train_time:83627ms step_avg:116.63ms
step:728/1393 train_time:83748ms step_avg:116.64ms
step:729/1393 train_time:83869ms step_avg:116.65ms
step:730/1393 train_time:83992ms step_avg:116.66ms
step:731/1393 train_time:84114ms step_avg:116.66ms
step:732/1393 train_time:84235ms step_avg:116.67ms
step:733/1393 train_time:84357ms step_avg:116.68ms
step:734/1393 train_time:84479ms step_avg:116.68ms
step:735/1393 train_time:84601ms step_avg:116.69ms
step:736/1393 train_time:84723ms step_avg:116.70ms
step:737/1393 train_time:84843ms step_avg:116.70ms
step:738/1393 train_time:84966ms step_avg:116.71ms
step:739/1393 train_time:85087ms step_avg:116.72ms
step:740/1393 train_time:85208ms step_avg:116.72ms
step:741/1393 train_time:85330ms step_avg:116.73ms
step:742/1393 train_time:85451ms step_avg:116.74ms
step:743/1393 train_time:85573ms step_avg:116.74ms
step:744/1393 train_time:85696ms step_avg:116.75ms
step:745/1393 train_time:85817ms step_avg:116.76ms
step:746/1393 train_time:85939ms step_avg:116.76ms
step:747/1393 train_time:86061ms step_avg:116.77ms
step:748/1393 train_time:86182ms step_avg:116.78ms
step:749/1393 train_time:86304ms step_avg:116.78ms
step:750/1393 train_time:86425ms step_avg:116.79ms
step:750/1393 val_loss:3.5163 train_time:86546ms step_avg:116.95ms
step:751/1393 train_time:86568ms step_avg:116.83ms
step:752/1393 train_time:86672ms step_avg:116.81ms
step:753/1393 train_time:86800ms step_avg:116.82ms
step:754/1393 train_time:86924ms step_avg:116.83ms
step:755/1393 train_time:87047ms step_avg:116.84ms
step:756/1393 train_time:87168ms step_avg:116.85ms
step:757/1393 train_time:87290ms step_avg:116.85ms
step:758/1393 train_time:87411ms step_avg:116.86ms
step:759/1393 train_time:87533ms step_avg:116.87ms
step:760/1393 train_time:87654ms step_avg:116.87ms
step:761/1393 train_time:87776ms step_avg:116.88ms
step:762/1393 train_time:87897ms step_avg:116.88ms
step:763/1393 train_time:88019ms step_avg:116.89ms
step:764/1393 train_time:88141ms step_avg:116.90ms
step:765/1393 train_time:88264ms step_avg:116.91ms
step:766/1393 train_time:88387ms step_avg:116.91ms
step:767/1393 train_time:88509ms step_avg:116.92ms
step:768/1393 train_time:88630ms step_avg:116.93ms
step:769/1393 train_time:88752ms step_avg:116.93ms
step:770/1393 train_time:88873ms step_avg:116.94ms
step:771/1393 train_time:88995ms step_avg:116.94ms
step:772/1393 train_time:89116ms step_avg:116.95ms
step:773/1393 train_time:89238ms step_avg:116.96ms
step:774/1393 train_time:89359ms step_avg:116.96ms
step:775/1393 train_time:89481ms step_avg:116.97ms
step:776/1393 train_time:89603ms step_avg:116.97ms
step:777/1393 train_time:89726ms step_avg:116.98ms
step:778/1393 train_time:89847ms step_avg:116.99ms
step:779/1393 train_time:89969ms step_avg:117.00ms
step:780/1393 train_time:90091ms step_avg:117.00ms
step:781/1393 train_time:90212ms step_avg:117.01ms
step:782/1393 train_time:90333ms step_avg:117.01ms
step:783/1393 train_time:90456ms step_avg:117.02ms
step:784/1393 train_time:90578ms step_avg:117.03ms
step:785/1393 train_time:90700ms step_avg:117.03ms
step:786/1393 train_time:90822ms step_avg:117.04ms
step:787/1393 train_time:90944ms step_avg:117.04ms
step:788/1393 train_time:91066ms step_avg:117.05ms
step:789/1393 train_time:91188ms step_avg:117.06ms
step:790/1393 train_time:91309ms step_avg:117.06ms
step:791/1393 train_time:91431ms step_avg:117.07ms
step:792/1393 train_time:91553ms step_avg:117.07ms
step:793/1393 train_time:91675ms step_avg:117.08ms
step:794/1393 train_time:91796ms step_avg:117.09ms
step:795/1393 train_time:91916ms step_avg:117.09ms
step:796/1393 train_time:92038ms step_avg:117.10ms
step:797/1393 train_time:92159ms step_avg:117.10ms
step:798/1393 train_time:92282ms step_avg:117.11ms
step:799/1393 train_time:92407ms step_avg:117.12ms
step:800/1393 train_time:92529ms step_avg:117.12ms
step:801/1393 train_time:92650ms step_avg:117.13ms
step:802/1393 train_time:92770ms step_avg:117.13ms
step:803/1393 train_time:92892ms step_avg:117.14ms
step:804/1393 train_time:93014ms step_avg:117.15ms
step:805/1393 train_time:93135ms step_avg:117.15ms
step:806/1393 train_time:93256ms step_avg:117.16ms
step:807/1393 train_time:93379ms step_avg:117.16ms
step:808/1393 train_time:93502ms step_avg:117.17ms
step:809/1393 train_time:93624ms step_avg:117.18ms
step:810/1393 train_time:93745ms step_avg:117.18ms
step:811/1393 train_time:93867ms step_avg:117.19ms
step:812/1393 train_time:93988ms step_avg:117.19ms
step:813/1393 train_time:94110ms step_avg:117.20ms
step:814/1393 train_time:94232ms step_avg:117.20ms
step:815/1393 train_time:94353ms step_avg:117.21ms
step:816/1393 train_time:94475ms step_avg:117.21ms
step:817/1393 train_time:94597ms step_avg:117.22ms
step:818/1393 train_time:94720ms step_avg:117.23ms
step:819/1393 train_time:94842ms step_avg:117.23ms
step:820/1393 train_time:94965ms step_avg:117.24ms
step:821/1393 train_time:95088ms step_avg:117.25ms
step:822/1393 train_time:95210ms step_avg:117.25ms
step:823/1393 train_time:95332ms step_avg:117.26ms
step:824/1393 train_time:95454ms step_avg:117.26ms
step:825/1393 train_time:95575ms step_avg:117.27ms
step:826/1393 train_time:95696ms step_avg:117.27ms
step:827/1393 train_time:95818ms step_avg:117.28ms
step:828/1393 train_time:95939ms step_avg:117.28ms
step:829/1393 train_time:96061ms step_avg:117.29ms
step:830/1393 train_time:96184ms step_avg:117.30ms
step:831/1393 train_time:96306ms step_avg:117.30ms
step:832/1393 train_time:96428ms step_avg:117.31ms
step:833/1393 train_time:96549ms step_avg:117.31ms
step:834/1393 train_time:96671ms step_avg:117.32ms
step:835/1393 train_time:96792ms step_avg:117.32ms
step:836/1393 train_time:96913ms step_avg:117.33ms
step:837/1393 train_time:97035ms step_avg:117.33ms
step:838/1393 train_time:97157ms step_avg:117.34ms
step:839/1393 train_time:97278ms step_avg:117.34ms
step:840/1393 train_time:97401ms step_avg:117.35ms
step:841/1393 train_time:97524ms step_avg:117.36ms
step:842/1393 train_time:97646ms step_avg:117.36ms
step:843/1393 train_time:97767ms step_avg:117.37ms
step:844/1393 train_time:97888ms step_avg:117.37ms
step:845/1393 train_time:98009ms step_avg:117.38ms
step:846/1393 train_time:98132ms step_avg:117.38ms
step:847/1393 train_time:98254ms step_avg:117.39ms
step:848/1393 train_time:98376ms step_avg:117.39ms
step:849/1393 train_time:98499ms step_avg:117.40ms
step:850/1393 train_time:98620ms step_avg:117.41ms
step:851/1393 train_time:98743ms step_avg:117.41ms
step:852/1393 train_time:98865ms step_avg:117.42ms
step:853/1393 train_time:98987ms step_avg:117.42ms
step:854/1393 train_time:99110ms step_avg:117.43ms
step:855/1393 train_time:99233ms step_avg:117.43ms
step:856/1393 train_time:99354ms step_avg:117.44ms
step:857/1393 train_time:99476ms step_avg:117.45ms
step:858/1393 train_time:99597ms step_avg:117.45ms
step:859/1393 train_time:99719ms step_avg:117.45ms
step:860/1393 train_time:99840ms step_avg:117.46ms
step:861/1393 train_time:99961ms step_avg:117.46ms
step:862/1393 train_time:100083ms step_avg:117.47ms
step:863/1393 train_time:100210ms step_avg:117.48ms
step:864/1393 train_time:100331ms step_avg:117.48ms
step:865/1393 train_time:100453ms step_avg:117.49ms
step:866/1393 train_time:100575ms step_avg:117.49ms
step:867/1393 train_time:100697ms step_avg:117.50ms
step:868/1393 train_time:100817ms step_avg:117.50ms
step:869/1393 train_time:100939ms step_avg:117.51ms
step:870/1393 train_time:101061ms step_avg:117.51ms
step:871/1393 train_time:101183ms step_avg:117.52ms
step:872/1393 train_time:101307ms step_avg:117.53ms
step:873/1393 train_time:101429ms step_avg:117.53ms
step:874/1393 train_time:101550ms step_avg:117.53ms
step:875/1393 train_time:101672ms step_avg:117.54ms
step:875/1393 val_loss:3.4680 train_time:101793ms step_avg:117.68ms
step:876/1393 train_time:101815ms step_avg:117.57ms
step:877/1393 train_time:101919ms step_avg:117.55ms
step:878/1393 train_time:102047ms step_avg:117.57ms
step:879/1393 train_time:102170ms step_avg:117.57ms
step:880/1393 train_time:102292ms step_avg:117.58ms
step:881/1393 train_time:102414ms step_avg:117.58ms
step:882/1393 train_time:102535ms step_avg:117.59ms
step:883/1393 train_time:102657ms step_avg:117.59ms
step:884/1393 train_time:102779ms step_avg:117.60ms
step:885/1393 train_time:102901ms step_avg:117.60ms
step:886/1393 train_time:103023ms step_avg:117.61ms
step:887/1393 train_time:103144ms step_avg:117.61ms
step:888/1393 train_time:103266ms step_avg:117.62ms
step:889/1393 train_time:103389ms step_avg:117.62ms
step:890/1393 train_time:103512ms step_avg:117.63ms
step:891/1393 train_time:103636ms step_avg:117.63ms
step:892/1393 train_time:103758ms step_avg:117.64ms
step:893/1393 train_time:103880ms step_avg:117.64ms
step:894/1393 train_time:104005ms step_avg:117.65ms
step:895/1393 train_time:104127ms step_avg:117.66ms
step:896/1393 train_time:104249ms step_avg:117.66ms
step:897/1393 train_time:104371ms step_avg:117.67ms
step:898/1393 train_time:104493ms step_avg:117.67ms
step:899/1393 train_time:104616ms step_avg:117.68ms
step:900/1393 train_time:104737ms step_avg:117.68ms
step:901/1393 train_time:104858ms step_avg:117.69ms
step:902/1393 train_time:104982ms step_avg:117.69ms
step:903/1393 train_time:105104ms step_avg:117.70ms
step:904/1393 train_time:105227ms step_avg:117.70ms
step:905/1393 train_time:105349ms step_avg:117.71ms
step:906/1393 train_time:105472ms step_avg:117.71ms
step:907/1393 train_time:105594ms step_avg:117.72ms
step:908/1393 train_time:105715ms step_avg:117.72ms
step:909/1393 train_time:105837ms step_avg:117.73ms
step:910/1393 train_time:105959ms step_avg:117.73ms
step:911/1393 train_time:106081ms step_avg:117.74ms
step:912/1393 train_time:106203ms step_avg:117.74ms
step:913/1393 train_time:106324ms step_avg:117.75ms
step:914/1393 train_time:106446ms step_avg:117.75ms
step:915/1393 train_time:106568ms step_avg:117.75ms
step:916/1393 train_time:106691ms step_avg:117.76ms
step:917/1393 train_time:106813ms step_avg:117.77ms
step:918/1393 train_time:106935ms step_avg:117.77ms
step:919/1393 train_time:107056ms step_avg:117.77ms
step:920/1393 train_time:107178ms step_avg:117.78ms
step:921/1393 train_time:107300ms step_avg:117.78ms
step:922/1393 train_time:107421ms step_avg:117.79ms
step:923/1393 train_time:107544ms step_avg:117.79ms
step:924/1393 train_time:107666ms step_avg:117.80ms
step:925/1393 train_time:107788ms step_avg:117.80ms
step:926/1393 train_time:107911ms step_avg:117.81ms
step:927/1393 train_time:108033ms step_avg:117.81ms
step:928/1393 train_time:108155ms step_avg:117.82ms
step:929/1393 train_time:108278ms step_avg:117.82ms
step:930/1393 train_time:108400ms step_avg:117.83ms
step:931/1393 train_time:108525ms step_avg:117.83ms
step:932/1393 train_time:108648ms step_avg:117.84ms
step:933/1393 train_time:108773ms step_avg:117.85ms
step:934/1393 train_time:108896ms step_avg:117.85ms
step:935/1393 train_time:109020ms step_avg:117.86ms
step:936/1393 train_time:109143ms step_avg:117.86ms
step:937/1393 train_time:109266ms step_avg:117.87ms
step:938/1393 train_time:109388ms step_avg:117.88ms
step:939/1393 train_time:109511ms step_avg:117.88ms
step:940/1393 train_time:109634ms step_avg:117.89ms
step:941/1393 train_time:109758ms step_avg:117.89ms
step:942/1393 train_time:109881ms step_avg:117.90ms
step:943/1393 train_time:110006ms step_avg:117.91ms
step:944/1393 train_time:110130ms step_avg:117.91ms
step:945/1393 train_time:110255ms step_avg:117.92ms
step:946/1393 train_time:110380ms step_avg:117.93ms
step:947/1393 train_time:110504ms step_avg:117.93ms
step:948/1393 train_time:110628ms step_avg:117.94ms
step:949/1393 train_time:110750ms step_avg:117.94ms
step:950/1393 train_time:110876ms step_avg:117.95ms
step:951/1393 train_time:110999ms step_avg:117.96ms
step:952/1393 train_time:111124ms step_avg:117.97ms
step:953/1393 train_time:111246ms step_avg:117.97ms
step:954/1393 train_time:111370ms step_avg:117.98ms
step:955/1393 train_time:111493ms step_avg:117.98ms
step:956/1393 train_time:111617ms step_avg:117.99ms
step:957/1393 train_time:111740ms step_avg:117.99ms
step:958/1393 train_time:111864ms step_avg:118.00ms
step:959/1393 train_time:111988ms step_avg:118.01ms
step:960/1393 train_time:112111ms step_avg:118.01ms
step:961/1393 train_time:112235ms step_avg:118.02ms
step:962/1393 train_time:112358ms step_avg:118.02ms
step:963/1393 train_time:112481ms step_avg:118.03ms
step:964/1393 train_time:112605ms step_avg:118.03ms
step:965/1393 train_time:112731ms step_avg:118.04ms
step:966/1393 train_time:112854ms step_avg:118.05ms
step:967/1393 train_time:112976ms step_avg:118.05ms
step:968/1393 train_time:113099ms step_avg:118.06ms
step:969/1393 train_time:113222ms step_avg:118.06ms
step:970/1393 train_time:113345ms step_avg:118.07ms
step:971/1393 train_time:113469ms step_avg:118.07ms
step:972/1393 train_time:113595ms step_avg:118.08ms
step:973/1393 train_time:113719ms step_avg:118.09ms
step:974/1393 train_time:113842ms step_avg:118.09ms
step:975/1393 train_time:113966ms step_avg:118.10ms
step:976/1393 train_time:114091ms step_avg:118.11ms
step:977/1393 train_time:114216ms step_avg:118.11ms
step:978/1393 train_time:114342ms step_avg:118.12ms
step:979/1393 train_time:114465ms step_avg:118.13ms
step:980/1393 train_time:114588ms step_avg:118.13ms
step:981/1393 train_time:114712ms step_avg:118.14ms
step:982/1393 train_time:114836ms step_avg:118.14ms
step:983/1393 train_time:114958ms step_avg:118.15ms
step:984/1393 train_time:115082ms step_avg:118.15ms
step:985/1393 train_time:115204ms step_avg:118.16ms
step:986/1393 train_time:115328ms step_avg:118.16ms
step:987/1393 train_time:115450ms step_avg:118.17ms
step:988/1393 train_time:115574ms step_avg:118.17ms
step:989/1393 train_time:115698ms step_avg:118.18ms
step:990/1393 train_time:115823ms step_avg:118.19ms
step:991/1393 train_time:115945ms step_avg:118.19ms
step:992/1393 train_time:116068ms step_avg:118.20ms
step:993/1393 train_time:116192ms step_avg:118.20ms
step:994/1393 train_time:116317ms step_avg:118.21ms
step:995/1393 train_time:116441ms step_avg:118.21ms
step:996/1393 train_time:116564ms step_avg:118.22ms
step:997/1393 train_time:116686ms step_avg:118.22ms
step:998/1393 train_time:116810ms step_avg:118.23ms
step:999/1393 train_time:116934ms step_avg:118.23ms
step:1000/1393 train_time:117057ms step_avg:118.24ms
step:1000/1393 val_loss:3.4098 train_time:117179ms step_avg:118.36ms
step:1001/1393 train_time:117202ms step_avg:118.27ms
step:1002/1393 train_time:117308ms step_avg:118.25ms
step:1003/1393 train_time:117438ms step_avg:118.27ms
step:1004/1393 train_time:117563ms step_avg:118.27ms
step:1005/1393 train_time:117687ms step_avg:118.28ms
step:1006/1393 train_time:117811ms step_avg:118.28ms
step:1007/1393 train_time:117934ms step_avg:118.29ms
step:1008/1393 train_time:118058ms step_avg:118.29ms
step:1009/1393 train_time:118182ms step_avg:118.30ms
step:1010/1393 train_time:118306ms step_avg:118.31ms
step:1011/1393 train_time:118430ms step_avg:118.31ms
step:1012/1393 train_time:118554ms step_avg:118.32ms
step:1013/1393 train_time:118679ms step_avg:118.32ms
step:1014/1393 train_time:118802ms step_avg:118.33ms
step:1015/1393 train_time:118925ms step_avg:118.33ms
step:1016/1393 train_time:119048ms step_avg:118.34ms
step:1017/1393 train_time:119171ms step_avg:118.34ms
step:1018/1393 train_time:119295ms step_avg:118.35ms
step:1019/1393 train_time:119419ms step_avg:118.35ms
step:1020/1393 train_time:119543ms step_avg:118.36ms
step:1021/1393 train_time:119666ms step_avg:118.36ms
step:1022/1393 train_time:119790ms step_avg:118.37ms
step:1023/1393 train_time:119914ms step_avg:118.38ms
step:1024/1393 train_time:120038ms step_avg:118.38ms
step:1025/1393 train_time:120160ms step_avg:118.38ms
step:1026/1393 train_time:120283ms step_avg:118.39ms
step:1027/1393 train_time:120407ms step_avg:118.39ms
step:1028/1393 train_time:120531ms step_avg:118.40ms
step:1029/1393 train_time:120655ms step_avg:118.41ms
step:1030/1393 train_time:120780ms step_avg:118.41ms
step:1031/1393 train_time:120904ms step_avg:118.42ms
step:1032/1393 train_time:121026ms step_avg:118.42ms
step:1033/1393 train_time:121149ms step_avg:118.43ms
step:1034/1393 train_time:121273ms step_avg:118.43ms
step:1035/1393 train_time:121397ms step_avg:118.44ms
step:1036/1393 train_time:121521ms step_avg:118.44ms
step:1037/1393 train_time:121644ms step_avg:118.45ms
step:1038/1393 train_time:121769ms step_avg:118.45ms
step:1039/1393 train_time:121892ms step_avg:118.46ms
step:1040/1393 train_time:122015ms step_avg:118.46ms
step:1041/1393 train_time:122139ms step_avg:118.47ms
step:1042/1393 train_time:122262ms step_avg:118.47ms
step:1043/1393 train_time:122386ms step_avg:118.48ms
step:1044/1393 train_time:122511ms step_avg:118.48ms
step:1045/1393 train_time:122634ms step_avg:118.49ms
step:1046/1393 train_time:122758ms step_avg:118.49ms
step:1047/1393 train_time:122881ms step_avg:118.50ms
step:1048/1393 train_time:123005ms step_avg:118.50ms
step:1049/1393 train_time:123128ms step_avg:118.51ms
step:1050/1393 train_time:123252ms step_avg:118.51ms
step:1051/1393 train_time:123376ms step_avg:118.52ms
step:1052/1393 train_time:123500ms step_avg:118.52ms
step:1053/1393 train_time:123626ms step_avg:118.53ms
step:1054/1393 train_time:123749ms step_avg:118.53ms
step:1055/1393 train_time:123875ms step_avg:118.54ms
step:1056/1393 train_time:123999ms step_avg:118.55ms
step:1057/1393 train_time:124122ms step_avg:118.55ms
step:1058/1393 train_time:124245ms step_avg:118.55ms
step:1059/1393 train_time:124369ms step_avg:118.56ms
step:1060/1393 train_time:124493ms step_avg:118.56ms
step:1061/1393 train_time:124616ms step_avg:118.57ms
step:1062/1393 train_time:124739ms step_avg:118.57ms
step:1063/1393 train_time:124865ms step_avg:118.58ms
step:1064/1393 train_time:124988ms step_avg:118.58ms
step:1065/1393 train_time:125113ms step_avg:118.59ms
step:1066/1393 train_time:125236ms step_avg:118.60ms
step:1067/1393 train_time:125360ms step_avg:118.60ms
step:1068/1393 train_time:125484ms step_avg:118.61ms
step:1069/1393 train_time:125609ms step_avg:118.61ms
step:1070/1393 train_time:125732ms step_avg:118.61ms
step:1071/1393 train_time:125857ms step_avg:118.62ms
step:1072/1393 train_time:125979ms step_avg:118.62ms
step:1073/1393 train_time:126104ms step_avg:118.63ms
step:1074/1393 train_time:126227ms step_avg:118.63ms
step:1075/1393 train_time:126352ms step_avg:118.64ms
step:1076/1393 train_time:126476ms step_avg:118.65ms
step:1077/1393 train_time:126600ms step_avg:118.65ms
step:1078/1393 train_time:126723ms step_avg:118.65ms
step:1079/1393 train_time:126847ms step_avg:118.66ms
step:1080/1393 train_time:126972ms step_avg:118.67ms
step:1081/1393 train_time:127097ms step_avg:118.67ms
step:1082/1393 train_time:127221ms step_avg:118.68ms
step:1083/1393 train_time:127346ms step_avg:118.68ms
step:1084/1393 train_time:127470ms step_avg:118.69ms
step:1085/1393 train_time:127593ms step_avg:118.69ms
step:1086/1393 train_time:127718ms step_avg:118.70ms
step:1087/1393 train_time:127841ms step_avg:118.70ms
step:1088/1393 train_time:127965ms step_avg:118.71ms
step:1089/1393 train_time:128090ms step_avg:118.71ms
step:1090/1393 train_time:128214ms step_avg:118.72ms
step:1091/1393 train_time:128338ms step_avg:118.72ms
step:1092/1393 train_time:128465ms step_avg:118.73ms
step:1093/1393 train_time:128588ms step_avg:118.73ms
step:1094/1393 train_time:128712ms step_avg:118.74ms
step:1095/1393 train_time:128836ms step_avg:118.74ms
step:1096/1393 train_time:128960ms step_avg:118.75ms
step:1097/1393 train_time:129083ms step_avg:118.75ms
step:1098/1393 train_time:129207ms step_avg:118.76ms
step:1099/1393 train_time:129329ms step_avg:118.76ms
step:1100/1393 train_time:129453ms step_avg:118.76ms
step:1101/1393 train_time:129579ms step_avg:118.77ms
step:1102/1393 train_time:129704ms step_avg:118.78ms
step:1103/1393 train_time:129828ms step_avg:118.78ms
step:1104/1393 train_time:129951ms step_avg:118.79ms
step:1105/1393 train_time:130074ms step_avg:118.79ms
step:1106/1393 train_time:130197ms step_avg:118.79ms
step:1107/1393 train_time:130320ms step_avg:118.80ms
step:1108/1393 train_time:130446ms step_avg:118.80ms
step:1109/1393 train_time:130569ms step_avg:118.81ms
step:1110/1393 train_time:130694ms step_avg:118.81ms
step:1111/1393 train_time:130818ms step_avg:118.82ms
step:1112/1393 train_time:130942ms step_avg:118.82ms
step:1113/1393 train_time:131068ms step_avg:118.83ms
step:1114/1393 train_time:131191ms step_avg:118.83ms
step:1115/1393 train_time:131315ms step_avg:118.84ms
step:1116/1393 train_time:131439ms step_avg:118.84ms
step:1117/1393 train_time:131564ms step_avg:118.85ms
step:1118/1393 train_time:131689ms step_avg:118.85ms
step:1119/1393 train_time:131812ms step_avg:118.86ms
step:1120/1393 train_time:131938ms step_avg:118.86ms
step:1121/1393 train_time:132061ms step_avg:118.87ms
step:1122/1393 train_time:132185ms step_avg:118.87ms
step:1123/1393 train_time:132308ms step_avg:118.88ms
step:1124/1393 train_time:132432ms step_avg:118.88ms
step:1125/1393 train_time:132555ms step_avg:118.88ms
step:1125/1393 val_loss:3.3591 train_time:132678ms step_avg:118.99ms
step:1126/1393 train_time:132701ms step_avg:118.91ms
step:1127/1393 train_time:132809ms step_avg:118.90ms
step:1128/1393 train_time:132938ms step_avg:118.91ms
step:1129/1393 train_time:133061ms step_avg:118.91ms
step:1130/1393 train_time:133185ms step_avg:118.92ms
step:1131/1393 train_time:133309ms step_avg:118.92ms
step:1132/1393 train_time:133434ms step_avg:118.93ms
step:1133/1393 train_time:133558ms step_avg:118.93ms
step:1134/1393 train_time:133681ms step_avg:118.93ms
step:1135/1393 train_time:133806ms step_avg:118.94ms
step:1136/1393 train_time:133930ms step_avg:118.94ms
step:1137/1393 train_time:134054ms step_avg:118.95ms
step:1138/1393 train_time:134181ms step_avg:118.95ms
step:1139/1393 train_time:134309ms step_avg:118.96ms
step:1140/1393 train_time:134433ms step_avg:118.97ms
step:1141/1393 train_time:134559ms step_avg:118.97ms
step:1142/1393 train_time:134682ms step_avg:118.98ms
step:1143/1393 train_time:134808ms step_avg:118.98ms
step:1144/1393 train_time:134934ms step_avg:118.99ms
step:1145/1393 train_time:135058ms step_avg:118.99ms
step:1146/1393 train_time:135183ms step_avg:119.00ms
step:1147/1393 train_time:135308ms step_avg:119.00ms
step:1148/1393 train_time:135433ms step_avg:119.01ms
step:1149/1393 train_time:135559ms step_avg:119.02ms
step:1150/1393 train_time:135683ms step_avg:119.02ms
step:1151/1393 train_time:135809ms step_avg:119.03ms
step:1152/1393 train_time:135934ms step_avg:119.03ms
step:1153/1393 train_time:136058ms step_avg:119.04ms
step:1154/1393 train_time:136183ms step_avg:119.04ms
step:1155/1393 train_time:136308ms step_avg:119.05ms
step:1156/1393 train_time:136433ms step_avg:119.05ms
step:1157/1393 train_time:136563ms step_avg:119.06ms
step:1158/1393 train_time:136688ms step_avg:119.07ms
step:1159/1393 train_time:136813ms step_avg:119.07ms
step:1160/1393 train_time:136942ms step_avg:119.08ms
step:1161/1393 train_time:137066ms step_avg:119.08ms
step:1162/1393 train_time:137191ms step_avg:119.09ms
step:1163/1393 train_time:137318ms step_avg:119.10ms
step:1164/1393 train_time:137442ms step_avg:119.10ms
step:1165/1393 train_time:137568ms step_avg:119.11ms
step:1166/1393 train_time:137696ms step_avg:119.11ms
step:1167/1393 train_time:137823ms step_avg:119.12ms
step:1168/1393 train_time:137949ms step_avg:119.13ms
step:1169/1393 train_time:138073ms step_avg:119.13ms
step:1170/1393 train_time:138204ms step_avg:119.14ms
step:1171/1393 train_time:138329ms step_avg:119.15ms
step:1172/1393 train_time:138454ms step_avg:119.15ms
step:1173/1393 train_time:138578ms step_avg:119.16ms
step:1174/1393 train_time:138705ms step_avg:119.16ms
step:1175/1393 train_time:138830ms step_avg:119.17ms
step:1176/1393 train_time:138955ms step_avg:119.17ms
step:1177/1393 train_time:139080ms step_avg:119.18ms
step:1178/1393 train_time:139205ms step_avg:119.18ms
step:1179/1393 train_time:139332ms step_avg:119.19ms
step:1180/1393 train_time:139459ms step_avg:119.20ms
step:1181/1393 train_time:139584ms step_avg:119.20ms
step:1182/1393 train_time:139709ms step_avg:119.21ms
step:1183/1393 train_time:139834ms step_avg:119.21ms
step:1184/1393 train_time:139960ms step_avg:119.22ms
step:1185/1393 train_time:140091ms step_avg:119.23ms
step:1186/1393 train_time:140216ms step_avg:119.23ms
step:1187/1393 train_time:140341ms step_avg:119.24ms
step:1188/1393 train_time:140465ms step_avg:119.24ms
step:1189/1393 train_time:140591ms step_avg:119.25ms
step:1190/1393 train_time:140716ms step_avg:119.25ms
step:1191/1393 train_time:140842ms step_avg:119.26ms
step:1192/1393 train_time:140969ms step_avg:119.26ms
step:1193/1393 train_time:141096ms step_avg:119.27ms
step:1194/1393 train_time:141221ms step_avg:119.27ms
step:1195/1393 train_time:141347ms step_avg:119.28ms
step:1196/1393 train_time:141472ms step_avg:119.29ms
step:1197/1393 train_time:141602ms step_avg:119.29ms
step:1198/1393 train_time:141725ms step_avg:119.30ms
step:1199/1393 train_time:141851ms step_avg:119.30ms
step:1200/1393 train_time:141975ms step_avg:119.31ms
step:1201/1393 train_time:142098ms step_avg:119.31ms
step:1202/1393 train_time:142224ms step_avg:119.32ms
step:1203/1393 train_time:142349ms step_avg:119.32ms
step:1204/1393 train_time:142475ms step_avg:119.33ms
step:1205/1393 train_time:142599ms step_avg:119.33ms
step:1206/1393 train_time:142725ms step_avg:119.34ms
step:1207/1393 train_time:142852ms step_avg:119.34ms
step:1208/1393 train_time:142977ms step_avg:119.35ms
step:1209/1393 train_time:143102ms step_avg:119.35ms
step:1210/1393 train_time:143228ms step_avg:119.36ms
step:1211/1393 train_time:143352ms step_avg:119.36ms
step:1212/1393 train_time:143480ms step_avg:119.37ms
step:1213/1393 train_time:143605ms step_avg:119.37ms
step:1214/1393 train_time:143731ms step_avg:119.38ms
step:1215/1393 train_time:143856ms step_avg:119.38ms
step:1216/1393 train_time:143981ms step_avg:119.39ms
step:1217/1393 train_time:144106ms step_avg:119.39ms
step:1218/1393 train_time:144231ms step_avg:119.40ms
step:1219/1393 train_time:144356ms step_avg:119.40ms
step:1220/1393 train_time:144485ms step_avg:119.41ms
step:1221/1393 train_time:144610ms step_avg:119.41ms
step:1222/1393 train_time:144737ms step_avg:119.42ms
step:1223/1393 train_time:144863ms step_avg:119.43ms
step:1224/1393 train_time:144988ms step_avg:119.43ms
step:1225/1393 train_time:145113ms step_avg:119.43ms
step:1226/1393 train_time:145238ms step_avg:119.44ms
step:1227/1393 train_time:145363ms step_avg:119.44ms
step:1228/1393 train_time:145487ms step_avg:119.45ms
step:1229/1393 train_time:145613ms step_avg:119.45ms
step:1230/1393 train_time:145736ms step_avg:119.46ms
step:1231/1393 train_time:145861ms step_avg:119.46ms
step:1232/1393 train_time:145987ms step_avg:119.47ms
step:1233/1393 train_time:146112ms step_avg:119.47ms
step:1234/1393 train_time:146237ms step_avg:119.47ms
step:1235/1393 train_time:146362ms step_avg:119.48ms
step:1236/1393 train_time:146491ms step_avg:119.49ms
step:1237/1393 train_time:146615ms step_avg:119.49ms
step:1238/1393 train_time:146741ms step_avg:119.50ms
step:1239/1393 train_time:146867ms step_avg:119.50ms
step:1240/1393 train_time:146996ms step_avg:119.51ms
step:1241/1393 train_time:147120ms step_avg:119.51ms
step:1242/1393 train_time:147245ms step_avg:119.52ms
step:1243/1393 train_time:147372ms step_avg:119.52ms
step:1244/1393 train_time:147499ms step_avg:119.53ms
step:1245/1393 train_time:147624ms step_avg:119.53ms
step:1246/1393 train_time:147749ms step_avg:119.54ms
step:1247/1393 train_time:147874ms step_avg:119.54ms
step:1248/1393 train_time:148001ms step_avg:119.55ms
step:1249/1393 train_time:148126ms step_avg:119.55ms
step:1250/1393 train_time:148252ms step_avg:119.56ms
step:1250/1393 val_loss:3.3151 train_time:148379ms step_avg:119.66ms
step:1251/1393 train_time:148402ms step_avg:119.58ms
step:1252/1393 train_time:148513ms step_avg:119.58ms
step:1253/1393 train_time:148644ms step_avg:119.59ms
step:1254/1393 train_time:148768ms step_avg:119.59ms
step:1255/1393 train_time:148893ms step_avg:119.59ms
step:1256/1393 train_time:149020ms step_avg:119.60ms
step:1257/1393 train_time:149145ms step_avg:119.60ms
step:1258/1393 train_time:149269ms step_avg:119.61ms
step:1259/1393 train_time:149395ms step_avg:119.61ms
step:1260/1393 train_time:149520ms step_avg:119.62ms
step:1261/1393 train_time:149650ms step_avg:119.62ms
step:1262/1393 train_time:149777ms step_avg:119.63ms
step:1263/1393 train_time:149901ms step_avg:119.63ms
step:1264/1393 train_time:150025ms step_avg:119.64ms
step:1265/1393 train_time:150150ms step_avg:119.64ms
step:1266/1393 train_time:150275ms step_avg:119.65ms
step:1267/1393 train_time:150402ms step_avg:119.65ms
step:1268/1393 train_time:150528ms step_avg:119.66ms
step:1269/1393 train_time:150653ms step_avg:119.66ms
step:1270/1393 train_time:150779ms step_avg:119.67ms
step:1271/1393 train_time:150908ms step_avg:119.67ms
step:1272/1393 train_time:151036ms step_avg:119.68ms
step:1273/1393 train_time:151162ms step_avg:119.68ms
step:1274/1393 train_time:151291ms step_avg:119.69ms
step:1275/1393 train_time:151418ms step_avg:119.70ms
step:1276/1393 train_time:151542ms step_avg:119.70ms
step:1277/1393 train_time:151669ms step_avg:119.71ms
step:1278/1393 train_time:151796ms step_avg:119.71ms
step:1279/1393 train_time:151922ms step_avg:119.72ms
step:1280/1393 train_time:152047ms step_avg:119.72ms
step:1281/1393 train_time:152172ms step_avg:119.73ms
step:1282/1393 train_time:152297ms step_avg:119.73ms
step:1283/1393 train_time:152423ms step_avg:119.74ms
step:1284/1393 train_time:152550ms step_avg:119.74ms
step:1285/1393 train_time:152677ms step_avg:119.75ms
step:1286/1393 train_time:152801ms step_avg:119.75ms
step:1287/1393 train_time:152929ms step_avg:119.76ms
step:1288/1393 train_time:153054ms step_avg:119.76ms
step:1289/1393 train_time:153180ms step_avg:119.77ms
step:1290/1393 train_time:153304ms step_avg:119.77ms
step:1291/1393 train_time:153430ms step_avg:119.77ms
step:1292/1393 train_time:153554ms step_avg:119.78ms
step:1293/1393 train_time:153679ms step_avg:119.78ms
step:1294/1393 train_time:153805ms step_avg:119.79ms
step:1295/1393 train_time:153932ms step_avg:119.79ms
step:1296/1393 train_time:154057ms step_avg:119.80ms
step:1297/1393 train_time:154182ms step_avg:119.80ms
step:1298/1393 train_time:154307ms step_avg:119.80ms
step:1299/1393 train_time:154435ms step_avg:119.81ms
step:1300/1393 train_time:154559ms step_avg:119.81ms
step:1301/1393 train_time:154684ms step_avg:119.82ms
step:1302/1393 train_time:154809ms step_avg:119.82ms
step:1303/1393 train_time:154937ms step_avg:119.83ms
step:1304/1393 train_time:155063ms step_avg:119.83ms
step:1305/1393 train_time:155188ms step_avg:119.84ms
step:1306/1393 train_time:155314ms step_avg:119.84ms
step:1307/1393 train_time:155440ms step_avg:119.85ms
step:1308/1393 train_time:155564ms step_avg:119.85ms
step:1309/1393 train_time:155695ms step_avg:119.86ms
step:1310/1393 train_time:155821ms step_avg:119.86ms
step:1311/1393 train_time:155947ms step_avg:119.87ms
step:1312/1393 train_time:156074ms step_avg:119.87ms
step:1313/1393 train_time:156202ms step_avg:119.88ms
step:1314/1393 train_time:156329ms step_avg:119.88ms
step:1315/1393 train_time:156455ms step_avg:119.89ms
step:1316/1393 train_time:156579ms step_avg:119.89ms
step:1317/1393 train_time:156706ms step_avg:119.90ms
step:1318/1393 train_time:156833ms step_avg:119.90ms
step:1319/1393 train_time:156958ms step_avg:119.91ms
step:1320/1393 train_time:157083ms step_avg:119.91ms
step:1321/1393 train_time:157209ms step_avg:119.92ms
step:1322/1393 train_time:157334ms step_avg:119.92ms
step:1323/1393 train_time:157459ms step_avg:119.92ms
step:1324/1393 train_time:157583ms step_avg:119.93ms
step:1325/1393 train_time:157708ms step_avg:119.93ms
step:1326/1393 train_time:157834ms step_avg:119.93ms
step:1327/1393 train_time:157960ms step_avg:119.94ms
step:1328/1393 train_time:158086ms step_avg:119.94ms
step:1329/1393 train_time:158211ms step_avg:119.95ms
step:1330/1393 train_time:158337ms step_avg:119.95ms
step:1331/1393 train_time:158461ms step_avg:119.96ms
step:1332/1393 train_time:158588ms step_avg:119.96ms
step:1333/1393 train_time:158717ms step_avg:119.97ms
step:1334/1393 train_time:158842ms step_avg:119.97ms
step:1335/1393 train_time:158966ms step_avg:119.97ms
step:1336/1393 train_time:159090ms step_avg:119.98ms
step:1337/1393 train_time:159216ms step_avg:119.98ms
step:1338/1393 train_time:159343ms step_avg:119.99ms
step:1339/1393 train_time:159468ms step_avg:119.99ms
step:1340/1393 train_time:159594ms step_avg:120.00ms
step:1341/1393 train_time:159721ms step_avg:120.00ms
step:1342/1393 train_time:159846ms step_avg:120.00ms
step:1343/1393 train_time:159973ms step_avg:120.01ms
step:1344/1393 train_time:160101ms step_avg:120.02ms
step:1345/1393 train_time:160225ms step_avg:120.02ms
step:1346/1393 train_time:160352ms step_avg:120.02ms
step:1347/1393 train_time:160479ms step_avg:120.03ms
step:1348/1393 train_time:160606ms step_avg:120.03ms
step:1349/1393 train_time:160737ms step_avg:120.04ms
step:1350/1393 train_time:160863ms step_avg:120.05ms
step:1351/1393 train_time:160992ms step_avg:120.05ms
step:1352/1393 train_time:161120ms step_avg:120.06ms
step:1353/1393 train_time:161246ms step_avg:120.06ms
step:1354/1393 train_time:161376ms step_avg:120.07ms
step:1355/1393 train_time:161502ms step_avg:120.08ms
step:1356/1393 train_time:161630ms step_avg:120.08ms
step:1357/1393 train_time:161756ms step_avg:120.09ms
step:1358/1393 train_time:161883ms step_avg:120.09ms
step:1359/1393 train_time:162012ms step_avg:120.10ms
step:1360/1393 train_time:162143ms step_avg:120.11ms
step:1361/1393 train_time:162273ms step_avg:120.11ms
step:1362/1393 train_time:162400ms step_avg:120.12ms
step:1363/1393 train_time:162528ms step_avg:120.12ms
step:1364/1393 train_time:162659ms step_avg:120.13ms
step:1365/1393 train_time:162785ms step_avg:120.14ms
step:1366/1393 train_time:162910ms step_avg:120.14ms
step:1367/1393 train_time:163036ms step_avg:120.14ms
step:1368/1393 train_time:163161ms step_avg:120.15ms
step:1369/1393 train_time:163287ms step_avg:120.15ms
step:1370/1393 train_time:163414ms step_avg:120.16ms
step:1371/1393 train_time:163540ms step_avg:120.16ms
step:1372/1393 train_time:163667ms step_avg:120.17ms
step:1373/1393 train_time:163792ms step_avg:120.17ms
step:1374/1393 train_time:163918ms step_avg:120.17ms
step:1375/1393 train_time:164044ms step_avg:120.18ms
step:1375/1393 val_loss:3.2813 train_time:164170ms step_avg:120.27ms
step:1376/1393 train_time:164192ms step_avg:120.20ms
step:1377/1393 train_time:164302ms step_avg:120.19ms
step:1378/1393 train_time:164432ms step_avg:120.20ms
step:1379/1393 train_time:164561ms step_avg:120.21ms
step:1380/1393 train_time:164688ms step_avg:120.21ms
step:1381/1393 train_time:164815ms step_avg:120.21ms
step:1382/1393 train_time:164941ms step_avg:120.22ms
step:1383/1393 train_time:165069ms step_avg:120.23ms
step:1384/1393 train_time:165195ms step_avg:120.23ms
step:1385/1393 train_time:165323ms step_avg:120.23ms
step:1386/1393 train_time:165449ms step_avg:120.24ms
step:1387/1393 train_time:165574ms step_avg:120.24ms
step:1388/1393 train_time:165706ms step_avg:120.25ms
step:1389/1393 train_time:165832ms step_avg:120.26ms
step:1390/1393 train_time:165958ms step_avg:120.26ms
step:1391/1393 train_time:166085ms step_avg:120.26ms
step:1392/1393 train_time:166212ms step_avg:120.27ms
step:1393/1393 train_time:166338ms step_avg:120.27ms
step:1393/1393 val_loss:3.2780 train_time:166465ms step_avg:120.37ms
peak memory allocated: 31573 MiB reserved: 32996 MiB
