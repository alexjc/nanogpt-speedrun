import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)
flex_kernel_options = None
if torch.cuda.get_device_name(0).endswith(("3090", "4090")):
    flex_kernel_options = {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_M1": 32, "BLOCK_N1": 64, "BLOCK_M2": 64, "BLOCK_N2": 32}

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale, kernel_options=flex_kernel_options)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor = None, sliding_window_num_blocks: Tensor = 0):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 28415).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x) if not self.training else lm_head_fp8(x, self.lm_head.weight)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)

        if target_seq is None:
            return logits

        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb-tokmon-10B/english-28416-balanced/fineweb-tokmon_train_*.bin" # input .bin to train on
    val_files = "data/fineweb-tokmon-10B/english-28416-balanced/fineweb-tokmon_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # fewer tokens but equivalent text for validation, snapped to nearest seq_len
    val_ratio = 0.99011 # equivalent token density on validation tokens to that of GPT-2
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()


def main():
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

    model = GPT(vocab_size=28416, num_layers=12, num_heads=6, model_dim=768).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    k = 1.08
    adam_params = [dict(params=head_params, lr=0.008*k), dict(params=embed_params, lr=0.6*k), dict(params=scalar_params, lr=0.04*k)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05*k, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model)
    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss = (val_loss * args.val_ratio) / val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
    )
    dist.destroy_process_group()


if __name__ == "__main__":
    main()
====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu124 compiled for CUDA 12.4
Mon Jan 20 17:32:00 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   39C    P0            131W /  700W |    7714MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   42C    P0            122W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   43C    P0            123W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   38C    P0            120W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   44C    P0            126W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   41C    P0            121W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   38C    P0            120W /  700W |    3212MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     44959      C   /usr/bin/python3                             3394MiB |
|    0   N/A  N/A     44960      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     44961      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     44962      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     44963      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     44964      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     44965      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     44966      C   /usr/bin/python3                              610MiB |
|    1   N/A  N/A     44960      C   /usr/bin/python3                             3442MiB |
|    2   N/A  N/A     44961      C   /usr/bin/python3                             3442MiB |
|    3   N/A  N/A     44962      C   /usr/bin/python3                             3442MiB |
|    4   N/A  N/A     44963      C   /usr/bin/python3                             3442MiB |
|    5   N/A  N/A     44964      C   /usr/bin/python3                             3442MiB |
|    6   N/A  N/A     44965      C   /usr/bin/python3                             3442MiB |
|    7   N/A  N/A     44966      C   /usr/bin/python3                             3202MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.1533 train_time:0ms step_avg:nanms
step:1/1393 train_time:20047ms step_avg:nanms
step:2/1393 train_time:20086ms step_avg:nanms
step:3/1393 train_time:20604ms step_avg:nanms
step:4/1393 train_time:20717ms step_avg:nanms
step:5/1393 train_time:20829ms step_avg:nanms
step:6/1393 train_time:20942ms step_avg:nanms
step:7/1393 train_time:21056ms step_avg:nanms
step:8/1393 train_time:21169ms step_avg:nanms
step:9/1393 train_time:21282ms step_avg:nanms
step:10/1393 train_time:21395ms step_avg:nanms
step:11/1393 train_time:113ms step_avg:nanms
step:12/1393 train_time:226ms step_avg:nanms
step:13/1393 train_time:338ms step_avg:112.71ms
step:14/1393 train_time:451ms step_avg:112.72ms
step:15/1393 train_time:564ms step_avg:112.84ms
step:16/1393 train_time:677ms step_avg:112.81ms
step:17/1393 train_time:790ms step_avg:112.84ms
step:18/1393 train_time:903ms step_avg:112.85ms
step:19/1393 train_time:1016ms step_avg:112.86ms
step:20/1393 train_time:1129ms step_avg:112.87ms
step:21/1393 train_time:1242ms step_avg:112.87ms
step:22/1393 train_time:1354ms step_avg:112.83ms
step:23/1393 train_time:1469ms step_avg:112.98ms
step:24/1393 train_time:1581ms step_avg:112.94ms
step:25/1393 train_time:1695ms step_avg:112.98ms
step:26/1393 train_time:1808ms step_avg:112.97ms
step:27/1393 train_time:1921ms step_avg:112.99ms
step:28/1393 train_time:2034ms step_avg:112.98ms
step:29/1393 train_time:2146ms step_avg:112.97ms
step:30/1393 train_time:2260ms step_avg:113.00ms
step:31/1393 train_time:2374ms step_avg:113.02ms
step:32/1393 train_time:2487ms step_avg:113.03ms
step:33/1393 train_time:2600ms step_avg:113.02ms
step:34/1393 train_time:2713ms step_avg:113.02ms
step:35/1393 train_time:2827ms step_avg:113.08ms
step:36/1393 train_time:2940ms step_avg:113.07ms
step:37/1393 train_time:3054ms step_avg:113.13ms
step:38/1393 train_time:3167ms step_avg:113.12ms
step:39/1393 train_time:3282ms step_avg:113.16ms
step:40/1393 train_time:3395ms step_avg:113.16ms
step:41/1393 train_time:3509ms step_avg:113.19ms
step:42/1393 train_time:3622ms step_avg:113.19ms
step:43/1393 train_time:3735ms step_avg:113.19ms
step:44/1393 train_time:3848ms step_avg:113.18ms
step:45/1393 train_time:3962ms step_avg:113.19ms
step:46/1393 train_time:4075ms step_avg:113.20ms
step:47/1393 train_time:4189ms step_avg:113.22ms
step:48/1393 train_time:4302ms step_avg:113.21ms
step:49/1393 train_time:4415ms step_avg:113.21ms
step:50/1393 train_time:4529ms step_avg:113.21ms
step:51/1393 train_time:4642ms step_avg:113.21ms
step:52/1393 train_time:4754ms step_avg:113.20ms
step:53/1393 train_time:4867ms step_avg:113.19ms
step:54/1393 train_time:4981ms step_avg:113.20ms
step:55/1393 train_time:5094ms step_avg:113.19ms
step:56/1393 train_time:5206ms step_avg:113.18ms
step:57/1393 train_time:5320ms step_avg:113.18ms
step:58/1393 train_time:5432ms step_avg:113.17ms
step:59/1393 train_time:5545ms step_avg:113.16ms
step:60/1393 train_time:5658ms step_avg:113.15ms
step:61/1393 train_time:5771ms step_avg:113.15ms
step:62/1393 train_time:5884ms step_avg:113.16ms
step:63/1393 train_time:5998ms step_avg:113.17ms
step:64/1393 train_time:6110ms step_avg:113.15ms
step:65/1393 train_time:6223ms step_avg:113.14ms
step:66/1393 train_time:6337ms step_avg:113.15ms
step:67/1393 train_time:6449ms step_avg:113.14ms
step:68/1393 train_time:6563ms step_avg:113.15ms
step:69/1393 train_time:6676ms step_avg:113.15ms
step:70/1393 train_time:6788ms step_avg:113.14ms
step:71/1393 train_time:6902ms step_avg:113.15ms
step:72/1393 train_time:7016ms step_avg:113.16ms
step:73/1393 train_time:7129ms step_avg:113.16ms
step:74/1393 train_time:7242ms step_avg:113.16ms
step:75/1393 train_time:7356ms step_avg:113.17ms
step:76/1393 train_time:7468ms step_avg:113.16ms
step:77/1393 train_time:7581ms step_avg:113.15ms
step:78/1393 train_time:7694ms step_avg:113.15ms
step:79/1393 train_time:7807ms step_avg:113.14ms
step:80/1393 train_time:7920ms step_avg:113.14ms
step:81/1393 train_time:8033ms step_avg:113.14ms
step:82/1393 train_time:8147ms step_avg:113.15ms
step:83/1393 train_time:8260ms step_avg:113.15ms
step:84/1393 train_time:8373ms step_avg:113.15ms
step:85/1393 train_time:8487ms step_avg:113.16ms
step:86/1393 train_time:8601ms step_avg:113.16ms
step:87/1393 train_time:8714ms step_avg:113.17ms
step:88/1393 train_time:8828ms step_avg:113.17ms
step:89/1393 train_time:8941ms step_avg:113.18ms
step:90/1393 train_time:9054ms step_avg:113.18ms
step:91/1393 train_time:9167ms step_avg:113.17ms
step:92/1393 train_time:9279ms step_avg:113.16ms
step:93/1393 train_time:9392ms step_avg:113.16ms
step:94/1393 train_time:9505ms step_avg:113.16ms
step:95/1393 train_time:9618ms step_avg:113.16ms
step:96/1393 train_time:9731ms step_avg:113.15ms
step:97/1393 train_time:9844ms step_avg:113.15ms
step:98/1393 train_time:9957ms step_avg:113.15ms
step:99/1393 train_time:10071ms step_avg:113.15ms
step:100/1393 train_time:10184ms step_avg:113.15ms
step:101/1393 train_time:10297ms step_avg:113.15ms
step:102/1393 train_time:10410ms step_avg:113.16ms
step:103/1393 train_time:10524ms step_avg:113.16ms
step:104/1393 train_time:10637ms step_avg:113.15ms
step:105/1393 train_time:10749ms step_avg:113.15ms
step:106/1393 train_time:10864ms step_avg:113.16ms
step:107/1393 train_time:10977ms step_avg:113.17ms
step:108/1393 train_time:11091ms step_avg:113.18ms
step:109/1393 train_time:11205ms step_avg:113.18ms
step:110/1393 train_time:11319ms step_avg:113.19ms
step:111/1393 train_time:11432ms step_avg:113.19ms
step:112/1393 train_time:11547ms step_avg:113.20ms
step:113/1393 train_time:11661ms step_avg:113.21ms
step:114/1393 train_time:11775ms step_avg:113.22ms
step:115/1393 train_time:11889ms step_avg:113.23ms
step:116/1393 train_time:12002ms step_avg:113.23ms
step:117/1393 train_time:12116ms step_avg:113.23ms
step:118/1393 train_time:12231ms step_avg:113.25ms
step:119/1393 train_time:12346ms step_avg:113.26ms
step:120/1393 train_time:12459ms step_avg:113.27ms
step:121/1393 train_time:12573ms step_avg:113.27ms
step:122/1393 train_time:12687ms step_avg:113.27ms
step:123/1393 train_time:12800ms step_avg:113.28ms
step:124/1393 train_time:12914ms step_avg:113.28ms
step:125/1393 train_time:13028ms step_avg:113.29ms
step:125/1393 val_loss:4.3438 train_time:13140ms step_avg:114.26ms
step:126/1393 train_time:13165ms step_avg:113.49ms
step:127/1393 train_time:13256ms step_avg:113.30ms
step:128/1393 train_time:13377ms step_avg:113.37ms
step:129/1393 train_time:13493ms step_avg:113.39ms
step:130/1393 train_time:13607ms step_avg:113.39ms
step:131/1393 train_time:13721ms step_avg:113.40ms
step:132/1393 train_time:13835ms step_avg:113.40ms
step:133/1393 train_time:13948ms step_avg:113.40ms
step:134/1393 train_time:14062ms step_avg:113.40ms
step:135/1393 train_time:14175ms step_avg:113.40ms
step:136/1393 train_time:14289ms step_avg:113.40ms
step:137/1393 train_time:14403ms step_avg:113.41ms
step:138/1393 train_time:14516ms step_avg:113.41ms
step:139/1393 train_time:14630ms step_avg:113.41ms
step:140/1393 train_time:14743ms step_avg:113.41ms
step:141/1393 train_time:14857ms step_avg:113.42ms
step:142/1393 train_time:14971ms step_avg:113.42ms
step:143/1393 train_time:15085ms step_avg:113.42ms
step:144/1393 train_time:15199ms step_avg:113.42ms
step:145/1393 train_time:15314ms step_avg:113.43ms
step:146/1393 train_time:15428ms step_avg:113.44ms
step:147/1393 train_time:15542ms step_avg:113.45ms
step:148/1393 train_time:15656ms step_avg:113.45ms
step:149/1393 train_time:15770ms step_avg:113.45ms
step:150/1393 train_time:15883ms step_avg:113.45ms
step:151/1393 train_time:15997ms step_avg:113.45ms
step:152/1393 train_time:16112ms step_avg:113.46ms
step:153/1393 train_time:16226ms step_avg:113.47ms
step:154/1393 train_time:16339ms step_avg:113.47ms
step:155/1393 train_time:16454ms step_avg:113.47ms
step:156/1393 train_time:16568ms step_avg:113.48ms
step:157/1393 train_time:16682ms step_avg:113.48ms
step:158/1393 train_time:16796ms step_avg:113.49ms
step:159/1393 train_time:16911ms step_avg:113.50ms
step:160/1393 train_time:17026ms step_avg:113.51ms
step:161/1393 train_time:17139ms step_avg:113.51ms
step:162/1393 train_time:17253ms step_avg:113.51ms
step:163/1393 train_time:17366ms step_avg:113.51ms
step:164/1393 train_time:17480ms step_avg:113.51ms
step:165/1393 train_time:17594ms step_avg:113.51ms
step:166/1393 train_time:17708ms step_avg:113.51ms
step:167/1393 train_time:17821ms step_avg:113.51ms
step:168/1393 train_time:17935ms step_avg:113.51ms
step:169/1393 train_time:18049ms step_avg:113.51ms
step:170/1393 train_time:18163ms step_avg:113.52ms
step:171/1393 train_time:18276ms step_avg:113.52ms
step:172/1393 train_time:18390ms step_avg:113.52ms
step:173/1393 train_time:18504ms step_avg:113.52ms
step:174/1393 train_time:18619ms step_avg:113.53ms
step:175/1393 train_time:18733ms step_avg:113.53ms
step:176/1393 train_time:18847ms step_avg:113.54ms
step:177/1393 train_time:18961ms step_avg:113.54ms
step:178/1393 train_time:19075ms step_avg:113.54ms
step:179/1393 train_time:19189ms step_avg:113.54ms
step:180/1393 train_time:19302ms step_avg:113.54ms
step:181/1393 train_time:19416ms step_avg:113.54ms
step:182/1393 train_time:19529ms step_avg:113.54ms
step:183/1393 train_time:19643ms step_avg:113.54ms
step:184/1393 train_time:19756ms step_avg:113.54ms
step:185/1393 train_time:19870ms step_avg:113.54ms
step:186/1393 train_time:19983ms step_avg:113.54ms
step:187/1393 train_time:20097ms step_avg:113.54ms
step:188/1393 train_time:20210ms step_avg:113.54ms
step:189/1393 train_time:20324ms step_avg:113.54ms
step:190/1393 train_time:20437ms step_avg:113.54ms
step:191/1393 train_time:20551ms step_avg:113.54ms
step:192/1393 train_time:20665ms step_avg:113.54ms
step:193/1393 train_time:20778ms step_avg:113.54ms
step:194/1393 train_time:20892ms step_avg:113.54ms
step:195/1393 train_time:21005ms step_avg:113.54ms
step:196/1393 train_time:21119ms step_avg:113.54ms
step:197/1393 train_time:21232ms step_avg:113.54ms
step:198/1393 train_time:21346ms step_avg:113.54ms
step:199/1393 train_time:21460ms step_avg:113.54ms
step:200/1393 train_time:21574ms step_avg:113.55ms
step:201/1393 train_time:21688ms step_avg:113.55ms
step:202/1393 train_time:21802ms step_avg:113.55ms
step:203/1393 train_time:21916ms step_avg:113.55ms
step:204/1393 train_time:22029ms step_avg:113.55ms
step:205/1393 train_time:22142ms step_avg:113.55ms
step:206/1393 train_time:22256ms step_avg:113.55ms
step:207/1393 train_time:22370ms step_avg:113.55ms
step:208/1393 train_time:22483ms step_avg:113.55ms
step:209/1393 train_time:22598ms step_avg:113.56ms
step:210/1393 train_time:22712ms step_avg:113.56ms
step:211/1393 train_time:22827ms step_avg:113.56ms
step:212/1393 train_time:22940ms step_avg:113.57ms
step:213/1393 train_time:23055ms step_avg:113.57ms
step:214/1393 train_time:23169ms step_avg:113.57ms
step:215/1393 train_time:23283ms step_avg:113.58ms
step:216/1393 train_time:23398ms step_avg:113.58ms
step:217/1393 train_time:23512ms step_avg:113.59ms
step:218/1393 train_time:23627ms step_avg:113.59ms
step:219/1393 train_time:23741ms step_avg:113.59ms
step:220/1393 train_time:23855ms step_avg:113.60ms
step:221/1393 train_time:23970ms step_avg:113.60ms
step:222/1393 train_time:24083ms step_avg:113.60ms
step:223/1393 train_time:24198ms step_avg:113.61ms
step:224/1393 train_time:24312ms step_avg:113.61ms
step:225/1393 train_time:24426ms step_avg:113.61ms
step:226/1393 train_time:24540ms step_avg:113.61ms
step:227/1393 train_time:24655ms step_avg:113.62ms
step:228/1393 train_time:24768ms step_avg:113.62ms
step:229/1393 train_time:24883ms step_avg:113.62ms
step:230/1393 train_time:24997ms step_avg:113.62ms
step:231/1393 train_time:25111ms step_avg:113.63ms
step:232/1393 train_time:25225ms step_avg:113.63ms
step:233/1393 train_time:25340ms step_avg:113.63ms
step:234/1393 train_time:25454ms step_avg:113.64ms
step:235/1393 train_time:25568ms step_avg:113.64ms
step:236/1393 train_time:25683ms step_avg:113.64ms
step:237/1393 train_time:25797ms step_avg:113.64ms
step:238/1393 train_time:25911ms step_avg:113.65ms
step:239/1393 train_time:26025ms step_avg:113.65ms
step:240/1393 train_time:26139ms step_avg:113.65ms
step:241/1393 train_time:26254ms step_avg:113.65ms
step:242/1393 train_time:26368ms step_avg:113.65ms
step:243/1393 train_time:26482ms step_avg:113.66ms
step:244/1393 train_time:26597ms step_avg:113.66ms
step:245/1393 train_time:26712ms step_avg:113.67ms
step:246/1393 train_time:26826ms step_avg:113.67ms
step:247/1393 train_time:26940ms step_avg:113.67ms
step:248/1393 train_time:27055ms step_avg:113.67ms
step:249/1393 train_time:27169ms step_avg:113.68ms
step:250/1393 train_time:27283ms step_avg:113.68ms
step:250/1393 val_loss:3.9579 train_time:27396ms step_avg:114.15ms
step:251/1393 train_time:27420ms step_avg:113.78ms
step:252/1393 train_time:27513ms step_avg:113.69ms
step:253/1393 train_time:27636ms step_avg:113.73ms
step:254/1393 train_time:27754ms step_avg:113.74ms
step:255/1393 train_time:27868ms step_avg:113.75ms
step:256/1393 train_time:27982ms step_avg:113.75ms
step:257/1393 train_time:28096ms step_avg:113.75ms
step:258/1393 train_time:28211ms step_avg:113.75ms
step:259/1393 train_time:28325ms step_avg:113.76ms
step:260/1393 train_time:28440ms step_avg:113.76ms
step:261/1393 train_time:28554ms step_avg:113.76ms
step:262/1393 train_time:28668ms step_avg:113.76ms
step:263/1393 train_time:28782ms step_avg:113.76ms
step:264/1393 train_time:28897ms step_avg:113.77ms
step:265/1393 train_time:29011ms step_avg:113.77ms
step:266/1393 train_time:29126ms step_avg:113.77ms
step:267/1393 train_time:29239ms step_avg:113.77ms
step:268/1393 train_time:29354ms step_avg:113.77ms
step:269/1393 train_time:29468ms step_avg:113.78ms
step:270/1393 train_time:29582ms step_avg:113.78ms
step:271/1393 train_time:29696ms step_avg:113.78ms
step:272/1393 train_time:29810ms step_avg:113.78ms
step:273/1393 train_time:29924ms step_avg:113.78ms
step:274/1393 train_time:30039ms step_avg:113.78ms
step:275/1393 train_time:30153ms step_avg:113.78ms
step:276/1393 train_time:30267ms step_avg:113.79ms
step:277/1393 train_time:30381ms step_avg:113.79ms
step:278/1393 train_time:30496ms step_avg:113.79ms
step:279/1393 train_time:30610ms step_avg:113.79ms
step:280/1393 train_time:30724ms step_avg:113.79ms
step:281/1393 train_time:30838ms step_avg:113.79ms
step:282/1393 train_time:30952ms step_avg:113.80ms
step:283/1393 train_time:31066ms step_avg:113.80ms
step:284/1393 train_time:31180ms step_avg:113.80ms
step:285/1393 train_time:31294ms step_avg:113.80ms
step:286/1393 train_time:31409ms step_avg:113.80ms
step:287/1393 train_time:31523ms step_avg:113.80ms
step:288/1393 train_time:31637ms step_avg:113.80ms
step:289/1393 train_time:31751ms step_avg:113.80ms
step:290/1393 train_time:31865ms step_avg:113.80ms
step:291/1393 train_time:31980ms step_avg:113.81ms
step:292/1393 train_time:32094ms step_avg:113.81ms
step:293/1393 train_time:32209ms step_avg:113.81ms
step:294/1393 train_time:32323ms step_avg:113.81ms
step:295/1393 train_time:32437ms step_avg:113.82ms
step:296/1393 train_time:32551ms step_avg:113.82ms
step:297/1393 train_time:32665ms step_avg:113.81ms
step:298/1393 train_time:32779ms step_avg:113.82ms
step:299/1393 train_time:32893ms step_avg:113.82ms
step:300/1393 train_time:33006ms step_avg:113.81ms
step:301/1393 train_time:33120ms step_avg:113.82ms
step:302/1393 train_time:33234ms step_avg:113.82ms
step:303/1393 train_time:33349ms step_avg:113.82ms
step:304/1393 train_time:33462ms step_avg:113.82ms
step:305/1393 train_time:33577ms step_avg:113.82ms
step:306/1393 train_time:33691ms step_avg:113.82ms
step:307/1393 train_time:33805ms step_avg:113.82ms
step:308/1393 train_time:33921ms step_avg:113.83ms
step:309/1393 train_time:34036ms step_avg:113.83ms
step:310/1393 train_time:34150ms step_avg:113.83ms
step:311/1393 train_time:34263ms step_avg:113.83ms
step:312/1393 train_time:34381ms step_avg:113.84ms
step:313/1393 train_time:34499ms step_avg:113.86ms
step:314/1393 train_time:34614ms step_avg:113.86ms
step:315/1393 train_time:34730ms step_avg:113.87ms
step:316/1393 train_time:34847ms step_avg:113.88ms
step:317/1393 train_time:34963ms step_avg:113.89ms
step:318/1393 train_time:35081ms step_avg:113.90ms
step:319/1393 train_time:35198ms step_avg:113.91ms
step:320/1393 train_time:35314ms step_avg:113.92ms
step:321/1393 train_time:35430ms step_avg:113.92ms
step:322/1393 train_time:35548ms step_avg:113.94ms
step:323/1393 train_time:35666ms step_avg:113.95ms
step:324/1393 train_time:35783ms step_avg:113.96ms
step:325/1393 train_time:35900ms step_avg:113.97ms
step:326/1393 train_time:36017ms step_avg:113.98ms
step:327/1393 train_time:36134ms step_avg:113.99ms
step:328/1393 train_time:36251ms step_avg:114.00ms
step:329/1393 train_time:36367ms step_avg:114.00ms
step:330/1393 train_time:36484ms step_avg:114.01ms
step:331/1393 train_time:36602ms step_avg:114.03ms
step:332/1393 train_time:36719ms step_avg:114.03ms
step:333/1393 train_time:36836ms step_avg:114.04ms
step:334/1393 train_time:36953ms step_avg:114.05ms
step:335/1393 train_time:37070ms step_avg:114.06ms
step:336/1393 train_time:37187ms step_avg:114.07ms
step:337/1393 train_time:37304ms step_avg:114.08ms
step:338/1393 train_time:37421ms step_avg:114.09ms
step:339/1393 train_time:37539ms step_avg:114.10ms
step:340/1393 train_time:37656ms step_avg:114.11ms
step:341/1393 train_time:37773ms step_avg:114.12ms
step:342/1393 train_time:37889ms step_avg:114.12ms
step:343/1393 train_time:38006ms step_avg:114.13ms
step:344/1393 train_time:38123ms step_avg:114.14ms
step:345/1393 train_time:38239ms step_avg:114.15ms
step:346/1393 train_time:38356ms step_avg:114.15ms
step:347/1393 train_time:38473ms step_avg:114.16ms
step:348/1393 train_time:38589ms step_avg:114.17ms
step:349/1393 train_time:38706ms step_avg:114.18ms
step:350/1393 train_time:38822ms step_avg:114.18ms
step:351/1393 train_time:38939ms step_avg:114.19ms
step:352/1393 train_time:39056ms step_avg:114.20ms
step:353/1393 train_time:39172ms step_avg:114.21ms
step:354/1393 train_time:39289ms step_avg:114.21ms
step:355/1393 train_time:39407ms step_avg:114.22ms
step:356/1393 train_time:39524ms step_avg:114.23ms
step:357/1393 train_time:39640ms step_avg:114.24ms
step:358/1393 train_time:39757ms step_avg:114.24ms
step:359/1393 train_time:39874ms step_avg:114.25ms
step:360/1393 train_time:39991ms step_avg:114.26ms
step:361/1393 train_time:40108ms step_avg:114.27ms
step:362/1393 train_time:40225ms step_avg:114.27ms
step:363/1393 train_time:40342ms step_avg:114.28ms
step:364/1393 train_time:40458ms step_avg:114.29ms
step:365/1393 train_time:40574ms step_avg:114.29ms
step:366/1393 train_time:40691ms step_avg:114.30ms
step:367/1393 train_time:40808ms step_avg:114.31ms
step:368/1393 train_time:40927ms step_avg:114.32ms
step:369/1393 train_time:41044ms step_avg:114.33ms
step:370/1393 train_time:41161ms step_avg:114.34ms
step:371/1393 train_time:41279ms step_avg:114.35ms
step:372/1393 train_time:41396ms step_avg:114.35ms
step:373/1393 train_time:41513ms step_avg:114.36ms
step:374/1393 train_time:41630ms step_avg:114.37ms
step:375/1393 train_time:41747ms step_avg:114.37ms
step:375/1393 val_loss:3.7668 train_time:41861ms step_avg:114.69ms
step:376/1393 train_time:41886ms step_avg:114.44ms
step:377/1393 train_time:41980ms step_avg:114.39ms
step:378/1393 train_time:42107ms step_avg:114.42ms
step:379/1393 train_time:42227ms step_avg:114.44ms
step:380/1393 train_time:42344ms step_avg:114.44ms
step:381/1393 train_time:42461ms step_avg:114.45ms
step:382/1393 train_time:42577ms step_avg:114.45ms
step:383/1393 train_time:42693ms step_avg:114.46ms
step:384/1393 train_time:42811ms step_avg:114.47ms
step:385/1393 train_time:42928ms step_avg:114.47ms
step:386/1393 train_time:43044ms step_avg:114.48ms
step:387/1393 train_time:43162ms step_avg:114.49ms
step:388/1393 train_time:43278ms step_avg:114.49ms
step:389/1393 train_time:43396ms step_avg:114.50ms
step:390/1393 train_time:43513ms step_avg:114.51ms
step:391/1393 train_time:43630ms step_avg:114.51ms
step:392/1393 train_time:43747ms step_avg:114.52ms
step:393/1393 train_time:43864ms step_avg:114.53ms
step:394/1393 train_time:43981ms step_avg:114.53ms
step:395/1393 train_time:44099ms step_avg:114.54ms
step:396/1393 train_time:44218ms step_avg:114.55ms
step:397/1393 train_time:44335ms step_avg:114.56ms
step:398/1393 train_time:44452ms step_avg:114.57ms
step:399/1393 train_time:44570ms step_avg:114.58ms
step:400/1393 train_time:44686ms step_avg:114.58ms
step:401/1393 train_time:44803ms step_avg:114.58ms
step:402/1393 train_time:44920ms step_avg:114.59ms
step:403/1393 train_time:45038ms step_avg:114.60ms
step:404/1393 train_time:45156ms step_avg:114.61ms
step:405/1393 train_time:45274ms step_avg:114.62ms
step:406/1393 train_time:45390ms step_avg:114.62ms
step:407/1393 train_time:45507ms step_avg:114.63ms
step:408/1393 train_time:45624ms step_avg:114.63ms
step:409/1393 train_time:45742ms step_avg:114.64ms
step:410/1393 train_time:45858ms step_avg:114.65ms
step:411/1393 train_time:45975ms step_avg:114.65ms
step:412/1393 train_time:46092ms step_avg:114.66ms
step:413/1393 train_time:46209ms step_avg:114.66ms
step:414/1393 train_time:46326ms step_avg:114.67ms
step:415/1393 train_time:46444ms step_avg:114.68ms
step:416/1393 train_time:46561ms step_avg:114.68ms
step:417/1393 train_time:46679ms step_avg:114.69ms
step:418/1393 train_time:46797ms step_avg:114.70ms
step:419/1393 train_time:46915ms step_avg:114.71ms
step:420/1393 train_time:47032ms step_avg:114.71ms
step:421/1393 train_time:47150ms step_avg:114.72ms
step:422/1393 train_time:47267ms step_avg:114.73ms
step:423/1393 train_time:47385ms step_avg:114.73ms
step:424/1393 train_time:47501ms step_avg:114.74ms
step:425/1393 train_time:47618ms step_avg:114.74ms
step:426/1393 train_time:47736ms step_avg:114.75ms
step:427/1393 train_time:47854ms step_avg:114.76ms
step:428/1393 train_time:47970ms step_avg:114.76ms
step:429/1393 train_time:48087ms step_avg:114.77ms
step:430/1393 train_time:48204ms step_avg:114.77ms
step:431/1393 train_time:48323ms step_avg:114.78ms
step:432/1393 train_time:48441ms step_avg:114.79ms
step:433/1393 train_time:48557ms step_avg:114.79ms
step:434/1393 train_time:48675ms step_avg:114.80ms
step:435/1393 train_time:48792ms step_avg:114.81ms
step:436/1393 train_time:48910ms step_avg:114.81ms
step:437/1393 train_time:49027ms step_avg:114.82ms
step:438/1393 train_time:49144ms step_avg:114.82ms
step:439/1393 train_time:49260ms step_avg:114.83ms
step:440/1393 train_time:49378ms step_avg:114.83ms
step:441/1393 train_time:49495ms step_avg:114.84ms
step:442/1393 train_time:49613ms step_avg:114.85ms
step:443/1393 train_time:49732ms step_avg:114.85ms
step:444/1393 train_time:49850ms step_avg:114.86ms
step:445/1393 train_time:49968ms step_avg:114.87ms
step:446/1393 train_time:50086ms step_avg:114.88ms
step:447/1393 train_time:50203ms step_avg:114.88ms
step:448/1393 train_time:50320ms step_avg:114.88ms
step:449/1393 train_time:50437ms step_avg:114.89ms
step:450/1393 train_time:50554ms step_avg:114.89ms
step:451/1393 train_time:50671ms step_avg:114.90ms
step:452/1393 train_time:50788ms step_avg:114.91ms
step:453/1393 train_time:50906ms step_avg:114.91ms
step:454/1393 train_time:51023ms step_avg:114.92ms
step:455/1393 train_time:51142ms step_avg:114.93ms
step:456/1393 train_time:51260ms step_avg:114.93ms
step:457/1393 train_time:51378ms step_avg:114.94ms
step:458/1393 train_time:51496ms step_avg:114.95ms
step:459/1393 train_time:51613ms step_avg:114.95ms
step:460/1393 train_time:51731ms step_avg:114.96ms
step:461/1393 train_time:51848ms step_avg:114.96ms
step:462/1393 train_time:51966ms step_avg:114.97ms
step:463/1393 train_time:52083ms step_avg:114.97ms
step:464/1393 train_time:52201ms step_avg:114.98ms
step:465/1393 train_time:52318ms step_avg:114.98ms
step:466/1393 train_time:52435ms step_avg:114.99ms
step:467/1393 train_time:52552ms step_avg:114.99ms
step:468/1393 train_time:52669ms step_avg:115.00ms
step:469/1393 train_time:52786ms step_avg:115.00ms
step:470/1393 train_time:52903ms step_avg:115.01ms
step:471/1393 train_time:53021ms step_avg:115.01ms
step:472/1393 train_time:53140ms step_avg:115.02ms
step:473/1393 train_time:53257ms step_avg:115.02ms
step:474/1393 train_time:53374ms step_avg:115.03ms
step:475/1393 train_time:53492ms step_avg:115.04ms
step:476/1393 train_time:53608ms step_avg:115.04ms
step:477/1393 train_time:53726ms step_avg:115.05ms
step:478/1393 train_time:53843ms step_avg:115.05ms
step:479/1393 train_time:53960ms step_avg:115.05ms
step:480/1393 train_time:54077ms step_avg:115.06ms
step:481/1393 train_time:54195ms step_avg:115.06ms
step:482/1393 train_time:54313ms step_avg:115.07ms
step:483/1393 train_time:54431ms step_avg:115.08ms
step:484/1393 train_time:54548ms step_avg:115.08ms
step:485/1393 train_time:54664ms step_avg:115.08ms
step:486/1393 train_time:54782ms step_avg:115.09ms
step:487/1393 train_time:54899ms step_avg:115.09ms
step:488/1393 train_time:55016ms step_avg:115.10ms
step:489/1393 train_time:55135ms step_avg:115.10ms
step:490/1393 train_time:55252ms step_avg:115.11ms
step:491/1393 train_time:55369ms step_avg:115.11ms
step:492/1393 train_time:55486ms step_avg:115.12ms
step:493/1393 train_time:55603ms step_avg:115.12ms
step:494/1393 train_time:55720ms step_avg:115.12ms
step:495/1393 train_time:55839ms step_avg:115.13ms
step:496/1393 train_time:55956ms step_avg:115.14ms
step:497/1393 train_time:56073ms step_avg:115.14ms
step:498/1393 train_time:56191ms step_avg:115.15ms
step:499/1393 train_time:56308ms step_avg:115.15ms
step:500/1393 train_time:56426ms step_avg:115.15ms
step:500/1393 val_loss:3.6508 train_time:56542ms step_avg:115.39ms
step:501/1393 train_time:56566ms step_avg:115.21ms
step:502/1393 train_time:56663ms step_avg:115.17ms
step:503/1393 train_time:56787ms step_avg:115.19ms
step:504/1393 train_time:56907ms step_avg:115.20ms
step:505/1393 train_time:57024ms step_avg:115.20ms
step:506/1393 train_time:57141ms step_avg:115.20ms
step:507/1393 train_time:57259ms step_avg:115.21ms
step:508/1393 train_time:57376ms step_avg:115.21ms
step:509/1393 train_time:57494ms step_avg:115.22ms
step:510/1393 train_time:57611ms step_avg:115.22ms
step:511/1393 train_time:57728ms step_avg:115.23ms
step:512/1393 train_time:57847ms step_avg:115.23ms
step:513/1393 train_time:57965ms step_avg:115.24ms
step:514/1393 train_time:58082ms step_avg:115.24ms
step:515/1393 train_time:58200ms step_avg:115.25ms
step:516/1393 train_time:58317ms step_avg:115.25ms
step:517/1393 train_time:58434ms step_avg:115.25ms
step:518/1393 train_time:58554ms step_avg:115.26ms
step:519/1393 train_time:58673ms step_avg:115.27ms
step:520/1393 train_time:58792ms step_avg:115.28ms
step:521/1393 train_time:58911ms step_avg:115.29ms
step:522/1393 train_time:59032ms step_avg:115.30ms
step:523/1393 train_time:59151ms step_avg:115.30ms
step:524/1393 train_time:59271ms step_avg:115.31ms
step:525/1393 train_time:59391ms step_avg:115.32ms
step:526/1393 train_time:59510ms step_avg:115.33ms
step:527/1393 train_time:59631ms step_avg:115.34ms
step:528/1393 train_time:59750ms step_avg:115.35ms
step:529/1393 train_time:59871ms step_avg:115.36ms
step:530/1393 train_time:59991ms step_avg:115.37ms
step:531/1393 train_time:60111ms step_avg:115.38ms
step:532/1393 train_time:60233ms step_avg:115.39ms
step:533/1393 train_time:60353ms step_avg:115.40ms
step:534/1393 train_time:60472ms step_avg:115.40ms
step:535/1393 train_time:60591ms step_avg:115.41ms
step:536/1393 train_time:60710ms step_avg:115.42ms
step:537/1393 train_time:60829ms step_avg:115.43ms
step:538/1393 train_time:60950ms step_avg:115.43ms
step:539/1393 train_time:61070ms step_avg:115.44ms
step:540/1393 train_time:61191ms step_avg:115.45ms
step:541/1393 train_time:61311ms step_avg:115.46ms
step:542/1393 train_time:61432ms step_avg:115.47ms
step:543/1393 train_time:61552ms step_avg:115.48ms
step:544/1393 train_time:61672ms step_avg:115.49ms
step:545/1393 train_time:61791ms step_avg:115.50ms
step:546/1393 train_time:61911ms step_avg:115.51ms
step:547/1393 train_time:62030ms step_avg:115.51ms
step:548/1393 train_time:62150ms step_avg:115.52ms
step:549/1393 train_time:62270ms step_avg:115.53ms
step:550/1393 train_time:62390ms step_avg:115.54ms
step:551/1393 train_time:62512ms step_avg:115.55ms
step:552/1393 train_time:62631ms step_avg:115.56ms
step:553/1393 train_time:62750ms step_avg:115.56ms
step:554/1393 train_time:62870ms step_avg:115.57ms
step:555/1393 train_time:62989ms step_avg:115.58ms
step:556/1393 train_time:63109ms step_avg:115.58ms
step:557/1393 train_time:63229ms step_avg:115.59ms
step:558/1393 train_time:63348ms step_avg:115.60ms
step:559/1393 train_time:63470ms step_avg:115.61ms
step:560/1393 train_time:63591ms step_avg:115.62ms
step:561/1393 train_time:63710ms step_avg:115.63ms
step:562/1393 train_time:63831ms step_avg:115.64ms
step:563/1393 train_time:63950ms step_avg:115.64ms
step:564/1393 train_time:64069ms step_avg:115.65ms
step:565/1393 train_time:64188ms step_avg:115.65ms
step:566/1393 train_time:64307ms step_avg:115.66ms
step:567/1393 train_time:64427ms step_avg:115.67ms
step:568/1393 train_time:64549ms step_avg:115.68ms
step:569/1393 train_time:64669ms step_avg:115.69ms
step:570/1393 train_time:64788ms step_avg:115.69ms
step:571/1393 train_time:64908ms step_avg:115.70ms
step:572/1393 train_time:65027ms step_avg:115.71ms
step:573/1393 train_time:65148ms step_avg:115.71ms
step:574/1393 train_time:65267ms step_avg:115.72ms
step:575/1393 train_time:65387ms step_avg:115.73ms
step:576/1393 train_time:65507ms step_avg:115.74ms
step:577/1393 train_time:65626ms step_avg:115.74ms
step:578/1393 train_time:65745ms step_avg:115.75ms
step:579/1393 train_time:65865ms step_avg:115.76ms
step:580/1393 train_time:65984ms step_avg:115.76ms
step:581/1393 train_time:66104ms step_avg:115.77ms
step:582/1393 train_time:66224ms step_avg:115.78ms
step:583/1393 train_time:66344ms step_avg:115.78ms
step:584/1393 train_time:66464ms step_avg:115.79ms
step:585/1393 train_time:66584ms step_avg:115.80ms
step:586/1393 train_time:66703ms step_avg:115.80ms
step:587/1393 train_time:66822ms step_avg:115.81ms
step:588/1393 train_time:66942ms step_avg:115.82ms
step:589/1393 train_time:67061ms step_avg:115.82ms
step:590/1393 train_time:67181ms step_avg:115.83ms
step:591/1393 train_time:67300ms step_avg:115.84ms
step:592/1393 train_time:67420ms step_avg:115.84ms
step:593/1393 train_time:67541ms step_avg:115.85ms
step:594/1393 train_time:67661ms step_avg:115.86ms
step:595/1393 train_time:67781ms step_avg:115.86ms
step:596/1393 train_time:67900ms step_avg:115.87ms
step:597/1393 train_time:68018ms step_avg:115.87ms
step:598/1393 train_time:68138ms step_avg:115.88ms
step:599/1393 train_time:68257ms step_avg:115.89ms
step:600/1393 train_time:68376ms step_avg:115.89ms
step:601/1393 train_time:68497ms step_avg:115.90ms
step:602/1393 train_time:68616ms step_avg:115.91ms
step:603/1393 train_time:68736ms step_avg:115.91ms
step:604/1393 train_time:68856ms step_avg:115.92ms
step:605/1393 train_time:68975ms step_avg:115.92ms
step:606/1393 train_time:69095ms step_avg:115.93ms
step:607/1393 train_time:69215ms step_avg:115.94ms
step:608/1393 train_time:69334ms step_avg:115.94ms
step:609/1393 train_time:69454ms step_avg:115.95ms
step:610/1393 train_time:69574ms step_avg:115.96ms
step:611/1393 train_time:69693ms step_avg:115.96ms
step:612/1393 train_time:69814ms step_avg:115.97ms
step:613/1393 train_time:69934ms step_avg:115.98ms
step:614/1393 train_time:70055ms step_avg:115.98ms
step:615/1393 train_time:70174ms step_avg:115.99ms
step:616/1393 train_time:70294ms step_avg:116.00ms
step:617/1393 train_time:70415ms step_avg:116.00ms
step:618/1393 train_time:70535ms step_avg:116.01ms
step:619/1393 train_time:70655ms step_avg:116.02ms
step:620/1393 train_time:70775ms step_avg:116.02ms
step:621/1393 train_time:70894ms step_avg:116.03ms
step:622/1393 train_time:71014ms step_avg:116.04ms
step:623/1393 train_time:71133ms step_avg:116.04ms
step:624/1393 train_time:71253ms step_avg:116.05ms
step:625/1393 train_time:71373ms step_avg:116.05ms
step:625/1393 val_loss:3.5671 train_time:71490ms step_avg:116.24ms
step:626/1393 train_time:71515ms step_avg:116.10ms
step:627/1393 train_time:71614ms step_avg:116.07ms
step:628/1393 train_time:71741ms step_avg:116.09ms
step:629/1393 train_time:71863ms step_avg:116.10ms
step:630/1393 train_time:71983ms step_avg:116.10ms
step:631/1393 train_time:72103ms step_avg:116.11ms
step:632/1393 train_time:72222ms step_avg:116.11ms
step:633/1393 train_time:72342ms step_avg:116.12ms
step:634/1393 train_time:72462ms step_avg:116.12ms
step:635/1393 train_time:72582ms step_avg:116.13ms
step:636/1393 train_time:72702ms step_avg:116.14ms
step:637/1393 train_time:72822ms step_avg:116.14ms
step:638/1393 train_time:72943ms step_avg:116.15ms
step:639/1393 train_time:73063ms step_avg:116.16ms
step:640/1393 train_time:73183ms step_avg:116.16ms
step:641/1393 train_time:73303ms step_avg:116.17ms
step:642/1393 train_time:73423ms step_avg:116.18ms
step:643/1393 train_time:73543ms step_avg:116.18ms
step:644/1393 train_time:73663ms step_avg:116.19ms
step:645/1393 train_time:73783ms step_avg:116.19ms
step:646/1393 train_time:73904ms step_avg:116.20ms
step:647/1393 train_time:74025ms step_avg:116.21ms
step:648/1393 train_time:74145ms step_avg:116.21ms
step:649/1393 train_time:74265ms step_avg:116.22ms
step:650/1393 train_time:74385ms step_avg:116.23ms
step:651/1393 train_time:74506ms step_avg:116.23ms
step:652/1393 train_time:74628ms step_avg:116.24ms
step:653/1393 train_time:74749ms step_avg:116.25ms
step:654/1393 train_time:74868ms step_avg:116.25ms
step:655/1393 train_time:74988ms step_avg:116.26ms
step:656/1393 train_time:75107ms step_avg:116.26ms
step:657/1393 train_time:75226ms step_avg:116.27ms
step:658/1393 train_time:75346ms step_avg:116.27ms
step:659/1393 train_time:75466ms step_avg:116.28ms
step:660/1393 train_time:75585ms step_avg:116.28ms
step:661/1393 train_time:75706ms step_avg:116.29ms
step:662/1393 train_time:75827ms step_avg:116.30ms
step:663/1393 train_time:75946ms step_avg:116.30ms
step:664/1393 train_time:76067ms step_avg:116.31ms
step:665/1393 train_time:76186ms step_avg:116.31ms
step:666/1393 train_time:76306ms step_avg:116.32ms
step:667/1393 train_time:76426ms step_avg:116.33ms
step:668/1393 train_time:76547ms step_avg:116.33ms
step:669/1393 train_time:76667ms step_avg:116.34ms
step:670/1393 train_time:76788ms step_avg:116.35ms
step:671/1393 train_time:76907ms step_avg:116.35ms
step:672/1393 train_time:77026ms step_avg:116.35ms
step:673/1393 train_time:77146ms step_avg:116.36ms
step:674/1393 train_time:77266ms step_avg:116.36ms
step:675/1393 train_time:77386ms step_avg:116.37ms
step:676/1393 train_time:77505ms step_avg:116.37ms
step:677/1393 train_time:77626ms step_avg:116.38ms
step:678/1393 train_time:77746ms step_avg:116.39ms
step:679/1393 train_time:77865ms step_avg:116.39ms
step:680/1393 train_time:77986ms step_avg:116.40ms
step:681/1393 train_time:78105ms step_avg:116.40ms
step:682/1393 train_time:78225ms step_avg:116.41ms
step:683/1393 train_time:78345ms step_avg:116.41ms
step:684/1393 train_time:78465ms step_avg:116.42ms
step:685/1393 train_time:78584ms step_avg:116.42ms
step:686/1393 train_time:78705ms step_avg:116.43ms
step:687/1393 train_time:78825ms step_avg:116.43ms
step:688/1393 train_time:78946ms step_avg:116.44ms
step:689/1393 train_time:79066ms step_avg:116.45ms
step:690/1393 train_time:79185ms step_avg:116.45ms
step:691/1393 train_time:79305ms step_avg:116.45ms
step:692/1393 train_time:79425ms step_avg:116.46ms
step:693/1393 train_time:79545ms step_avg:116.46ms
step:694/1393 train_time:79665ms step_avg:116.47ms
step:695/1393 train_time:79785ms step_avg:116.47ms
step:696/1393 train_time:79906ms step_avg:116.48ms
step:697/1393 train_time:80025ms step_avg:116.48ms
step:698/1393 train_time:80145ms step_avg:116.49ms
step:699/1393 train_time:80264ms step_avg:116.49ms
step:700/1393 train_time:80384ms step_avg:116.50ms
step:701/1393 train_time:80505ms step_avg:116.51ms
step:702/1393 train_time:80625ms step_avg:116.51ms
step:703/1393 train_time:80744ms step_avg:116.51ms
step:704/1393 train_time:80865ms step_avg:116.52ms
step:705/1393 train_time:80986ms step_avg:116.53ms
step:706/1393 train_time:81106ms step_avg:116.53ms
step:707/1393 train_time:81227ms step_avg:116.54ms
step:708/1393 train_time:81347ms step_avg:116.54ms
step:709/1393 train_time:81467ms step_avg:116.55ms
step:710/1393 train_time:81587ms step_avg:116.55ms
step:711/1393 train_time:81708ms step_avg:116.56ms
step:712/1393 train_time:81828ms step_avg:116.56ms
step:713/1393 train_time:81950ms step_avg:116.57ms
step:714/1393 train_time:82070ms step_avg:116.58ms
step:715/1393 train_time:82190ms step_avg:116.58ms
step:716/1393 train_time:82312ms step_avg:116.59ms
step:717/1393 train_time:82431ms step_avg:116.59ms
step:718/1393 train_time:82551ms step_avg:116.60ms
step:719/1393 train_time:82671ms step_avg:116.60ms
step:720/1393 train_time:82791ms step_avg:116.61ms
step:721/1393 train_time:82911ms step_avg:116.61ms
step:722/1393 train_time:83031ms step_avg:116.62ms
step:723/1393 train_time:83152ms step_avg:116.62ms
step:724/1393 train_time:83272ms step_avg:116.63ms
step:725/1393 train_time:83393ms step_avg:116.63ms
step:726/1393 train_time:83514ms step_avg:116.64ms
step:727/1393 train_time:83635ms step_avg:116.65ms
step:728/1393 train_time:83757ms step_avg:116.65ms
step:729/1393 train_time:83878ms step_avg:116.66ms
step:730/1393 train_time:84001ms step_avg:116.67ms
step:731/1393 train_time:84123ms step_avg:116.68ms
step:732/1393 train_time:84245ms step_avg:116.68ms
step:733/1393 train_time:84366ms step_avg:116.69ms
step:734/1393 train_time:84488ms step_avg:116.70ms
step:735/1393 train_time:84609ms step_avg:116.70ms
step:736/1393 train_time:84732ms step_avg:116.71ms
step:737/1393 train_time:84853ms step_avg:116.72ms
step:738/1393 train_time:84975ms step_avg:116.72ms
step:739/1393 train_time:85097ms step_avg:116.73ms
step:740/1393 train_time:85218ms step_avg:116.74ms
step:741/1393 train_time:85341ms step_avg:116.75ms
step:742/1393 train_time:85461ms step_avg:116.75ms
step:743/1393 train_time:85584ms step_avg:116.76ms
step:744/1393 train_time:85705ms step_avg:116.76ms
step:745/1393 train_time:85826ms step_avg:116.77ms
step:746/1393 train_time:85948ms step_avg:116.78ms
step:747/1393 train_time:86070ms step_avg:116.78ms
step:748/1393 train_time:86192ms step_avg:116.79ms
step:749/1393 train_time:86313ms step_avg:116.80ms
step:750/1393 train_time:86435ms step_avg:116.80ms
step:750/1393 val_loss:3.5163 train_time:86555ms step_avg:116.97ms
step:751/1393 train_time:86579ms step_avg:116.84ms
step:752/1393 train_time:86681ms step_avg:116.82ms
step:753/1393 train_time:86812ms step_avg:116.84ms
step:754/1393 train_time:86937ms step_avg:116.85ms
step:755/1393 train_time:87057ms step_avg:116.86ms
step:756/1393 train_time:87179ms step_avg:116.86ms
step:757/1393 train_time:87300ms step_avg:116.87ms
step:758/1393 train_time:87423ms step_avg:116.88ms
step:759/1393 train_time:87544ms step_avg:116.88ms
step:760/1393 train_time:87666ms step_avg:116.89ms
step:761/1393 train_time:87787ms step_avg:116.89ms
step:762/1393 train_time:87908ms step_avg:116.90ms
step:763/1393 train_time:88030ms step_avg:116.91ms
step:764/1393 train_time:88152ms step_avg:116.91ms
step:765/1393 train_time:88274ms step_avg:116.92ms
step:766/1393 train_time:88397ms step_avg:116.93ms
step:767/1393 train_time:88519ms step_avg:116.93ms
step:768/1393 train_time:88640ms step_avg:116.94ms
step:769/1393 train_time:88762ms step_avg:116.95ms
step:770/1393 train_time:88883ms step_avg:116.95ms
step:771/1393 train_time:89004ms step_avg:116.96ms
step:772/1393 train_time:89126ms step_avg:116.96ms
step:773/1393 train_time:89249ms step_avg:116.97ms
step:774/1393 train_time:89370ms step_avg:116.98ms
step:775/1393 train_time:89492ms step_avg:116.98ms
step:776/1393 train_time:89614ms step_avg:116.99ms
step:777/1393 train_time:89736ms step_avg:117.00ms
step:778/1393 train_time:89857ms step_avg:117.00ms
step:779/1393 train_time:89978ms step_avg:117.01ms
step:780/1393 train_time:90099ms step_avg:117.01ms
step:781/1393 train_time:90221ms step_avg:117.02ms
step:782/1393 train_time:90344ms step_avg:117.03ms
step:783/1393 train_time:90467ms step_avg:117.03ms
step:784/1393 train_time:90589ms step_avg:117.04ms
step:785/1393 train_time:90710ms step_avg:117.05ms
step:786/1393 train_time:90832ms step_avg:117.05ms
step:787/1393 train_time:90954ms step_avg:117.06ms
step:788/1393 train_time:91076ms step_avg:117.06ms
step:789/1393 train_time:91197ms step_avg:117.07ms
step:790/1393 train_time:91319ms step_avg:117.08ms
step:791/1393 train_time:91441ms step_avg:117.08ms
step:792/1393 train_time:91562ms step_avg:117.09ms
step:793/1393 train_time:91685ms step_avg:117.09ms
step:794/1393 train_time:91808ms step_avg:117.10ms
step:795/1393 train_time:91929ms step_avg:117.11ms
step:796/1393 train_time:92050ms step_avg:117.11ms
step:797/1393 train_time:92172ms step_avg:117.12ms
step:798/1393 train_time:92296ms step_avg:117.13ms
step:799/1393 train_time:92418ms step_avg:117.13ms
step:800/1393 train_time:92539ms step_avg:117.14ms
step:801/1393 train_time:92660ms step_avg:117.14ms
step:802/1393 train_time:92781ms step_avg:117.15ms
step:803/1393 train_time:92903ms step_avg:117.15ms
step:804/1393 train_time:93024ms step_avg:117.16ms
step:805/1393 train_time:93146ms step_avg:117.16ms
step:806/1393 train_time:93267ms step_avg:117.17ms
step:807/1393 train_time:93390ms step_avg:117.18ms
step:808/1393 train_time:93513ms step_avg:117.18ms
step:809/1393 train_time:93635ms step_avg:117.19ms
step:810/1393 train_time:93756ms step_avg:117.20ms
step:811/1393 train_time:93877ms step_avg:117.20ms
step:812/1393 train_time:93998ms step_avg:117.20ms
step:813/1393 train_time:94120ms step_avg:117.21ms
step:814/1393 train_time:94242ms step_avg:117.22ms
step:815/1393 train_time:94363ms step_avg:117.22ms
step:816/1393 train_time:94484ms step_avg:117.23ms
step:817/1393 train_time:94606ms step_avg:117.23ms
step:818/1393 train_time:94728ms step_avg:117.24ms
step:819/1393 train_time:94850ms step_avg:117.24ms
step:820/1393 train_time:94972ms step_avg:117.25ms
step:821/1393 train_time:95095ms step_avg:117.26ms
step:822/1393 train_time:95218ms step_avg:117.26ms
step:823/1393 train_time:95340ms step_avg:117.27ms
step:824/1393 train_time:95462ms step_avg:117.28ms
step:825/1393 train_time:95584ms step_avg:117.28ms
step:826/1393 train_time:95705ms step_avg:117.29ms
step:827/1393 train_time:95826ms step_avg:117.29ms
step:828/1393 train_time:95948ms step_avg:117.30ms
step:829/1393 train_time:96069ms step_avg:117.30ms
step:830/1393 train_time:96192ms step_avg:117.31ms
step:831/1393 train_time:96314ms step_avg:117.31ms
step:832/1393 train_time:96438ms step_avg:117.32ms
step:833/1393 train_time:96560ms step_avg:117.33ms
step:834/1393 train_time:96682ms step_avg:117.33ms
step:835/1393 train_time:96804ms step_avg:117.34ms
step:836/1393 train_time:96925ms step_avg:117.34ms
step:837/1393 train_time:97047ms step_avg:117.35ms
step:838/1393 train_time:97169ms step_avg:117.35ms
step:839/1393 train_time:97291ms step_avg:117.36ms
step:840/1393 train_time:97416ms step_avg:117.37ms
step:841/1393 train_time:97537ms step_avg:117.37ms
step:842/1393 train_time:97658ms step_avg:117.38ms
step:843/1393 train_time:97780ms step_avg:117.38ms
step:844/1393 train_time:97901ms step_avg:117.39ms
step:845/1393 train_time:98023ms step_avg:117.39ms
step:846/1393 train_time:98146ms step_avg:117.40ms
step:847/1393 train_time:98269ms step_avg:117.41ms
step:848/1393 train_time:98390ms step_avg:117.41ms
step:849/1393 train_time:98512ms step_avg:117.42ms
step:850/1393 train_time:98635ms step_avg:117.42ms
step:851/1393 train_time:98757ms step_avg:117.43ms
step:852/1393 train_time:98879ms step_avg:117.43ms
step:853/1393 train_time:99000ms step_avg:117.44ms
step:854/1393 train_time:99123ms step_avg:117.44ms
step:855/1393 train_time:99245ms step_avg:117.45ms
step:856/1393 train_time:99366ms step_avg:117.45ms
step:857/1393 train_time:99488ms step_avg:117.46ms
step:858/1393 train_time:99610ms step_avg:117.46ms
step:859/1393 train_time:99732ms step_avg:117.47ms
step:860/1393 train_time:99854ms step_avg:117.47ms
step:861/1393 train_time:99977ms step_avg:117.48ms
step:862/1393 train_time:100100ms step_avg:117.49ms
step:863/1393 train_time:100222ms step_avg:117.49ms
step:864/1393 train_time:100343ms step_avg:117.50ms
step:865/1393 train_time:100465ms step_avg:117.50ms
step:866/1393 train_time:100586ms step_avg:117.51ms
step:867/1393 train_time:100708ms step_avg:117.51ms
step:868/1393 train_time:100829ms step_avg:117.52ms
step:869/1393 train_time:100952ms step_avg:117.52ms
step:870/1393 train_time:101076ms step_avg:117.53ms
step:871/1393 train_time:101198ms step_avg:117.54ms
step:872/1393 train_time:101320ms step_avg:117.54ms
step:873/1393 train_time:101442ms step_avg:117.55ms
step:874/1393 train_time:101564ms step_avg:117.55ms
step:875/1393 train_time:101686ms step_avg:117.56ms
step:875/1393 val_loss:3.4674 train_time:101807ms step_avg:117.70ms
step:876/1393 train_time:101831ms step_avg:117.59ms
step:877/1393 train_time:101934ms step_avg:117.57ms
step:878/1393 train_time:102062ms step_avg:117.58ms
step:879/1393 train_time:102185ms step_avg:117.59ms
step:880/1393 train_time:102307ms step_avg:117.59ms
step:881/1393 train_time:102428ms step_avg:117.60ms
step:882/1393 train_time:102551ms step_avg:117.60ms
step:883/1393 train_time:102673ms step_avg:117.61ms
step:884/1393 train_time:102794ms step_avg:117.61ms
step:885/1393 train_time:102916ms step_avg:117.62ms
step:886/1393 train_time:103038ms step_avg:117.62ms
step:887/1393 train_time:103160ms step_avg:117.63ms
step:888/1393 train_time:103282ms step_avg:117.63ms
step:889/1393 train_time:103406ms step_avg:117.64ms
step:890/1393 train_time:103528ms step_avg:117.65ms
step:891/1393 train_time:103650ms step_avg:117.65ms
step:892/1393 train_time:103771ms step_avg:117.65ms
step:893/1393 train_time:103893ms step_avg:117.66ms
step:894/1393 train_time:104017ms step_avg:117.67ms
step:895/1393 train_time:104139ms step_avg:117.67ms
step:896/1393 train_time:104261ms step_avg:117.68ms
step:897/1393 train_time:104384ms step_avg:117.68ms
step:898/1393 train_time:104505ms step_avg:117.69ms
step:899/1393 train_time:104627ms step_avg:117.69ms
step:900/1393 train_time:104749ms step_avg:117.70ms
step:901/1393 train_time:104872ms step_avg:117.70ms
step:902/1393 train_time:104994ms step_avg:117.71ms
step:903/1393 train_time:105116ms step_avg:117.71ms
step:904/1393 train_time:105238ms step_avg:117.72ms
step:905/1393 train_time:105362ms step_avg:117.72ms
step:906/1393 train_time:105485ms step_avg:117.73ms
step:907/1393 train_time:105606ms step_avg:117.73ms
step:908/1393 train_time:105728ms step_avg:117.74ms
step:909/1393 train_time:105849ms step_avg:117.74ms
step:910/1393 train_time:105971ms step_avg:117.75ms
step:911/1393 train_time:106093ms step_avg:117.75ms
step:912/1393 train_time:106215ms step_avg:117.75ms
step:913/1393 train_time:106338ms step_avg:117.76ms
step:914/1393 train_time:106461ms step_avg:117.77ms
step:915/1393 train_time:106583ms step_avg:117.77ms
step:916/1393 train_time:106704ms step_avg:117.77ms
step:917/1393 train_time:106825ms step_avg:117.78ms
step:918/1393 train_time:106946ms step_avg:117.78ms
step:919/1393 train_time:107069ms step_avg:117.79ms
step:920/1393 train_time:107191ms step_avg:117.79ms
step:921/1393 train_time:107314ms step_avg:117.80ms
step:922/1393 train_time:107436ms step_avg:117.80ms
step:923/1393 train_time:107559ms step_avg:117.81ms
step:924/1393 train_time:107682ms step_avg:117.81ms
step:925/1393 train_time:107804ms step_avg:117.82ms
step:926/1393 train_time:107929ms step_avg:117.83ms
step:927/1393 train_time:108051ms step_avg:117.83ms
step:928/1393 train_time:108173ms step_avg:117.84ms
step:929/1393 train_time:108295ms step_avg:117.84ms
step:930/1393 train_time:108418ms step_avg:117.85ms
step:931/1393 train_time:108542ms step_avg:117.85ms
step:932/1393 train_time:108665ms step_avg:117.86ms
step:933/1393 train_time:108790ms step_avg:117.87ms
step:934/1393 train_time:108914ms step_avg:117.87ms
step:935/1393 train_time:109038ms step_avg:117.88ms
step:936/1393 train_time:109161ms step_avg:117.88ms
step:937/1393 train_time:109285ms step_avg:117.89ms
step:938/1393 train_time:109408ms step_avg:117.90ms
step:939/1393 train_time:109531ms step_avg:117.90ms
step:940/1393 train_time:109654ms step_avg:117.91ms
step:941/1393 train_time:109777ms step_avg:117.91ms
step:942/1393 train_time:109901ms step_avg:117.92ms
step:943/1393 train_time:110026ms step_avg:117.93ms
step:944/1393 train_time:110149ms step_avg:117.93ms
step:945/1393 train_time:110274ms step_avg:117.94ms
step:946/1393 train_time:110398ms step_avg:117.95ms
step:947/1393 train_time:110522ms step_avg:117.95ms
step:948/1393 train_time:110645ms step_avg:117.96ms
step:949/1393 train_time:110769ms step_avg:117.96ms
step:950/1393 train_time:110894ms step_avg:117.97ms
step:951/1393 train_time:111018ms step_avg:117.98ms
step:952/1393 train_time:111141ms step_avg:117.98ms
step:953/1393 train_time:111264ms step_avg:117.99ms
step:954/1393 train_time:111387ms step_avg:117.99ms
step:955/1393 train_time:111512ms step_avg:118.00ms
step:956/1393 train_time:111635ms step_avg:118.01ms
step:957/1393 train_time:111757ms step_avg:118.01ms
step:958/1393 train_time:111882ms step_avg:118.02ms
step:959/1393 train_time:112005ms step_avg:118.02ms
step:960/1393 train_time:112128ms step_avg:118.03ms
step:961/1393 train_time:112251ms step_avg:118.04ms
step:962/1393 train_time:112375ms step_avg:118.04ms
step:963/1393 train_time:112500ms step_avg:118.05ms
step:964/1393 train_time:112622ms step_avg:118.05ms
step:965/1393 train_time:112746ms step_avg:118.06ms
step:966/1393 train_time:112869ms step_avg:118.06ms
step:967/1393 train_time:112992ms step_avg:118.07ms
step:968/1393 train_time:113115ms step_avg:118.07ms
step:969/1393 train_time:113238ms step_avg:118.08ms
step:970/1393 train_time:113361ms step_avg:118.08ms
step:971/1393 train_time:113485ms step_avg:118.09ms
step:972/1393 train_time:113612ms step_avg:118.10ms
step:973/1393 train_time:113734ms step_avg:118.10ms
step:974/1393 train_time:113857ms step_avg:118.11ms
step:975/1393 train_time:113980ms step_avg:118.11ms
step:976/1393 train_time:114105ms step_avg:118.12ms
step:977/1393 train_time:114230ms step_avg:118.13ms
step:978/1393 train_time:114355ms step_avg:118.14ms
step:979/1393 train_time:114479ms step_avg:118.14ms
step:980/1393 train_time:114602ms step_avg:118.15ms
step:981/1393 train_time:114724ms step_avg:118.15ms
step:982/1393 train_time:114848ms step_avg:118.16ms
step:983/1393 train_time:114970ms step_avg:118.16ms
step:984/1393 train_time:115094ms step_avg:118.17ms
step:985/1393 train_time:115217ms step_avg:118.17ms
step:986/1393 train_time:115342ms step_avg:118.18ms
step:987/1393 train_time:115464ms step_avg:118.18ms
step:988/1393 train_time:115588ms step_avg:118.19ms
step:989/1393 train_time:115711ms step_avg:118.19ms
step:990/1393 train_time:115835ms step_avg:118.20ms
step:991/1393 train_time:115958ms step_avg:118.20ms
step:992/1393 train_time:116082ms step_avg:118.21ms
step:993/1393 train_time:116205ms step_avg:118.22ms
step:994/1393 train_time:116331ms step_avg:118.22ms
step:995/1393 train_time:116455ms step_avg:118.23ms
step:996/1393 train_time:116580ms step_avg:118.23ms
step:997/1393 train_time:116703ms step_avg:118.24ms
step:998/1393 train_time:116828ms step_avg:118.25ms
step:999/1393 train_time:116950ms step_avg:118.25ms
step:1000/1393 train_time:117074ms step_avg:118.26ms
step:1000/1393 val_loss:3.4097 train_time:117196ms step_avg:118.38ms
step:1001/1393 train_time:117220ms step_avg:118.29ms
step:1002/1393 train_time:117326ms step_avg:118.27ms
step:1003/1393 train_time:117455ms step_avg:118.28ms
step:1004/1393 train_time:117579ms step_avg:118.29ms
step:1005/1393 train_time:117703ms step_avg:118.29ms
step:1006/1393 train_time:117825ms step_avg:118.30ms
step:1007/1393 train_time:117948ms step_avg:118.30ms
step:1008/1393 train_time:118072ms step_avg:118.31ms
step:1009/1393 train_time:118197ms step_avg:118.31ms
step:1010/1393 train_time:118319ms step_avg:118.32ms
step:1011/1393 train_time:118444ms step_avg:118.33ms
step:1012/1393 train_time:118568ms step_avg:118.33ms
step:1013/1393 train_time:118693ms step_avg:118.34ms
step:1014/1393 train_time:118817ms step_avg:118.34ms
step:1015/1393 train_time:118940ms step_avg:118.35ms
step:1016/1393 train_time:119064ms step_avg:118.35ms
step:1017/1393 train_time:119187ms step_avg:118.36ms
step:1018/1393 train_time:119311ms step_avg:118.36ms
step:1019/1393 train_time:119435ms step_avg:118.37ms
step:1020/1393 train_time:119559ms step_avg:118.38ms
step:1021/1393 train_time:119682ms step_avg:118.38ms
step:1022/1393 train_time:119809ms step_avg:118.39ms
step:1023/1393 train_time:119933ms step_avg:118.39ms
step:1024/1393 train_time:120056ms step_avg:118.40ms
step:1025/1393 train_time:120179ms step_avg:118.40ms
step:1026/1393 train_time:120303ms step_avg:118.41ms
step:1027/1393 train_time:120426ms step_avg:118.41ms
step:1028/1393 train_time:120550ms step_avg:118.42ms
step:1029/1393 train_time:120673ms step_avg:118.42ms
step:1030/1393 train_time:120797ms step_avg:118.43ms
step:1031/1393 train_time:120921ms step_avg:118.43ms
step:1032/1393 train_time:121043ms step_avg:118.44ms
step:1033/1393 train_time:121166ms step_avg:118.44ms
step:1034/1393 train_time:121290ms step_avg:118.45ms
step:1035/1393 train_time:121414ms step_avg:118.45ms
step:1036/1393 train_time:121538ms step_avg:118.46ms
step:1037/1393 train_time:121662ms step_avg:118.46ms
step:1038/1393 train_time:121785ms step_avg:118.47ms
step:1039/1393 train_time:121908ms step_avg:118.47ms
step:1040/1393 train_time:122031ms step_avg:118.48ms
step:1041/1393 train_time:122156ms step_avg:118.48ms
step:1042/1393 train_time:122280ms step_avg:118.49ms
step:1043/1393 train_time:122404ms step_avg:118.49ms
step:1044/1393 train_time:122530ms step_avg:118.50ms
step:1045/1393 train_time:122654ms step_avg:118.51ms
step:1046/1393 train_time:122777ms step_avg:118.51ms
step:1047/1393 train_time:122900ms step_avg:118.51ms
step:1048/1393 train_time:123024ms step_avg:118.52ms
step:1049/1393 train_time:123146ms step_avg:118.52ms
step:1050/1393 train_time:123270ms step_avg:118.53ms
step:1051/1393 train_time:123395ms step_avg:118.54ms
step:1052/1393 train_time:123519ms step_avg:118.54ms
step:1053/1393 train_time:123644ms step_avg:118.55ms
step:1054/1393 train_time:123768ms step_avg:118.55ms
step:1055/1393 train_time:123892ms step_avg:118.56ms
step:1056/1393 train_time:124015ms step_avg:118.56ms
step:1057/1393 train_time:124138ms step_avg:118.57ms
step:1058/1393 train_time:124261ms step_avg:118.57ms
step:1059/1393 train_time:124387ms step_avg:118.58ms
step:1060/1393 train_time:124510ms step_avg:118.58ms
step:1061/1393 train_time:124633ms step_avg:118.59ms
step:1062/1393 train_time:124756ms step_avg:118.59ms
step:1063/1393 train_time:124882ms step_avg:118.60ms
step:1064/1393 train_time:125005ms step_avg:118.60ms
step:1065/1393 train_time:125130ms step_avg:118.61ms
step:1066/1393 train_time:125252ms step_avg:118.61ms
step:1067/1393 train_time:125377ms step_avg:118.62ms
step:1068/1393 train_time:125502ms step_avg:118.62ms
step:1069/1393 train_time:125625ms step_avg:118.63ms
step:1070/1393 train_time:125749ms step_avg:118.63ms
step:1071/1393 train_time:125873ms step_avg:118.64ms
step:1072/1393 train_time:125996ms step_avg:118.64ms
step:1073/1393 train_time:126121ms step_avg:118.65ms
step:1074/1393 train_time:126245ms step_avg:118.65ms
step:1075/1393 train_time:126369ms step_avg:118.66ms
step:1076/1393 train_time:126493ms step_avg:118.66ms
step:1077/1393 train_time:126616ms step_avg:118.67ms
step:1078/1393 train_time:126739ms step_avg:118.67ms
step:1079/1393 train_time:126863ms step_avg:118.67ms
step:1080/1393 train_time:126988ms step_avg:118.68ms
step:1081/1393 train_time:127113ms step_avg:118.69ms
step:1082/1393 train_time:127237ms step_avg:118.69ms
step:1083/1393 train_time:127360ms step_avg:118.70ms
step:1084/1393 train_time:127483ms step_avg:118.70ms
step:1085/1393 train_time:127607ms step_avg:118.70ms
step:1086/1393 train_time:127733ms step_avg:118.71ms
step:1087/1393 train_time:127857ms step_avg:118.72ms
step:1088/1393 train_time:127980ms step_avg:118.72ms
step:1089/1393 train_time:128106ms step_avg:118.73ms
step:1090/1393 train_time:128231ms step_avg:118.73ms
step:1091/1393 train_time:128355ms step_avg:118.74ms
step:1092/1393 train_time:128481ms step_avg:118.74ms
step:1093/1393 train_time:128605ms step_avg:118.75ms
step:1094/1393 train_time:128729ms step_avg:118.75ms
step:1095/1393 train_time:128852ms step_avg:118.76ms
step:1096/1393 train_time:128975ms step_avg:118.76ms
step:1097/1393 train_time:129099ms step_avg:118.77ms
step:1098/1393 train_time:129223ms step_avg:118.77ms
step:1099/1393 train_time:129347ms step_avg:118.78ms
step:1100/1393 train_time:129470ms step_avg:118.78ms
step:1101/1393 train_time:129596ms step_avg:118.79ms
step:1102/1393 train_time:129722ms step_avg:118.79ms
step:1103/1393 train_time:129845ms step_avg:118.80ms
step:1104/1393 train_time:129969ms step_avg:118.80ms
step:1105/1393 train_time:130093ms step_avg:118.81ms
step:1106/1393 train_time:130216ms step_avg:118.81ms
step:1107/1393 train_time:130339ms step_avg:118.81ms
step:1108/1393 train_time:130464ms step_avg:118.82ms
step:1109/1393 train_time:130588ms step_avg:118.82ms
step:1110/1393 train_time:130712ms step_avg:118.83ms
step:1111/1393 train_time:130836ms step_avg:118.83ms
step:1112/1393 train_time:130960ms step_avg:118.84ms
step:1113/1393 train_time:131085ms step_avg:118.84ms
step:1114/1393 train_time:131208ms step_avg:118.85ms
step:1115/1393 train_time:131335ms step_avg:118.85ms
step:1116/1393 train_time:131458ms step_avg:118.86ms
step:1117/1393 train_time:131583ms step_avg:118.86ms
step:1118/1393 train_time:131706ms step_avg:118.87ms
step:1119/1393 train_time:131830ms step_avg:118.87ms
step:1120/1393 train_time:131957ms step_avg:118.88ms
step:1121/1393 train_time:132080ms step_avg:118.88ms
step:1122/1393 train_time:132203ms step_avg:118.89ms
step:1123/1393 train_time:132327ms step_avg:118.89ms
step:1124/1393 train_time:132451ms step_avg:118.90ms
step:1125/1393 train_time:132575ms step_avg:118.90ms
step:1125/1393 val_loss:3.3601 train_time:132697ms step_avg:119.01ms
step:1126/1393 train_time:132721ms step_avg:118.93ms
step:1127/1393 train_time:132828ms step_avg:118.92ms
step:1128/1393 train_time:132957ms step_avg:118.92ms
step:1129/1393 train_time:133082ms step_avg:118.93ms
step:1130/1393 train_time:133206ms step_avg:118.93ms
step:1131/1393 train_time:133332ms step_avg:118.94ms
step:1132/1393 train_time:133456ms step_avg:118.94ms
step:1133/1393 train_time:133579ms step_avg:118.95ms
step:1134/1393 train_time:133703ms step_avg:118.95ms
step:1135/1393 train_time:133827ms step_avg:118.96ms
step:1136/1393 train_time:133952ms step_avg:118.96ms
step:1137/1393 train_time:134076ms step_avg:118.97ms
step:1138/1393 train_time:134203ms step_avg:118.97ms
step:1139/1393 train_time:134331ms step_avg:118.98ms
step:1140/1393 train_time:134455ms step_avg:118.99ms
step:1141/1393 train_time:134581ms step_avg:118.99ms
step:1142/1393 train_time:134705ms step_avg:119.00ms
step:1143/1393 train_time:134830ms step_avg:119.00ms
step:1144/1393 train_time:134954ms step_avg:119.01ms
step:1145/1393 train_time:135081ms step_avg:119.01ms
step:1146/1393 train_time:135205ms step_avg:119.02ms
step:1147/1393 train_time:135330ms step_avg:119.02ms
step:1148/1393 train_time:135455ms step_avg:119.03ms
step:1149/1393 train_time:135580ms step_avg:119.03ms
step:1150/1393 train_time:135704ms step_avg:119.04ms
step:1151/1393 train_time:135829ms step_avg:119.04ms
step:1152/1393 train_time:135954ms step_avg:119.05ms
step:1153/1393 train_time:136079ms step_avg:119.05ms
step:1154/1393 train_time:136203ms step_avg:119.06ms
step:1155/1393 train_time:136328ms step_avg:119.06ms
step:1156/1393 train_time:136453ms step_avg:119.07ms
step:1157/1393 train_time:136583ms step_avg:119.08ms
step:1158/1393 train_time:136707ms step_avg:119.08ms
step:1159/1393 train_time:136833ms step_avg:119.09ms
step:1160/1393 train_time:136961ms step_avg:119.10ms
step:1161/1393 train_time:137087ms step_avg:119.10ms
step:1162/1393 train_time:137211ms step_avg:119.11ms
step:1163/1393 train_time:137337ms step_avg:119.11ms
step:1164/1393 train_time:137462ms step_avg:119.12ms
step:1165/1393 train_time:137587ms step_avg:119.12ms
step:1166/1393 train_time:137715ms step_avg:119.13ms
step:1167/1393 train_time:137841ms step_avg:119.14ms
step:1168/1393 train_time:137968ms step_avg:119.14ms
step:1169/1393 train_time:138095ms step_avg:119.15ms
step:1170/1393 train_time:138224ms step_avg:119.16ms
step:1171/1393 train_time:138350ms step_avg:119.16ms
step:1172/1393 train_time:138475ms step_avg:119.17ms
step:1173/1393 train_time:138599ms step_avg:119.17ms
step:1174/1393 train_time:138725ms step_avg:119.18ms
step:1175/1393 train_time:138850ms step_avg:119.18ms
step:1176/1393 train_time:138976ms step_avg:119.19ms
step:1177/1393 train_time:139102ms step_avg:119.20ms
step:1178/1393 train_time:139226ms step_avg:119.20ms
step:1179/1393 train_time:139355ms step_avg:119.21ms
step:1180/1393 train_time:139481ms step_avg:119.21ms
step:1181/1393 train_time:139607ms step_avg:119.22ms
step:1182/1393 train_time:139732ms step_avg:119.22ms
step:1183/1393 train_time:139856ms step_avg:119.23ms
step:1184/1393 train_time:139982ms step_avg:119.23ms
step:1185/1393 train_time:140112ms step_avg:119.24ms
step:1186/1393 train_time:140237ms step_avg:119.25ms
step:1187/1393 train_time:140361ms step_avg:119.25ms
step:1188/1393 train_time:140486ms step_avg:119.26ms
step:1189/1393 train_time:140611ms step_avg:119.26ms
step:1190/1393 train_time:140735ms step_avg:119.27ms
step:1191/1393 train_time:140860ms step_avg:119.27ms
step:1192/1393 train_time:140988ms step_avg:119.28ms
step:1193/1393 train_time:141114ms step_avg:119.29ms
step:1194/1393 train_time:141239ms step_avg:119.29ms
step:1195/1393 train_time:141364ms step_avg:119.29ms
step:1196/1393 train_time:141490ms step_avg:119.30ms
step:1197/1393 train_time:141619ms step_avg:119.31ms
step:1198/1393 train_time:141743ms step_avg:119.31ms
step:1199/1393 train_time:141869ms step_avg:119.32ms
step:1200/1393 train_time:141995ms step_avg:119.32ms
step:1201/1393 train_time:142119ms step_avg:119.33ms
step:1202/1393 train_time:142245ms step_avg:119.33ms
step:1203/1393 train_time:142369ms step_avg:119.34ms
step:1204/1393 train_time:142494ms step_avg:119.34ms
step:1205/1393 train_time:142620ms step_avg:119.35ms
step:1206/1393 train_time:142745ms step_avg:119.35ms
step:1207/1393 train_time:142872ms step_avg:119.36ms
step:1208/1393 train_time:142998ms step_avg:119.36ms
step:1209/1393 train_time:143124ms step_avg:119.37ms
step:1210/1393 train_time:143248ms step_avg:119.37ms
step:1211/1393 train_time:143373ms step_avg:119.38ms
step:1212/1393 train_time:143500ms step_avg:119.38ms
step:1213/1393 train_time:143626ms step_avg:119.39ms
step:1214/1393 train_time:143752ms step_avg:119.40ms
step:1215/1393 train_time:143877ms step_avg:119.40ms
step:1216/1393 train_time:144000ms step_avg:119.40ms
step:1217/1393 train_time:144126ms step_avg:119.41ms
step:1218/1393 train_time:144251ms step_avg:119.41ms
step:1219/1393 train_time:144375ms step_avg:119.42ms
step:1220/1393 train_time:144503ms step_avg:119.42ms
step:1221/1393 train_time:144628ms step_avg:119.43ms
step:1222/1393 train_time:144754ms step_avg:119.43ms
step:1223/1393 train_time:144880ms step_avg:119.44ms
step:1224/1393 train_time:145005ms step_avg:119.44ms
step:1225/1393 train_time:145130ms step_avg:119.45ms
step:1226/1393 train_time:145255ms step_avg:119.45ms
step:1227/1393 train_time:145380ms step_avg:119.46ms
step:1228/1393 train_time:145505ms step_avg:119.46ms
step:1229/1393 train_time:145632ms step_avg:119.47ms
step:1230/1393 train_time:145756ms step_avg:119.47ms
step:1231/1393 train_time:145880ms step_avg:119.48ms
step:1232/1393 train_time:146006ms step_avg:119.48ms
step:1233/1393 train_time:146131ms step_avg:119.49ms
step:1234/1393 train_time:146256ms step_avg:119.49ms
step:1235/1393 train_time:146381ms step_avg:119.49ms
step:1236/1393 train_time:146509ms step_avg:119.50ms
step:1237/1393 train_time:146634ms step_avg:119.51ms
step:1238/1393 train_time:146761ms step_avg:119.51ms
step:1239/1393 train_time:146887ms step_avg:119.52ms
step:1240/1393 train_time:147016ms step_avg:119.53ms
step:1241/1393 train_time:147140ms step_avg:119.53ms
step:1242/1393 train_time:147266ms step_avg:119.53ms
step:1243/1393 train_time:147393ms step_avg:119.54ms
step:1244/1393 train_time:147521ms step_avg:119.55ms
step:1245/1393 train_time:147646ms step_avg:119.55ms
step:1246/1393 train_time:147770ms step_avg:119.56ms
step:1247/1393 train_time:147896ms step_avg:119.56ms
step:1248/1393 train_time:148021ms step_avg:119.56ms
step:1249/1393 train_time:148147ms step_avg:119.57ms
step:1250/1393 train_time:148272ms step_avg:119.57ms
step:1250/1393 val_loss:3.3154 train_time:148397ms step_avg:119.68ms
step:1251/1393 train_time:148421ms step_avg:119.60ms
step:1252/1393 train_time:148528ms step_avg:119.59ms
step:1253/1393 train_time:148658ms step_avg:119.60ms
step:1254/1393 train_time:148784ms step_avg:119.60ms
step:1255/1393 train_time:148910ms step_avg:119.61ms
step:1256/1393 train_time:149035ms step_avg:119.61ms
step:1257/1393 train_time:149159ms step_avg:119.61ms
step:1258/1393 train_time:149284ms step_avg:119.62ms
step:1259/1393 train_time:149410ms step_avg:119.62ms
step:1260/1393 train_time:149535ms step_avg:119.63ms
step:1261/1393 train_time:149664ms step_avg:119.64ms
step:1262/1393 train_time:149791ms step_avg:119.64ms
step:1263/1393 train_time:149916ms step_avg:119.65ms
step:1264/1393 train_time:150041ms step_avg:119.65ms
step:1265/1393 train_time:150166ms step_avg:119.65ms
step:1266/1393 train_time:150291ms step_avg:119.66ms
step:1267/1393 train_time:150417ms step_avg:119.66ms
step:1268/1393 train_time:150542ms step_avg:119.67ms
step:1269/1393 train_time:150666ms step_avg:119.67ms
step:1270/1393 train_time:150792ms step_avg:119.68ms
step:1271/1393 train_time:150919ms step_avg:119.68ms
step:1272/1393 train_time:151047ms step_avg:119.69ms
step:1273/1393 train_time:151172ms step_avg:119.69ms
step:1274/1393 train_time:151300ms step_avg:119.70ms
step:1275/1393 train_time:151426ms step_avg:119.70ms
step:1276/1393 train_time:151551ms step_avg:119.71ms
step:1277/1393 train_time:151677ms step_avg:119.71ms
step:1278/1393 train_time:151803ms step_avg:119.72ms
step:1279/1393 train_time:151928ms step_avg:119.72ms
step:1280/1393 train_time:152053ms step_avg:119.73ms
step:1281/1393 train_time:152177ms step_avg:119.73ms
step:1282/1393 train_time:152302ms step_avg:119.73ms
step:1283/1393 train_time:152428ms step_avg:119.74ms
step:1284/1393 train_time:152555ms step_avg:119.74ms
step:1285/1393 train_time:152681ms step_avg:119.75ms
step:1286/1393 train_time:152805ms step_avg:119.75ms
step:1287/1393 train_time:152932ms step_avg:119.76ms
step:1288/1393 train_time:153057ms step_avg:119.76ms
step:1289/1393 train_time:153183ms step_avg:119.77ms
step:1290/1393 train_time:153309ms step_avg:119.77ms
step:1291/1393 train_time:153434ms step_avg:119.78ms
step:1292/1393 train_time:153559ms step_avg:119.78ms
step:1293/1393 train_time:153683ms step_avg:119.78ms
step:1294/1393 train_time:153809ms step_avg:119.79ms
step:1295/1393 train_time:153935ms step_avg:119.79ms
step:1296/1393 train_time:154061ms step_avg:119.80ms
step:1297/1393 train_time:154186ms step_avg:119.80ms
step:1298/1393 train_time:154311ms step_avg:119.81ms
step:1299/1393 train_time:154440ms step_avg:119.81ms
step:1300/1393 train_time:154565ms step_avg:119.82ms
step:1301/1393 train_time:154689ms step_avg:119.82ms
step:1302/1393 train_time:154814ms step_avg:119.82ms
step:1303/1393 train_time:154942ms step_avg:119.83ms
step:1304/1393 train_time:155068ms step_avg:119.84ms
step:1305/1393 train_time:155192ms step_avg:119.84ms
step:1306/1393 train_time:155318ms step_avg:119.84ms
step:1307/1393 train_time:155444ms step_avg:119.85ms
step:1308/1393 train_time:155569ms step_avg:119.85ms
step:1309/1393 train_time:155699ms step_avg:119.86ms
step:1310/1393 train_time:155824ms step_avg:119.86ms
step:1311/1393 train_time:155950ms step_avg:119.87ms
step:1312/1393 train_time:156077ms step_avg:119.87ms
step:1313/1393 train_time:156204ms step_avg:119.88ms
step:1314/1393 train_time:156331ms step_avg:119.89ms
step:1315/1393 train_time:156456ms step_avg:119.89ms
step:1316/1393 train_time:156581ms step_avg:119.89ms
step:1317/1393 train_time:156709ms step_avg:119.90ms
step:1318/1393 train_time:156835ms step_avg:119.90ms
step:1319/1393 train_time:156960ms step_avg:119.91ms
step:1320/1393 train_time:157085ms step_avg:119.91ms
step:1321/1393 train_time:157212ms step_avg:119.92ms
step:1322/1393 train_time:157335ms step_avg:119.92ms
step:1323/1393 train_time:157460ms step_avg:119.92ms
step:1324/1393 train_time:157585ms step_avg:119.93ms
step:1325/1393 train_time:157710ms step_avg:119.93ms
step:1326/1393 train_time:157836ms step_avg:119.94ms
step:1327/1393 train_time:157962ms step_avg:119.94ms
step:1328/1393 train_time:158088ms step_avg:119.95ms
step:1329/1393 train_time:158213ms step_avg:119.95ms
step:1330/1393 train_time:158338ms step_avg:119.95ms
step:1331/1393 train_time:158462ms step_avg:119.96ms
step:1332/1393 train_time:158589ms step_avg:119.96ms
step:1333/1393 train_time:158716ms step_avg:119.97ms
step:1334/1393 train_time:158840ms step_avg:119.97ms
step:1335/1393 train_time:158964ms step_avg:119.97ms
step:1336/1393 train_time:159089ms step_avg:119.98ms
step:1337/1393 train_time:159215ms step_avg:119.98ms
step:1338/1393 train_time:159344ms step_avg:119.99ms
step:1339/1393 train_time:159468ms step_avg:119.99ms
step:1340/1393 train_time:159594ms step_avg:120.00ms
step:1341/1393 train_time:159720ms step_avg:120.00ms
step:1342/1393 train_time:159845ms step_avg:120.00ms
step:1343/1393 train_time:159971ms step_avg:120.01ms
step:1344/1393 train_time:160099ms step_avg:120.01ms
step:1345/1393 train_time:160225ms step_avg:120.02ms
step:1346/1393 train_time:160352ms step_avg:120.02ms
step:1347/1393 train_time:160479ms step_avg:120.03ms
step:1348/1393 train_time:160605ms step_avg:120.03ms
step:1349/1393 train_time:160737ms step_avg:120.04ms
step:1350/1393 train_time:160864ms step_avg:120.05ms
step:1351/1393 train_time:160993ms step_avg:120.05ms
step:1352/1393 train_time:161121ms step_avg:120.06ms
step:1353/1393 train_time:161248ms step_avg:120.07ms
step:1354/1393 train_time:161378ms step_avg:120.07ms
step:1355/1393 train_time:161504ms step_avg:120.08ms
step:1356/1393 train_time:161631ms step_avg:120.08ms
step:1357/1393 train_time:161758ms step_avg:120.09ms
step:1358/1393 train_time:161884ms step_avg:120.09ms
step:1359/1393 train_time:162014ms step_avg:120.10ms
step:1360/1393 train_time:162144ms step_avg:120.11ms
step:1361/1393 train_time:162274ms step_avg:120.11ms
step:1362/1393 train_time:162402ms step_avg:120.12ms
step:1363/1393 train_time:162528ms step_avg:120.12ms
step:1364/1393 train_time:162658ms step_avg:120.13ms
step:1365/1393 train_time:162784ms step_avg:120.14ms
step:1366/1393 train_time:162909ms step_avg:120.14ms
step:1367/1393 train_time:163036ms step_avg:120.14ms
step:1368/1393 train_time:163161ms step_avg:120.15ms
step:1369/1393 train_time:163287ms step_avg:120.15ms
step:1370/1393 train_time:163414ms step_avg:120.16ms
step:1371/1393 train_time:163540ms step_avg:120.16ms
step:1372/1393 train_time:163666ms step_avg:120.17ms
step:1373/1393 train_time:163791ms step_avg:120.17ms
step:1374/1393 train_time:163919ms step_avg:120.18ms
step:1375/1393 train_time:164044ms step_avg:120.18ms
step:1375/1393 val_loss:3.2816 train_time:164170ms step_avg:120.27ms
step:1376/1393 train_time:164194ms step_avg:120.20ms
step:1377/1393 train_time:164303ms step_avg:120.19ms
step:1378/1393 train_time:164432ms step_avg:120.20ms
step:1379/1393 train_time:164563ms step_avg:120.21ms
step:1380/1393 train_time:164690ms step_avg:120.21ms
step:1381/1393 train_time:164818ms step_avg:120.22ms
step:1382/1393 train_time:164944ms step_avg:120.22ms
step:1383/1393 train_time:165073ms step_avg:120.23ms
step:1384/1393 train_time:165198ms step_avg:120.23ms
step:1385/1393 train_time:165326ms step_avg:120.24ms
step:1386/1393 train_time:165451ms step_avg:120.24ms
step:1387/1393 train_time:165577ms step_avg:120.24ms
step:1388/1393 train_time:165709ms step_avg:120.25ms
step:1389/1393 train_time:165835ms step_avg:120.26ms
step:1390/1393 train_time:165962ms step_avg:120.26ms
step:1391/1393 train_time:166089ms step_avg:120.27ms
step:1392/1393 train_time:166214ms step_avg:120.27ms
step:1393/1393 train_time:166340ms step_avg:120.28ms
step:1393/1393 val_loss:3.2781 train_time:166466ms step_avg:120.37ms
peak memory allocated: 31573 MiB reserved: 32976 MiB
