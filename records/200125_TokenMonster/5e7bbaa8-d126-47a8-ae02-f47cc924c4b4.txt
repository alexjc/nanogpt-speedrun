import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)
flex_kernel_options = None
if torch.cuda.get_device_name(0).endswith(("3090", "4090")):
    flex_kernel_options = {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_M1": 32, "BLOCK_N1": 64, "BLOCK_M2": 64, "BLOCK_N2": 32}

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale, kernel_options=flex_kernel_options)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor = None, sliding_window_num_blocks: Tensor = 0):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 28415).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x) if not self.training else lm_head_fp8(x, self.lm_head.weight)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)

        if target_seq is None:
            return logits

        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb-tokmon-10B/english-28416-balanced/fineweb-tokmon_train_*.bin" # input .bin to train on
    val_files = "data/fineweb-tokmon-10B/english-28416-balanced/fineweb-tokmon_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # fewer tokens but equivalent text for validation, snapped to nearest seq_len
    val_ratio = 0.99011 # equivalent token density on validation tokens to that of GPT-2
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()


def main():
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

    model = GPT(vocab_size=28416, num_layers=12, num_heads=6, model_dim=768).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    k = 1.08
    adam_params = [dict(params=head_params, lr=0.008*k), dict(params=embed_params, lr=0.6*k), dict(params=scalar_params, lr=0.04*k)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05*k, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model)
    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss = (val_loss * args.val_ratio) / val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
    )
    dist.destroy_process_group()


if __name__ == "__main__":
    main()
====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu124 compiled for CUDA 12.4
Mon Jan 20 17:40:22 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   38C    P0            131W /  700W |    7714MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   41C    P0            121W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   42C    P0            122W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   44C    P0            126W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   41C    P0            121W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   38C    P0            121W /  700W |    3212MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     48468      C   /usr/bin/python3                             3394MiB |
|    0   N/A  N/A     48469      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     48470      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     48471      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     48472      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     48473      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     48474      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     48475      C   /usr/bin/python3                              610MiB |
|    1   N/A  N/A     48469      C   /usr/bin/python3                             3442MiB |
|    2   N/A  N/A     48470      C   /usr/bin/python3                             3442MiB |
|    3   N/A  N/A     48471      C   /usr/bin/python3                             3442MiB |
|    4   N/A  N/A     48472      C   /usr/bin/python3                             3442MiB |
|    5   N/A  N/A     48473      C   /usr/bin/python3                             3442MiB |
|    6   N/A  N/A     48474      C   /usr/bin/python3                             3442MiB |
|    7   N/A  N/A     48475      C   /usr/bin/python3                             3202MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.1533 train_time:0ms step_avg:nanms
step:1/1393 train_time:19711ms step_avg:nanms
step:2/1393 train_time:19746ms step_avg:nanms
step:3/1393 train_time:20234ms step_avg:nanms
step:4/1393 train_time:20345ms step_avg:nanms
step:5/1393 train_time:20458ms step_avg:nanms
step:6/1393 train_time:20570ms step_avg:nanms
step:7/1393 train_time:20682ms step_avg:nanms
step:8/1393 train_time:20795ms step_avg:nanms
step:9/1393 train_time:20908ms step_avg:nanms
step:10/1393 train_time:21021ms step_avg:nanms
step:11/1393 train_time:113ms step_avg:nanms
step:12/1393 train_time:225ms step_avg:nanms
step:13/1393 train_time:339ms step_avg:112.95ms
step:14/1393 train_time:452ms step_avg:112.99ms
step:15/1393 train_time:565ms step_avg:113.10ms
step:16/1393 train_time:679ms step_avg:113.15ms
step:17/1393 train_time:792ms step_avg:113.09ms
step:18/1393 train_time:905ms step_avg:113.12ms
step:19/1393 train_time:1018ms step_avg:113.11ms
step:20/1393 train_time:1132ms step_avg:113.18ms
step:21/1393 train_time:1245ms step_avg:113.16ms
step:22/1393 train_time:1357ms step_avg:113.09ms
step:23/1393 train_time:1470ms step_avg:113.11ms
step:24/1393 train_time:1583ms step_avg:113.08ms
step:25/1393 train_time:1697ms step_avg:113.13ms
step:26/1393 train_time:1810ms step_avg:113.13ms
step:27/1393 train_time:1923ms step_avg:113.13ms
step:28/1393 train_time:2036ms step_avg:113.12ms
step:29/1393 train_time:2149ms step_avg:113.11ms
step:30/1393 train_time:2262ms step_avg:113.12ms
step:31/1393 train_time:2376ms step_avg:113.16ms
step:32/1393 train_time:2489ms step_avg:113.12ms
step:33/1393 train_time:2601ms step_avg:113.10ms
step:34/1393 train_time:2715ms step_avg:113.11ms
step:35/1393 train_time:2828ms step_avg:113.11ms
step:36/1393 train_time:2942ms step_avg:113.17ms
step:37/1393 train_time:3055ms step_avg:113.16ms
step:38/1393 train_time:3168ms step_avg:113.15ms
step:39/1393 train_time:3282ms step_avg:113.17ms
step:40/1393 train_time:3395ms step_avg:113.15ms
step:41/1393 train_time:3507ms step_avg:113.13ms
step:42/1393 train_time:3619ms step_avg:113.11ms
step:43/1393 train_time:3732ms step_avg:113.10ms
step:44/1393 train_time:3845ms step_avg:113.10ms
step:45/1393 train_time:3959ms step_avg:113.11ms
step:46/1393 train_time:4073ms step_avg:113.13ms
step:47/1393 train_time:4186ms step_avg:113.12ms
step:48/1393 train_time:4299ms step_avg:113.12ms
step:49/1393 train_time:4412ms step_avg:113.13ms
step:50/1393 train_time:4525ms step_avg:113.12ms
step:51/1393 train_time:4637ms step_avg:113.10ms
step:52/1393 train_time:4750ms step_avg:113.10ms
step:53/1393 train_time:4864ms step_avg:113.11ms
step:54/1393 train_time:4976ms step_avg:113.10ms
step:55/1393 train_time:5090ms step_avg:113.10ms
step:56/1393 train_time:5204ms step_avg:113.12ms
step:57/1393 train_time:5317ms step_avg:113.12ms
step:58/1393 train_time:5429ms step_avg:113.11ms
step:59/1393 train_time:5542ms step_avg:113.11ms
step:60/1393 train_time:5656ms step_avg:113.11ms
step:61/1393 train_time:5768ms step_avg:113.11ms
step:62/1393 train_time:5881ms step_avg:113.10ms
step:63/1393 train_time:5995ms step_avg:113.11ms
step:64/1393 train_time:6108ms step_avg:113.10ms
step:65/1393 train_time:6220ms step_avg:113.09ms
step:66/1393 train_time:6334ms step_avg:113.10ms
step:67/1393 train_time:6446ms step_avg:113.09ms
step:68/1393 train_time:6560ms step_avg:113.10ms
step:69/1393 train_time:6674ms step_avg:113.11ms
step:70/1393 train_time:6787ms step_avg:113.11ms
step:71/1393 train_time:6900ms step_avg:113.11ms
step:72/1393 train_time:7013ms step_avg:113.11ms
step:73/1393 train_time:7126ms step_avg:113.11ms
step:74/1393 train_time:7239ms step_avg:113.11ms
step:75/1393 train_time:7352ms step_avg:113.11ms
step:76/1393 train_time:7466ms step_avg:113.12ms
step:77/1393 train_time:7579ms step_avg:113.12ms
step:78/1393 train_time:7692ms step_avg:113.12ms
step:79/1393 train_time:7806ms step_avg:113.12ms
step:80/1393 train_time:7919ms step_avg:113.12ms
step:81/1393 train_time:8032ms step_avg:113.13ms
step:82/1393 train_time:8145ms step_avg:113.13ms
step:83/1393 train_time:8258ms step_avg:113.12ms
step:84/1393 train_time:8371ms step_avg:113.13ms
step:85/1393 train_time:8485ms step_avg:113.13ms
step:86/1393 train_time:8597ms step_avg:113.12ms
step:87/1393 train_time:8710ms step_avg:113.12ms
step:88/1393 train_time:8824ms step_avg:113.13ms
step:89/1393 train_time:8938ms step_avg:113.13ms
step:90/1393 train_time:9051ms step_avg:113.14ms
step:91/1393 train_time:9164ms step_avg:113.14ms
step:92/1393 train_time:9278ms step_avg:113.15ms
step:93/1393 train_time:9391ms step_avg:113.14ms
step:94/1393 train_time:9504ms step_avg:113.14ms
step:95/1393 train_time:9617ms step_avg:113.14ms
step:96/1393 train_time:9731ms step_avg:113.15ms
step:97/1393 train_time:9844ms step_avg:113.15ms
step:98/1393 train_time:9957ms step_avg:113.15ms
step:99/1393 train_time:10070ms step_avg:113.15ms
step:100/1393 train_time:10183ms step_avg:113.14ms
step:101/1393 train_time:10295ms step_avg:113.14ms
step:102/1393 train_time:10408ms step_avg:113.13ms
step:103/1393 train_time:10522ms step_avg:113.14ms
step:104/1393 train_time:10635ms step_avg:113.14ms
step:105/1393 train_time:10748ms step_avg:113.14ms
step:106/1393 train_time:10862ms step_avg:113.14ms
step:107/1393 train_time:10976ms step_avg:113.15ms
step:108/1393 train_time:11089ms step_avg:113.15ms
step:109/1393 train_time:11203ms step_avg:113.16ms
step:110/1393 train_time:11317ms step_avg:113.17ms
step:111/1393 train_time:11431ms step_avg:113.17ms
step:112/1393 train_time:11544ms step_avg:113.17ms
step:113/1393 train_time:11658ms step_avg:113.18ms
step:114/1393 train_time:11771ms step_avg:113.18ms
step:115/1393 train_time:11885ms step_avg:113.19ms
step:116/1393 train_time:11999ms step_avg:113.20ms
step:117/1393 train_time:12113ms step_avg:113.20ms
step:118/1393 train_time:12226ms step_avg:113.20ms
step:119/1393 train_time:12339ms step_avg:113.20ms
step:120/1393 train_time:12453ms step_avg:113.20ms
step:121/1393 train_time:12566ms step_avg:113.21ms
step:122/1393 train_time:12680ms step_avg:113.22ms
step:123/1393 train_time:12794ms step_avg:113.22ms
step:124/1393 train_time:12907ms step_avg:113.22ms
step:125/1393 train_time:13022ms step_avg:113.23ms
step:125/1393 val_loss:4.3418 train_time:13135ms step_avg:114.21ms
step:126/1393 train_time:13159ms step_avg:113.44ms
step:127/1393 train_time:13252ms step_avg:113.26ms
step:128/1393 train_time:13375ms step_avg:113.34ms
step:129/1393 train_time:13491ms step_avg:113.37ms
step:130/1393 train_time:13605ms step_avg:113.37ms
step:131/1393 train_time:13718ms step_avg:113.37ms
step:132/1393 train_time:13831ms step_avg:113.37ms
step:133/1393 train_time:13945ms step_avg:113.37ms
step:134/1393 train_time:14059ms step_avg:113.38ms
step:135/1393 train_time:14172ms step_avg:113.38ms
step:136/1393 train_time:14285ms step_avg:113.38ms
step:137/1393 train_time:14399ms step_avg:113.38ms
step:138/1393 train_time:14513ms step_avg:113.38ms
step:139/1393 train_time:14627ms step_avg:113.39ms
step:140/1393 train_time:14741ms step_avg:113.39ms
step:141/1393 train_time:14854ms step_avg:113.39ms
step:142/1393 train_time:14969ms step_avg:113.40ms
step:143/1393 train_time:15083ms step_avg:113.41ms
step:144/1393 train_time:15197ms step_avg:113.41ms
step:145/1393 train_time:15312ms step_avg:113.42ms
step:146/1393 train_time:15425ms step_avg:113.42ms
step:147/1393 train_time:15539ms step_avg:113.42ms
step:148/1393 train_time:15652ms step_avg:113.42ms
step:149/1393 train_time:15767ms step_avg:113.43ms
step:150/1393 train_time:15881ms step_avg:113.43ms
step:151/1393 train_time:15994ms step_avg:113.43ms
step:152/1393 train_time:16108ms step_avg:113.44ms
step:153/1393 train_time:16222ms step_avg:113.44ms
step:154/1393 train_time:16335ms step_avg:113.44ms
step:155/1393 train_time:16449ms step_avg:113.44ms
step:156/1393 train_time:16563ms step_avg:113.44ms
step:157/1393 train_time:16676ms step_avg:113.44ms
step:158/1393 train_time:16790ms step_avg:113.44ms
step:159/1393 train_time:16903ms step_avg:113.44ms
step:160/1393 train_time:17016ms step_avg:113.44ms
step:161/1393 train_time:17130ms step_avg:113.45ms
step:162/1393 train_time:17244ms step_avg:113.45ms
step:163/1393 train_time:17358ms step_avg:113.45ms
step:164/1393 train_time:17471ms step_avg:113.45ms
step:165/1393 train_time:17586ms step_avg:113.46ms
step:166/1393 train_time:17700ms step_avg:113.46ms
step:167/1393 train_time:17814ms step_avg:113.47ms
step:168/1393 train_time:17928ms step_avg:113.47ms
step:169/1393 train_time:18042ms step_avg:113.47ms
step:170/1393 train_time:18156ms step_avg:113.48ms
step:171/1393 train_time:18270ms step_avg:113.48ms
step:172/1393 train_time:18384ms step_avg:113.48ms
step:173/1393 train_time:18497ms step_avg:113.48ms
step:174/1393 train_time:18611ms step_avg:113.48ms
step:175/1393 train_time:18725ms step_avg:113.49ms
step:176/1393 train_time:18839ms step_avg:113.48ms
step:177/1393 train_time:18953ms step_avg:113.49ms
step:178/1393 train_time:19067ms step_avg:113.49ms
step:179/1393 train_time:19180ms step_avg:113.49ms
step:180/1393 train_time:19294ms step_avg:113.49ms
step:181/1393 train_time:19407ms step_avg:113.49ms
step:182/1393 train_time:19520ms step_avg:113.49ms
step:183/1393 train_time:19634ms step_avg:113.49ms
step:184/1393 train_time:19748ms step_avg:113.49ms
step:185/1393 train_time:19862ms step_avg:113.50ms
step:186/1393 train_time:19975ms step_avg:113.50ms
step:187/1393 train_time:20089ms step_avg:113.50ms
step:188/1393 train_time:20203ms step_avg:113.50ms
step:189/1393 train_time:20317ms step_avg:113.50ms
step:190/1393 train_time:20430ms step_avg:113.50ms
step:191/1393 train_time:20545ms step_avg:113.51ms
step:192/1393 train_time:20658ms step_avg:113.50ms
step:193/1393 train_time:20772ms step_avg:113.51ms
step:194/1393 train_time:20886ms step_avg:113.51ms
step:195/1393 train_time:21000ms step_avg:113.51ms
step:196/1393 train_time:21113ms step_avg:113.51ms
step:197/1393 train_time:21227ms step_avg:113.51ms
step:198/1393 train_time:21340ms step_avg:113.51ms
step:199/1393 train_time:21453ms step_avg:113.51ms
step:200/1393 train_time:21567ms step_avg:113.51ms
step:201/1393 train_time:21680ms step_avg:113.51ms
step:202/1393 train_time:21794ms step_avg:113.51ms
step:203/1393 train_time:21907ms step_avg:113.51ms
step:204/1393 train_time:22021ms step_avg:113.51ms
step:205/1393 train_time:22135ms step_avg:113.51ms
step:206/1393 train_time:22248ms step_avg:113.51ms
step:207/1393 train_time:22361ms step_avg:113.51ms
step:208/1393 train_time:22475ms step_avg:113.51ms
step:209/1393 train_time:22589ms step_avg:113.51ms
step:210/1393 train_time:22704ms step_avg:113.52ms
step:211/1393 train_time:22818ms step_avg:113.52ms
step:212/1393 train_time:22932ms step_avg:113.53ms
step:213/1393 train_time:23046ms step_avg:113.53ms
step:214/1393 train_time:23160ms step_avg:113.53ms
step:215/1393 train_time:23275ms step_avg:113.54ms
step:216/1393 train_time:23390ms step_avg:113.54ms
step:217/1393 train_time:23504ms step_avg:113.55ms
step:218/1393 train_time:23618ms step_avg:113.55ms
step:219/1393 train_time:23732ms step_avg:113.55ms
step:220/1393 train_time:23847ms step_avg:113.56ms
step:221/1393 train_time:23961ms step_avg:113.56ms
step:222/1393 train_time:24075ms step_avg:113.56ms
step:223/1393 train_time:24189ms step_avg:113.56ms
step:224/1393 train_time:24303ms step_avg:113.57ms
step:225/1393 train_time:24417ms step_avg:113.57ms
step:226/1393 train_time:24531ms step_avg:113.57ms
step:227/1393 train_time:24645ms step_avg:113.57ms
step:228/1393 train_time:24760ms step_avg:113.58ms
step:229/1393 train_time:24874ms step_avg:113.58ms
step:230/1393 train_time:24988ms step_avg:113.58ms
step:231/1393 train_time:25102ms step_avg:113.58ms
step:232/1393 train_time:25217ms step_avg:113.59ms
step:233/1393 train_time:25330ms step_avg:113.59ms
step:234/1393 train_time:25445ms step_avg:113.59ms
step:235/1393 train_time:25559ms step_avg:113.59ms
step:236/1393 train_time:25673ms step_avg:113.60ms
step:237/1393 train_time:25788ms step_avg:113.60ms
step:238/1393 train_time:25902ms step_avg:113.60ms
step:239/1393 train_time:26016ms step_avg:113.61ms
step:240/1393 train_time:26132ms step_avg:113.62ms
step:241/1393 train_time:26244ms step_avg:113.61ms
step:242/1393 train_time:26358ms step_avg:113.61ms
step:243/1393 train_time:26472ms step_avg:113.62ms
step:244/1393 train_time:26587ms step_avg:113.62ms
step:245/1393 train_time:26701ms step_avg:113.62ms
step:246/1393 train_time:26815ms step_avg:113.62ms
step:247/1393 train_time:26929ms step_avg:113.62ms
step:248/1393 train_time:27043ms step_avg:113.63ms
step:249/1393 train_time:27157ms step_avg:113.63ms
step:250/1393 train_time:27273ms step_avg:113.64ms
step:250/1393 val_loss:3.9516 train_time:27385ms step_avg:114.10ms
step:251/1393 train_time:27408ms step_avg:113.73ms
step:252/1393 train_time:27502ms step_avg:113.65ms
step:253/1393 train_time:27628ms step_avg:113.69ms
step:254/1393 train_time:27746ms step_avg:113.71ms
step:255/1393 train_time:27860ms step_avg:113.71ms
step:256/1393 train_time:27974ms step_avg:113.71ms
step:257/1393 train_time:28088ms step_avg:113.72ms
step:258/1393 train_time:28202ms step_avg:113.72ms
step:259/1393 train_time:28317ms step_avg:113.72ms
step:260/1393 train_time:28431ms step_avg:113.72ms
step:261/1393 train_time:28545ms step_avg:113.72ms
step:262/1393 train_time:28658ms step_avg:113.72ms
step:263/1393 train_time:28773ms step_avg:113.73ms
step:264/1393 train_time:28887ms step_avg:113.73ms
step:265/1393 train_time:29002ms step_avg:113.73ms
step:266/1393 train_time:29117ms step_avg:113.74ms
step:267/1393 train_time:29231ms step_avg:113.74ms
step:268/1393 train_time:29345ms step_avg:113.74ms
step:269/1393 train_time:29458ms step_avg:113.74ms
step:270/1393 train_time:29573ms step_avg:113.74ms
step:271/1393 train_time:29687ms step_avg:113.74ms
step:272/1393 train_time:29802ms step_avg:113.75ms
step:273/1393 train_time:29917ms step_avg:113.75ms
step:274/1393 train_time:30031ms step_avg:113.75ms
step:275/1393 train_time:30145ms step_avg:113.75ms
step:276/1393 train_time:30259ms step_avg:113.76ms
step:277/1393 train_time:30374ms step_avg:113.76ms
step:278/1393 train_time:30489ms step_avg:113.76ms
step:279/1393 train_time:30603ms step_avg:113.77ms
step:280/1393 train_time:30717ms step_avg:113.77ms
step:281/1393 train_time:30832ms step_avg:113.77ms
step:282/1393 train_time:30946ms step_avg:113.77ms
step:283/1393 train_time:31059ms step_avg:113.77ms
step:284/1393 train_time:31174ms step_avg:113.77ms
step:285/1393 train_time:31289ms step_avg:113.78ms
step:286/1393 train_time:31403ms step_avg:113.78ms
step:287/1393 train_time:31518ms step_avg:113.78ms
step:288/1393 train_time:31632ms step_avg:113.78ms
step:289/1393 train_time:31745ms step_avg:113.78ms
step:290/1393 train_time:31859ms step_avg:113.78ms
step:291/1393 train_time:31974ms step_avg:113.79ms
step:292/1393 train_time:32088ms step_avg:113.79ms
step:293/1393 train_time:32203ms step_avg:113.79ms
step:294/1393 train_time:32317ms step_avg:113.79ms
step:295/1393 train_time:32431ms step_avg:113.79ms
step:296/1393 train_time:32545ms step_avg:113.80ms
step:297/1393 train_time:32659ms step_avg:113.80ms
step:298/1393 train_time:32774ms step_avg:113.80ms
step:299/1393 train_time:32888ms step_avg:113.80ms
step:300/1393 train_time:33003ms step_avg:113.80ms
step:301/1393 train_time:33116ms step_avg:113.80ms
step:302/1393 train_time:33230ms step_avg:113.80ms
step:303/1393 train_time:33345ms step_avg:113.81ms
step:304/1393 train_time:33459ms step_avg:113.81ms
step:305/1393 train_time:33573ms step_avg:113.81ms
step:306/1393 train_time:33687ms step_avg:113.81ms
step:307/1393 train_time:33802ms step_avg:113.81ms
step:308/1393 train_time:33916ms step_avg:113.81ms
step:309/1393 train_time:34030ms step_avg:113.81ms
step:310/1393 train_time:34144ms step_avg:113.81ms
step:311/1393 train_time:34258ms step_avg:113.81ms
step:312/1393 train_time:34375ms step_avg:113.82ms
step:313/1393 train_time:34492ms step_avg:113.84ms
step:314/1393 train_time:34609ms step_avg:113.84ms
step:315/1393 train_time:34726ms step_avg:113.86ms
step:316/1393 train_time:34844ms step_avg:113.87ms
step:317/1393 train_time:34960ms step_avg:113.88ms
step:318/1393 train_time:35077ms step_avg:113.89ms
step:319/1393 train_time:35194ms step_avg:113.90ms
step:320/1393 train_time:35311ms step_avg:113.91ms
step:321/1393 train_time:35427ms step_avg:113.91ms
step:322/1393 train_time:35545ms step_avg:113.93ms
step:323/1393 train_time:35661ms step_avg:113.93ms
step:324/1393 train_time:35778ms step_avg:113.94ms
step:325/1393 train_time:35894ms step_avg:113.95ms
step:326/1393 train_time:36011ms step_avg:113.96ms
step:327/1393 train_time:36127ms step_avg:113.97ms
step:328/1393 train_time:36244ms step_avg:113.97ms
step:329/1393 train_time:36361ms step_avg:113.98ms
step:330/1393 train_time:36478ms step_avg:113.99ms
step:331/1393 train_time:36595ms step_avg:114.00ms
step:332/1393 train_time:36713ms step_avg:114.02ms
step:333/1393 train_time:36830ms step_avg:114.02ms
step:334/1393 train_time:36947ms step_avg:114.03ms
step:335/1393 train_time:37064ms step_avg:114.04ms
step:336/1393 train_time:37180ms step_avg:114.05ms
step:337/1393 train_time:37297ms step_avg:114.06ms
step:338/1393 train_time:37414ms step_avg:114.07ms
step:339/1393 train_time:37530ms step_avg:114.07ms
step:340/1393 train_time:37647ms step_avg:114.08ms
step:341/1393 train_time:37764ms step_avg:114.09ms
step:342/1393 train_time:37881ms step_avg:114.10ms
step:343/1393 train_time:37998ms step_avg:114.11ms
step:344/1393 train_time:38116ms step_avg:114.12ms
step:345/1393 train_time:38233ms step_avg:114.13ms
step:346/1393 train_time:38350ms step_avg:114.14ms
step:347/1393 train_time:38467ms step_avg:114.14ms
step:348/1393 train_time:38584ms step_avg:114.15ms
step:349/1393 train_time:38701ms step_avg:114.16ms
step:350/1393 train_time:38818ms step_avg:114.17ms
step:351/1393 train_time:38934ms step_avg:114.18ms
step:352/1393 train_time:39051ms step_avg:114.18ms
step:353/1393 train_time:39168ms step_avg:114.19ms
step:354/1393 train_time:39286ms step_avg:114.20ms
step:355/1393 train_time:39404ms step_avg:114.21ms
step:356/1393 train_time:39522ms step_avg:114.22ms
step:357/1393 train_time:39639ms step_avg:114.23ms
step:358/1393 train_time:39756ms step_avg:114.24ms
step:359/1393 train_time:39873ms step_avg:114.25ms
step:360/1393 train_time:39989ms step_avg:114.25ms
step:361/1393 train_time:40106ms step_avg:114.26ms
step:362/1393 train_time:40223ms step_avg:114.27ms
step:363/1393 train_time:40340ms step_avg:114.28ms
step:364/1393 train_time:40456ms step_avg:114.28ms
step:365/1393 train_time:40573ms step_avg:114.29ms
step:366/1393 train_time:40690ms step_avg:114.30ms
step:367/1393 train_time:40807ms step_avg:114.31ms
step:368/1393 train_time:40924ms step_avg:114.31ms
step:369/1393 train_time:41042ms step_avg:114.32ms
step:370/1393 train_time:41159ms step_avg:114.33ms
step:371/1393 train_time:41276ms step_avg:114.34ms
step:372/1393 train_time:41394ms step_avg:114.35ms
step:373/1393 train_time:41510ms step_avg:114.35ms
step:374/1393 train_time:41627ms step_avg:114.36ms
step:375/1393 train_time:41745ms step_avg:114.37ms
step:375/1393 val_loss:3.7594 train_time:41861ms step_avg:114.69ms
step:376/1393 train_time:41883ms step_avg:114.44ms
step:377/1393 train_time:41979ms step_avg:114.39ms
step:378/1393 train_time:42105ms step_avg:114.42ms
step:379/1393 train_time:42226ms step_avg:114.43ms
step:380/1393 train_time:42343ms step_avg:114.44ms
step:381/1393 train_time:42461ms step_avg:114.45ms
step:382/1393 train_time:42577ms step_avg:114.45ms
step:383/1393 train_time:42693ms step_avg:114.46ms
step:384/1393 train_time:42811ms step_avg:114.47ms
step:385/1393 train_time:42927ms step_avg:114.47ms
step:386/1393 train_time:43044ms step_avg:114.48ms
step:387/1393 train_time:43161ms step_avg:114.49ms
step:388/1393 train_time:43278ms step_avg:114.49ms
step:389/1393 train_time:43395ms step_avg:114.50ms
step:390/1393 train_time:43511ms step_avg:114.50ms
step:391/1393 train_time:43627ms step_avg:114.51ms
step:392/1393 train_time:43744ms step_avg:114.51ms
step:393/1393 train_time:43860ms step_avg:114.52ms
step:394/1393 train_time:43978ms step_avg:114.52ms
step:395/1393 train_time:44094ms step_avg:114.53ms
step:396/1393 train_time:44213ms step_avg:114.54ms
step:397/1393 train_time:44330ms step_avg:114.55ms
step:398/1393 train_time:44447ms step_avg:114.55ms
step:399/1393 train_time:44564ms step_avg:114.56ms
step:400/1393 train_time:44681ms step_avg:114.57ms
step:401/1393 train_time:44798ms step_avg:114.57ms
step:402/1393 train_time:44915ms step_avg:114.58ms
step:403/1393 train_time:45032ms step_avg:114.59ms
step:404/1393 train_time:45149ms step_avg:114.59ms
step:405/1393 train_time:45266ms step_avg:114.60ms
step:406/1393 train_time:45382ms step_avg:114.60ms
step:407/1393 train_time:45499ms step_avg:114.61ms
step:408/1393 train_time:45616ms step_avg:114.61ms
step:409/1393 train_time:45734ms step_avg:114.62ms
step:410/1393 train_time:45850ms step_avg:114.63ms
step:411/1393 train_time:45967ms step_avg:114.63ms
step:412/1393 train_time:46083ms step_avg:114.63ms
step:413/1393 train_time:46200ms step_avg:114.64ms
step:414/1393 train_time:46317ms step_avg:114.64ms
step:415/1393 train_time:46434ms step_avg:114.65ms
step:416/1393 train_time:46551ms step_avg:114.66ms
step:417/1393 train_time:46668ms step_avg:114.66ms
step:418/1393 train_time:46786ms step_avg:114.67ms
step:419/1393 train_time:46903ms step_avg:114.68ms
step:420/1393 train_time:47021ms step_avg:114.69ms
step:421/1393 train_time:47140ms step_avg:114.70ms
step:422/1393 train_time:47257ms step_avg:114.70ms
step:423/1393 train_time:47374ms step_avg:114.71ms
step:424/1393 train_time:47492ms step_avg:114.71ms
step:425/1393 train_time:47610ms step_avg:114.72ms
step:426/1393 train_time:47727ms step_avg:114.73ms
step:427/1393 train_time:47845ms step_avg:114.74ms
step:428/1393 train_time:47962ms step_avg:114.74ms
step:429/1393 train_time:48079ms step_avg:114.75ms
step:430/1393 train_time:48196ms step_avg:114.75ms
step:431/1393 train_time:48313ms step_avg:114.76ms
step:432/1393 train_time:48430ms step_avg:114.76ms
step:433/1393 train_time:48548ms step_avg:114.77ms
step:434/1393 train_time:48665ms step_avg:114.78ms
step:435/1393 train_time:48782ms step_avg:114.78ms
step:436/1393 train_time:48900ms step_avg:114.79ms
step:437/1393 train_time:49019ms step_avg:114.80ms
step:438/1393 train_time:49136ms step_avg:114.80ms
step:439/1393 train_time:49253ms step_avg:114.81ms
step:440/1393 train_time:49371ms step_avg:114.82ms
step:441/1393 train_time:49488ms step_avg:114.82ms
step:442/1393 train_time:49605ms step_avg:114.83ms
step:443/1393 train_time:49722ms step_avg:114.83ms
step:444/1393 train_time:49840ms step_avg:114.84ms
step:445/1393 train_time:49957ms step_avg:114.84ms
step:446/1393 train_time:50074ms step_avg:114.85ms
step:447/1393 train_time:50191ms step_avg:114.85ms
step:448/1393 train_time:50307ms step_avg:114.86ms
step:449/1393 train_time:50424ms step_avg:114.86ms
step:450/1393 train_time:50542ms step_avg:114.87ms
step:451/1393 train_time:50659ms step_avg:114.87ms
step:452/1393 train_time:50776ms step_avg:114.88ms
step:453/1393 train_time:50894ms step_avg:114.88ms
step:454/1393 train_time:51011ms step_avg:114.89ms
step:455/1393 train_time:51128ms step_avg:114.89ms
step:456/1393 train_time:51244ms step_avg:114.90ms
step:457/1393 train_time:51361ms step_avg:114.90ms
step:458/1393 train_time:51478ms step_avg:114.91ms
step:459/1393 train_time:51596ms step_avg:114.91ms
step:460/1393 train_time:51715ms step_avg:114.92ms
step:461/1393 train_time:51832ms step_avg:114.93ms
step:462/1393 train_time:51950ms step_avg:114.93ms
step:463/1393 train_time:52067ms step_avg:114.94ms
step:464/1393 train_time:52184ms step_avg:114.94ms
step:465/1393 train_time:52301ms step_avg:114.95ms
step:466/1393 train_time:52418ms step_avg:114.95ms
step:467/1393 train_time:52536ms step_avg:114.96ms
step:468/1393 train_time:52653ms step_avg:114.96ms
step:469/1393 train_time:52770ms step_avg:114.97ms
step:470/1393 train_time:52887ms step_avg:114.97ms
step:471/1393 train_time:53004ms step_avg:114.98ms
step:472/1393 train_time:53121ms step_avg:114.98ms
step:473/1393 train_time:53239ms step_avg:114.99ms
step:474/1393 train_time:53357ms step_avg:114.99ms
step:475/1393 train_time:53475ms step_avg:115.00ms
step:476/1393 train_time:53592ms step_avg:115.00ms
step:477/1393 train_time:53710ms step_avg:115.01ms
step:478/1393 train_time:53827ms step_avg:115.02ms
step:479/1393 train_time:53944ms step_avg:115.02ms
step:480/1393 train_time:54061ms step_avg:115.02ms
step:481/1393 train_time:54179ms step_avg:115.03ms
step:482/1393 train_time:54295ms step_avg:115.03ms
step:483/1393 train_time:54413ms step_avg:115.04ms
step:484/1393 train_time:54530ms step_avg:115.04ms
step:485/1393 train_time:54648ms step_avg:115.05ms
step:486/1393 train_time:54765ms step_avg:115.05ms
step:487/1393 train_time:54882ms step_avg:115.06ms
step:488/1393 train_time:55000ms step_avg:115.06ms
step:489/1393 train_time:55119ms step_avg:115.07ms
step:490/1393 train_time:55236ms step_avg:115.08ms
step:491/1393 train_time:55354ms step_avg:115.08ms
step:492/1393 train_time:55472ms step_avg:115.09ms
step:493/1393 train_time:55590ms step_avg:115.09ms
step:494/1393 train_time:55707ms step_avg:115.10ms
step:495/1393 train_time:55824ms step_avg:115.10ms
step:496/1393 train_time:55941ms step_avg:115.10ms
step:497/1393 train_time:56058ms step_avg:115.11ms
step:498/1393 train_time:56176ms step_avg:115.11ms
step:499/1393 train_time:56294ms step_avg:115.12ms
step:500/1393 train_time:56411ms step_avg:115.13ms
step:500/1393 val_loss:3.6450 train_time:56528ms step_avg:115.36ms
step:501/1393 train_time:56551ms step_avg:115.17ms
step:502/1393 train_time:56649ms step_avg:115.14ms
step:503/1393 train_time:56780ms step_avg:115.17ms
step:504/1393 train_time:56899ms step_avg:115.18ms
step:505/1393 train_time:57016ms step_avg:115.18ms
step:506/1393 train_time:57132ms step_avg:115.19ms
step:507/1393 train_time:57249ms step_avg:115.19ms
step:508/1393 train_time:57366ms step_avg:115.19ms
step:509/1393 train_time:57484ms step_avg:115.20ms
step:510/1393 train_time:57601ms step_avg:115.20ms
step:511/1393 train_time:57718ms step_avg:115.20ms
step:512/1393 train_time:57834ms step_avg:115.21ms
step:513/1393 train_time:57952ms step_avg:115.21ms
step:514/1393 train_time:58070ms step_avg:115.22ms
step:515/1393 train_time:58187ms step_avg:115.22ms
step:516/1393 train_time:58304ms step_avg:115.23ms
step:517/1393 train_time:58422ms step_avg:115.23ms
step:518/1393 train_time:58542ms step_avg:115.24ms
step:519/1393 train_time:58660ms step_avg:115.25ms
step:520/1393 train_time:58780ms step_avg:115.25ms
step:521/1393 train_time:58899ms step_avg:115.26ms
step:522/1393 train_time:59018ms step_avg:115.27ms
step:523/1393 train_time:59138ms step_avg:115.28ms
step:524/1393 train_time:59257ms step_avg:115.29ms
step:525/1393 train_time:59377ms step_avg:115.29ms
step:526/1393 train_time:59496ms step_avg:115.30ms
step:527/1393 train_time:59615ms step_avg:115.31ms
step:528/1393 train_time:59734ms step_avg:115.32ms
step:529/1393 train_time:59853ms step_avg:115.32ms
step:530/1393 train_time:59973ms step_avg:115.33ms
step:531/1393 train_time:60092ms step_avg:115.34ms
step:532/1393 train_time:60212ms step_avg:115.35ms
step:533/1393 train_time:60332ms step_avg:115.36ms
step:534/1393 train_time:60452ms step_avg:115.37ms
step:535/1393 train_time:60572ms step_avg:115.38ms
step:536/1393 train_time:60692ms step_avg:115.38ms
step:537/1393 train_time:60811ms step_avg:115.39ms
step:538/1393 train_time:60931ms step_avg:115.40ms
step:539/1393 train_time:61051ms step_avg:115.41ms
step:540/1393 train_time:61171ms step_avg:115.42ms
step:541/1393 train_time:61291ms step_avg:115.43ms
step:542/1393 train_time:61411ms step_avg:115.43ms
step:543/1393 train_time:61533ms step_avg:115.45ms
step:544/1393 train_time:61653ms step_avg:115.45ms
step:545/1393 train_time:61772ms step_avg:115.46ms
step:546/1393 train_time:61892ms step_avg:115.47ms
step:547/1393 train_time:62011ms step_avg:115.48ms
step:548/1393 train_time:62131ms step_avg:115.48ms
step:549/1393 train_time:62251ms step_avg:115.49ms
step:550/1393 train_time:62370ms step_avg:115.50ms
step:551/1393 train_time:62492ms step_avg:115.51ms
step:552/1393 train_time:62611ms step_avg:115.52ms
step:553/1393 train_time:62730ms step_avg:115.53ms
step:554/1393 train_time:62849ms step_avg:115.53ms
step:555/1393 train_time:62968ms step_avg:115.54ms
step:556/1393 train_time:63088ms step_avg:115.54ms
step:557/1393 train_time:63207ms step_avg:115.55ms
step:558/1393 train_time:63327ms step_avg:115.56ms
step:559/1393 train_time:63446ms step_avg:115.57ms
step:560/1393 train_time:63566ms step_avg:115.57ms
step:561/1393 train_time:63687ms step_avg:115.58ms
step:562/1393 train_time:63808ms step_avg:115.59ms
step:563/1393 train_time:63928ms step_avg:115.60ms
step:564/1393 train_time:64048ms step_avg:115.61ms
step:565/1393 train_time:64169ms step_avg:115.62ms
step:566/1393 train_time:64289ms step_avg:115.63ms
step:567/1393 train_time:64407ms step_avg:115.63ms
step:568/1393 train_time:64527ms step_avg:115.64ms
step:569/1393 train_time:64646ms step_avg:115.65ms
step:570/1393 train_time:64765ms step_avg:115.65ms
step:571/1393 train_time:64885ms step_avg:115.66ms
step:572/1393 train_time:65005ms step_avg:115.67ms
step:573/1393 train_time:65124ms step_avg:115.67ms
step:574/1393 train_time:65244ms step_avg:115.68ms
step:575/1393 train_time:65363ms step_avg:115.69ms
step:576/1393 train_time:65483ms step_avg:115.69ms
step:577/1393 train_time:65602ms step_avg:115.70ms
step:578/1393 train_time:65721ms step_avg:115.71ms
step:579/1393 train_time:65840ms step_avg:115.71ms
step:580/1393 train_time:65959ms step_avg:115.72ms
step:581/1393 train_time:66079ms step_avg:115.72ms
step:582/1393 train_time:66197ms step_avg:115.73ms
step:583/1393 train_time:66316ms step_avg:115.74ms
step:584/1393 train_time:66436ms step_avg:115.74ms
step:585/1393 train_time:66556ms step_avg:115.75ms
step:586/1393 train_time:66676ms step_avg:115.76ms
step:587/1393 train_time:66796ms step_avg:115.76ms
step:588/1393 train_time:66915ms step_avg:115.77ms
step:589/1393 train_time:67034ms step_avg:115.78ms
step:590/1393 train_time:67154ms step_avg:115.78ms
step:591/1393 train_time:67273ms step_avg:115.79ms
step:592/1393 train_time:67392ms step_avg:115.79ms
step:593/1393 train_time:67511ms step_avg:115.80ms
step:594/1393 train_time:67632ms step_avg:115.81ms
step:595/1393 train_time:67752ms step_avg:115.82ms
step:596/1393 train_time:67872ms step_avg:115.82ms
step:597/1393 train_time:67993ms step_avg:115.83ms
step:598/1393 train_time:68113ms step_avg:115.84ms
step:599/1393 train_time:68233ms step_avg:115.85ms
step:600/1393 train_time:68353ms step_avg:115.85ms
step:601/1393 train_time:68473ms step_avg:115.86ms
step:602/1393 train_time:68592ms step_avg:115.86ms
step:603/1393 train_time:68712ms step_avg:115.87ms
step:604/1393 train_time:68832ms step_avg:115.88ms
step:605/1393 train_time:68952ms step_avg:115.89ms
step:606/1393 train_time:69072ms step_avg:115.89ms
step:607/1393 train_time:69191ms step_avg:115.90ms
step:608/1393 train_time:69311ms step_avg:115.91ms
step:609/1393 train_time:69431ms step_avg:115.91ms
step:610/1393 train_time:69550ms step_avg:115.92ms
step:611/1393 train_time:69671ms step_avg:115.93ms
step:612/1393 train_time:69791ms step_avg:115.93ms
step:613/1393 train_time:69912ms step_avg:115.94ms
step:614/1393 train_time:70033ms step_avg:115.95ms
step:615/1393 train_time:70154ms step_avg:115.96ms
step:616/1393 train_time:70273ms step_avg:115.96ms
step:617/1393 train_time:70393ms step_avg:115.97ms
step:618/1393 train_time:70512ms step_avg:115.97ms
step:619/1393 train_time:70631ms step_avg:115.98ms
step:620/1393 train_time:70751ms step_avg:115.99ms
step:621/1393 train_time:70871ms step_avg:115.99ms
step:622/1393 train_time:70991ms step_avg:116.00ms
step:623/1393 train_time:71110ms step_avg:116.00ms
step:624/1393 train_time:71232ms step_avg:116.01ms
step:625/1393 train_time:71352ms step_avg:116.02ms
step:625/1393 val_loss:3.5646 train_time:71471ms step_avg:116.21ms
step:626/1393 train_time:71494ms step_avg:116.06ms
step:627/1393 train_time:71595ms step_avg:116.04ms
step:628/1393 train_time:71722ms step_avg:116.05ms
step:629/1393 train_time:71843ms step_avg:116.06ms
step:630/1393 train_time:71963ms step_avg:116.07ms
step:631/1393 train_time:72083ms step_avg:116.08ms
step:632/1393 train_time:72203ms step_avg:116.08ms
step:633/1393 train_time:72322ms step_avg:116.09ms
step:634/1393 train_time:72442ms step_avg:116.09ms
step:635/1393 train_time:72561ms step_avg:116.10ms
step:636/1393 train_time:72681ms step_avg:116.10ms
step:637/1393 train_time:72801ms step_avg:116.11ms
step:638/1393 train_time:72920ms step_avg:116.12ms
step:639/1393 train_time:73040ms step_avg:116.12ms
step:640/1393 train_time:73161ms step_avg:116.13ms
step:641/1393 train_time:73281ms step_avg:116.13ms
step:642/1393 train_time:73401ms step_avg:116.14ms
step:643/1393 train_time:73520ms step_avg:116.15ms
step:644/1393 train_time:73640ms step_avg:116.15ms
step:645/1393 train_time:73760ms step_avg:116.16ms
step:646/1393 train_time:73879ms step_avg:116.16ms
step:647/1393 train_time:73999ms step_avg:116.17ms
step:648/1393 train_time:74118ms step_avg:116.17ms
step:649/1393 train_time:74238ms step_avg:116.18ms
step:650/1393 train_time:74358ms step_avg:116.18ms
step:651/1393 train_time:74478ms step_avg:116.19ms
step:652/1393 train_time:74598ms step_avg:116.20ms
step:653/1393 train_time:74718ms step_avg:116.20ms
step:654/1393 train_time:74838ms step_avg:116.21ms
step:655/1393 train_time:74958ms step_avg:116.21ms
step:656/1393 train_time:75078ms step_avg:116.22ms
step:657/1393 train_time:75197ms step_avg:116.22ms
step:658/1393 train_time:75317ms step_avg:116.23ms
step:659/1393 train_time:75436ms step_avg:116.23ms
step:660/1393 train_time:75557ms step_avg:116.24ms
step:661/1393 train_time:75676ms step_avg:116.25ms
step:662/1393 train_time:75796ms step_avg:116.25ms
step:663/1393 train_time:75915ms step_avg:116.26ms
step:664/1393 train_time:76035ms step_avg:116.26ms
step:665/1393 train_time:76154ms step_avg:116.27ms
step:666/1393 train_time:76274ms step_avg:116.27ms
step:667/1393 train_time:76394ms step_avg:116.28ms
step:668/1393 train_time:76514ms step_avg:116.28ms
step:669/1393 train_time:76633ms step_avg:116.29ms
step:670/1393 train_time:76753ms step_avg:116.29ms
step:671/1393 train_time:76873ms step_avg:116.30ms
step:672/1393 train_time:76993ms step_avg:116.30ms
step:673/1393 train_time:77113ms step_avg:116.31ms
step:674/1393 train_time:77233ms step_avg:116.31ms
step:675/1393 train_time:77353ms step_avg:116.32ms
step:676/1393 train_time:77473ms step_avg:116.33ms
step:677/1393 train_time:77593ms step_avg:116.33ms
step:678/1393 train_time:77713ms step_avg:116.34ms
step:679/1393 train_time:77833ms step_avg:116.34ms
step:680/1393 train_time:77954ms step_avg:116.35ms
step:681/1393 train_time:78073ms step_avg:116.35ms
step:682/1393 train_time:78193ms step_avg:116.36ms
step:683/1393 train_time:78312ms step_avg:116.36ms
step:684/1393 train_time:78432ms step_avg:116.37ms
step:685/1393 train_time:78552ms step_avg:116.37ms
step:686/1393 train_time:78671ms step_avg:116.38ms
step:687/1393 train_time:78791ms step_avg:116.38ms
step:688/1393 train_time:78911ms step_avg:116.39ms
step:689/1393 train_time:79032ms step_avg:116.39ms
step:690/1393 train_time:79153ms step_avg:116.40ms
step:691/1393 train_time:79273ms step_avg:116.41ms
step:692/1393 train_time:79392ms step_avg:116.41ms
step:693/1393 train_time:79512ms step_avg:116.42ms
step:694/1393 train_time:79632ms step_avg:116.42ms
step:695/1393 train_time:79752ms step_avg:116.43ms
step:696/1393 train_time:79872ms step_avg:116.43ms
step:697/1393 train_time:79992ms step_avg:116.44ms
step:698/1393 train_time:80112ms step_avg:116.44ms
step:699/1393 train_time:80232ms step_avg:116.45ms
step:700/1393 train_time:80352ms step_avg:116.45ms
step:701/1393 train_time:80472ms step_avg:116.46ms
step:702/1393 train_time:80592ms step_avg:116.46ms
step:703/1393 train_time:80712ms step_avg:116.47ms
step:704/1393 train_time:80832ms step_avg:116.47ms
step:705/1393 train_time:80953ms step_avg:116.48ms
step:706/1393 train_time:81072ms step_avg:116.48ms
step:707/1393 train_time:81192ms step_avg:116.49ms
step:708/1393 train_time:81312ms step_avg:116.49ms
step:709/1393 train_time:81431ms step_avg:116.50ms
step:710/1393 train_time:81551ms step_avg:116.50ms
step:711/1393 train_time:81672ms step_avg:116.51ms
step:712/1393 train_time:81791ms step_avg:116.51ms
step:713/1393 train_time:81910ms step_avg:116.52ms
step:714/1393 train_time:82031ms step_avg:116.52ms
step:715/1393 train_time:82153ms step_avg:116.53ms
step:716/1393 train_time:82274ms step_avg:116.54ms
step:717/1393 train_time:82393ms step_avg:116.54ms
step:718/1393 train_time:82513ms step_avg:116.54ms
step:719/1393 train_time:82633ms step_avg:116.55ms
step:720/1393 train_time:82752ms step_avg:116.55ms
step:721/1393 train_time:82871ms step_avg:116.56ms
step:722/1393 train_time:82992ms step_avg:116.56ms
step:723/1393 train_time:83111ms step_avg:116.57ms
step:724/1393 train_time:83233ms step_avg:116.57ms
step:725/1393 train_time:83354ms step_avg:116.58ms
step:726/1393 train_time:83476ms step_avg:116.59ms
step:727/1393 train_time:83597ms step_avg:116.59ms
step:728/1393 train_time:83718ms step_avg:116.60ms
step:729/1393 train_time:83839ms step_avg:116.60ms
step:730/1393 train_time:83962ms step_avg:116.61ms
step:731/1393 train_time:84084ms step_avg:116.62ms
step:732/1393 train_time:84205ms step_avg:116.63ms
step:733/1393 train_time:84327ms step_avg:116.63ms
step:734/1393 train_time:84448ms step_avg:116.64ms
step:735/1393 train_time:84571ms step_avg:116.65ms
step:736/1393 train_time:84694ms step_avg:116.66ms
step:737/1393 train_time:84815ms step_avg:116.66ms
step:738/1393 train_time:84937ms step_avg:116.67ms
step:739/1393 train_time:85058ms step_avg:116.68ms
step:740/1393 train_time:85180ms step_avg:116.68ms
step:741/1393 train_time:85301ms step_avg:116.69ms
step:742/1393 train_time:85422ms step_avg:116.70ms
step:743/1393 train_time:85543ms step_avg:116.70ms
step:744/1393 train_time:85665ms step_avg:116.71ms
step:745/1393 train_time:85786ms step_avg:116.72ms
step:746/1393 train_time:85908ms step_avg:116.72ms
step:747/1393 train_time:86031ms step_avg:116.73ms
step:748/1393 train_time:86154ms step_avg:116.74ms
step:749/1393 train_time:86275ms step_avg:116.75ms
step:750/1393 train_time:86396ms step_avg:116.75ms
step:750/1393 val_loss:3.5137 train_time:86516ms step_avg:116.91ms
step:751/1393 train_time:86538ms step_avg:116.79ms
step:752/1393 train_time:86642ms step_avg:116.77ms
step:753/1393 train_time:86770ms step_avg:116.78ms
step:754/1393 train_time:86892ms step_avg:116.79ms
step:755/1393 train_time:87013ms step_avg:116.80ms
step:756/1393 train_time:87136ms step_avg:116.80ms
step:757/1393 train_time:87258ms step_avg:116.81ms
step:758/1393 train_time:87380ms step_avg:116.82ms
step:759/1393 train_time:87502ms step_avg:116.82ms
step:760/1393 train_time:87623ms step_avg:116.83ms
step:761/1393 train_time:87744ms step_avg:116.84ms
step:762/1393 train_time:87866ms step_avg:116.84ms
step:763/1393 train_time:87988ms step_avg:116.85ms
step:764/1393 train_time:88109ms step_avg:116.86ms
step:765/1393 train_time:88231ms step_avg:116.86ms
step:766/1393 train_time:88352ms step_avg:116.87ms
step:767/1393 train_time:88473ms step_avg:116.87ms
step:768/1393 train_time:88595ms step_avg:116.88ms
step:769/1393 train_time:88720ms step_avg:116.89ms
step:770/1393 train_time:88841ms step_avg:116.90ms
step:771/1393 train_time:88962ms step_avg:116.90ms
step:772/1393 train_time:89083ms step_avg:116.91ms
step:773/1393 train_time:89205ms step_avg:116.91ms
step:774/1393 train_time:89327ms step_avg:116.92ms
step:775/1393 train_time:89448ms step_avg:116.93ms
step:776/1393 train_time:89570ms step_avg:116.93ms
step:777/1393 train_time:89692ms step_avg:116.94ms
step:778/1393 train_time:89812ms step_avg:116.94ms
step:779/1393 train_time:89935ms step_avg:116.95ms
step:780/1393 train_time:90058ms step_avg:116.96ms
step:781/1393 train_time:90179ms step_avg:116.96ms
step:782/1393 train_time:90301ms step_avg:116.97ms
step:783/1393 train_time:90422ms step_avg:116.98ms
step:784/1393 train_time:90545ms step_avg:116.98ms
step:785/1393 train_time:90667ms step_avg:116.99ms
step:786/1393 train_time:90788ms step_avg:117.00ms
step:787/1393 train_time:90911ms step_avg:117.00ms
step:788/1393 train_time:91032ms step_avg:117.01ms
step:789/1393 train_time:91153ms step_avg:117.01ms
step:790/1393 train_time:91275ms step_avg:117.02ms
step:791/1393 train_time:91397ms step_avg:117.03ms
step:792/1393 train_time:91521ms step_avg:117.03ms
step:793/1393 train_time:91643ms step_avg:117.04ms
step:794/1393 train_time:91765ms step_avg:117.05ms
step:795/1393 train_time:91886ms step_avg:117.05ms
step:796/1393 train_time:92009ms step_avg:117.06ms
step:797/1393 train_time:92130ms step_avg:117.06ms
step:798/1393 train_time:92251ms step_avg:117.07ms
step:799/1393 train_time:92373ms step_avg:117.08ms
step:800/1393 train_time:92493ms step_avg:117.08ms
step:801/1393 train_time:92616ms step_avg:117.09ms
step:802/1393 train_time:92739ms step_avg:117.09ms
step:803/1393 train_time:92861ms step_avg:117.10ms
step:804/1393 train_time:92983ms step_avg:117.11ms
step:805/1393 train_time:93105ms step_avg:117.11ms
step:806/1393 train_time:93226ms step_avg:117.12ms
step:807/1393 train_time:93347ms step_avg:117.12ms
step:808/1393 train_time:93469ms step_avg:117.13ms
step:809/1393 train_time:93591ms step_avg:117.13ms
step:810/1393 train_time:93712ms step_avg:117.14ms
step:811/1393 train_time:93834ms step_avg:117.15ms
step:812/1393 train_time:93956ms step_avg:117.15ms
step:813/1393 train_time:94078ms step_avg:117.16ms
step:814/1393 train_time:94199ms step_avg:117.16ms
step:815/1393 train_time:94320ms step_avg:117.17ms
step:816/1393 train_time:94441ms step_avg:117.17ms
step:817/1393 train_time:94562ms step_avg:117.18ms
step:818/1393 train_time:94683ms step_avg:117.18ms
step:819/1393 train_time:94805ms step_avg:117.19ms
step:820/1393 train_time:94927ms step_avg:117.19ms
step:821/1393 train_time:95048ms step_avg:117.20ms
step:822/1393 train_time:95170ms step_avg:117.20ms
step:823/1393 train_time:95292ms step_avg:117.21ms
step:824/1393 train_time:95414ms step_avg:117.22ms
step:825/1393 train_time:95535ms step_avg:117.22ms
step:826/1393 train_time:95657ms step_avg:117.23ms
step:827/1393 train_time:95778ms step_avg:117.23ms
step:828/1393 train_time:95900ms step_avg:117.24ms
step:829/1393 train_time:96022ms step_avg:117.24ms
step:830/1393 train_time:96143ms step_avg:117.25ms
step:831/1393 train_time:96265ms step_avg:117.25ms
step:832/1393 train_time:96387ms step_avg:117.26ms
step:833/1393 train_time:96509ms step_avg:117.26ms
step:834/1393 train_time:96631ms step_avg:117.27ms
step:835/1393 train_time:96752ms step_avg:117.28ms
step:836/1393 train_time:96874ms step_avg:117.28ms
step:837/1393 train_time:96997ms step_avg:117.29ms
step:838/1393 train_time:97120ms step_avg:117.29ms
step:839/1393 train_time:97242ms step_avg:117.30ms
step:840/1393 train_time:97363ms step_avg:117.31ms
step:841/1393 train_time:97485ms step_avg:117.31ms
step:842/1393 train_time:97606ms step_avg:117.32ms
step:843/1393 train_time:97728ms step_avg:117.32ms
step:844/1393 train_time:97850ms step_avg:117.33ms
step:845/1393 train_time:97971ms step_avg:117.33ms
step:846/1393 train_time:98093ms step_avg:117.34ms
step:847/1393 train_time:98215ms step_avg:117.34ms
step:848/1393 train_time:98337ms step_avg:117.35ms
step:849/1393 train_time:98460ms step_avg:117.35ms
step:850/1393 train_time:98583ms step_avg:117.36ms
step:851/1393 train_time:98705ms step_avg:117.37ms
step:852/1393 train_time:98827ms step_avg:117.37ms
step:853/1393 train_time:98949ms step_avg:117.38ms
step:854/1393 train_time:99072ms step_avg:117.38ms
step:855/1393 train_time:99193ms step_avg:117.39ms
step:856/1393 train_time:99315ms step_avg:117.39ms
step:857/1393 train_time:99437ms step_avg:117.40ms
step:858/1393 train_time:99560ms step_avg:117.41ms
step:859/1393 train_time:99681ms step_avg:117.41ms
step:860/1393 train_time:99803ms step_avg:117.42ms
step:861/1393 train_time:99925ms step_avg:117.42ms
step:862/1393 train_time:100048ms step_avg:117.43ms
step:863/1393 train_time:100170ms step_avg:117.43ms
step:864/1393 train_time:100292ms step_avg:117.44ms
step:865/1393 train_time:100413ms step_avg:117.44ms
step:866/1393 train_time:100535ms step_avg:117.45ms
step:867/1393 train_time:100658ms step_avg:117.45ms
step:868/1393 train_time:100779ms step_avg:117.46ms
step:869/1393 train_time:100901ms step_avg:117.46ms
step:870/1393 train_time:101023ms step_avg:117.47ms
step:871/1393 train_time:101144ms step_avg:117.47ms
step:872/1393 train_time:101267ms step_avg:117.48ms
step:873/1393 train_time:101389ms step_avg:117.48ms
step:874/1393 train_time:101511ms step_avg:117.49ms
step:875/1393 train_time:101633ms step_avg:117.49ms
step:875/1393 val_loss:3.4666 train_time:101755ms step_avg:117.64ms
step:876/1393 train_time:101777ms step_avg:117.53ms
step:877/1393 train_time:101881ms step_avg:117.51ms
step:878/1393 train_time:102011ms step_avg:117.52ms
step:879/1393 train_time:102134ms step_avg:117.53ms
step:880/1393 train_time:102256ms step_avg:117.54ms
step:881/1393 train_time:102377ms step_avg:117.54ms
step:882/1393 train_time:102498ms step_avg:117.54ms
step:883/1393 train_time:102620ms step_avg:117.55ms
step:884/1393 train_time:102743ms step_avg:117.55ms
step:885/1393 train_time:102865ms step_avg:117.56ms
step:886/1393 train_time:102987ms step_avg:117.57ms
step:887/1393 train_time:103108ms step_avg:117.57ms
step:888/1393 train_time:103230ms step_avg:117.57ms
step:889/1393 train_time:103353ms step_avg:117.58ms
step:890/1393 train_time:103475ms step_avg:117.59ms
step:891/1393 train_time:103598ms step_avg:117.59ms
step:892/1393 train_time:103720ms step_avg:117.60ms
step:893/1393 train_time:103844ms step_avg:117.60ms
step:894/1393 train_time:103968ms step_avg:117.61ms
step:895/1393 train_time:104090ms step_avg:117.62ms
step:896/1393 train_time:104211ms step_avg:117.62ms
step:897/1393 train_time:104334ms step_avg:117.63ms
step:898/1393 train_time:104456ms step_avg:117.63ms
step:899/1393 train_time:104577ms step_avg:117.63ms
step:900/1393 train_time:104698ms step_avg:117.64ms
step:901/1393 train_time:104822ms step_avg:117.64ms
step:902/1393 train_time:104944ms step_avg:117.65ms
step:903/1393 train_time:105067ms step_avg:117.66ms
step:904/1393 train_time:105189ms step_avg:117.66ms
step:905/1393 train_time:105311ms step_avg:117.67ms
step:906/1393 train_time:105434ms step_avg:117.67ms
step:907/1393 train_time:105556ms step_avg:117.68ms
step:908/1393 train_time:105677ms step_avg:117.68ms
step:909/1393 train_time:105798ms step_avg:117.68ms
step:910/1393 train_time:105921ms step_avg:117.69ms
step:911/1393 train_time:106044ms step_avg:117.70ms
step:912/1393 train_time:106166ms step_avg:117.70ms
step:913/1393 train_time:106288ms step_avg:117.71ms
step:914/1393 train_time:106410ms step_avg:117.71ms
step:915/1393 train_time:106531ms step_avg:117.71ms
step:916/1393 train_time:106653ms step_avg:117.72ms
step:917/1393 train_time:106775ms step_avg:117.72ms
step:918/1393 train_time:106897ms step_avg:117.73ms
step:919/1393 train_time:107021ms step_avg:117.73ms
step:920/1393 train_time:107144ms step_avg:117.74ms
step:921/1393 train_time:107267ms step_avg:117.75ms
step:922/1393 train_time:107389ms step_avg:117.75ms
step:923/1393 train_time:107512ms step_avg:117.76ms
step:924/1393 train_time:107634ms step_avg:117.76ms
step:925/1393 train_time:107757ms step_avg:117.77ms
step:926/1393 train_time:107880ms step_avg:117.77ms
step:927/1393 train_time:108002ms step_avg:117.78ms
step:928/1393 train_time:108124ms step_avg:117.78ms
step:929/1393 train_time:108246ms step_avg:117.79ms
step:930/1393 train_time:108369ms step_avg:117.79ms
step:931/1393 train_time:108494ms step_avg:117.80ms
step:932/1393 train_time:108616ms step_avg:117.81ms
step:933/1393 train_time:108741ms step_avg:117.81ms
step:934/1393 train_time:108864ms step_avg:117.82ms
step:935/1393 train_time:108988ms step_avg:117.83ms
step:936/1393 train_time:109111ms step_avg:117.83ms
step:937/1393 train_time:109235ms step_avg:117.84ms
step:938/1393 train_time:109358ms step_avg:117.84ms
step:939/1393 train_time:109482ms step_avg:117.85ms
step:940/1393 train_time:109605ms step_avg:117.85ms
step:941/1393 train_time:109728ms step_avg:117.86ms
step:942/1393 train_time:109851ms step_avg:117.87ms
step:943/1393 train_time:109976ms step_avg:117.87ms
step:944/1393 train_time:110101ms step_avg:117.88ms
step:945/1393 train_time:110225ms step_avg:117.89ms
step:946/1393 train_time:110350ms step_avg:117.90ms
step:947/1393 train_time:110474ms step_avg:117.90ms
step:948/1393 train_time:110597ms step_avg:117.91ms
step:949/1393 train_time:110720ms step_avg:117.91ms
step:950/1393 train_time:110846ms step_avg:117.92ms
step:951/1393 train_time:110970ms step_avg:117.93ms
step:952/1393 train_time:111094ms step_avg:117.93ms
step:953/1393 train_time:111218ms step_avg:117.94ms
step:954/1393 train_time:111341ms step_avg:117.95ms
step:955/1393 train_time:111465ms step_avg:117.95ms
step:956/1393 train_time:111587ms step_avg:117.96ms
step:957/1393 train_time:111711ms step_avg:117.96ms
step:958/1393 train_time:111834ms step_avg:117.97ms
step:959/1393 train_time:111957ms step_avg:117.97ms
step:960/1393 train_time:112081ms step_avg:117.98ms
step:961/1393 train_time:112205ms step_avg:117.99ms
step:962/1393 train_time:112329ms step_avg:117.99ms
step:963/1393 train_time:112452ms step_avg:118.00ms
step:964/1393 train_time:112576ms step_avg:118.00ms
step:965/1393 train_time:112701ms step_avg:118.01ms
step:966/1393 train_time:112823ms step_avg:118.02ms
step:967/1393 train_time:112946ms step_avg:118.02ms
step:968/1393 train_time:113069ms step_avg:118.03ms
step:969/1393 train_time:113192ms step_avg:118.03ms
step:970/1393 train_time:113315ms step_avg:118.04ms
step:971/1393 train_time:113438ms step_avg:118.04ms
step:972/1393 train_time:113565ms step_avg:118.05ms
step:973/1393 train_time:113688ms step_avg:118.06ms
step:974/1393 train_time:113812ms step_avg:118.06ms
step:975/1393 train_time:113936ms step_avg:118.07ms
step:976/1393 train_time:114061ms step_avg:118.08ms
step:977/1393 train_time:114186ms step_avg:118.08ms
step:978/1393 train_time:114311ms step_avg:118.09ms
step:979/1393 train_time:114435ms step_avg:118.10ms
step:980/1393 train_time:114558ms step_avg:118.10ms
step:981/1393 train_time:114680ms step_avg:118.11ms
step:982/1393 train_time:114803ms step_avg:118.11ms
step:983/1393 train_time:114926ms step_avg:118.11ms
step:984/1393 train_time:115049ms step_avg:118.12ms
step:985/1393 train_time:115172ms step_avg:118.13ms
step:986/1393 train_time:115295ms step_avg:118.13ms
step:987/1393 train_time:115418ms step_avg:118.14ms
step:988/1393 train_time:115542ms step_avg:118.14ms
step:989/1393 train_time:115666ms step_avg:118.15ms
step:990/1393 train_time:115790ms step_avg:118.15ms
step:991/1393 train_time:115913ms step_avg:118.16ms
step:992/1393 train_time:116037ms step_avg:118.16ms
step:993/1393 train_time:116160ms step_avg:118.17ms
step:994/1393 train_time:116287ms step_avg:118.18ms
step:995/1393 train_time:116411ms step_avg:118.18ms
step:996/1393 train_time:116535ms step_avg:118.19ms
step:997/1393 train_time:116658ms step_avg:118.19ms
step:998/1393 train_time:116782ms step_avg:118.20ms
step:999/1393 train_time:116905ms step_avg:118.21ms
step:1000/1393 train_time:117029ms step_avg:118.21ms
step:1000/1393 val_loss:3.4068 train_time:117151ms step_avg:118.33ms
step:1001/1393 train_time:117173ms step_avg:118.24ms
step:1002/1393 train_time:117279ms step_avg:118.22ms
step:1003/1393 train_time:117409ms step_avg:118.24ms
step:1004/1393 train_time:117534ms step_avg:118.24ms
step:1005/1393 train_time:117658ms step_avg:118.25ms
step:1006/1393 train_time:117781ms step_avg:118.25ms
step:1007/1393 train_time:117905ms step_avg:118.26ms
step:1008/1393 train_time:118029ms step_avg:118.27ms
step:1009/1393 train_time:118153ms step_avg:118.27ms
step:1010/1393 train_time:118276ms step_avg:118.28ms
step:1011/1393 train_time:118400ms step_avg:118.28ms
step:1012/1393 train_time:118523ms step_avg:118.29ms
step:1013/1393 train_time:118647ms step_avg:118.29ms
step:1014/1393 train_time:118771ms step_avg:118.30ms
step:1015/1393 train_time:118894ms step_avg:118.30ms
step:1016/1393 train_time:119017ms step_avg:118.31ms
step:1017/1393 train_time:119139ms step_avg:118.31ms
step:1018/1393 train_time:119263ms step_avg:118.32ms
step:1019/1393 train_time:119386ms step_avg:118.32ms
step:1020/1393 train_time:119510ms step_avg:118.33ms
step:1021/1393 train_time:119633ms step_avg:118.33ms
step:1022/1393 train_time:119758ms step_avg:118.34ms
step:1023/1393 train_time:119883ms step_avg:118.34ms
step:1024/1393 train_time:120005ms step_avg:118.35ms
step:1025/1393 train_time:120129ms step_avg:118.35ms
step:1026/1393 train_time:120253ms step_avg:118.36ms
step:1027/1393 train_time:120377ms step_avg:118.36ms
step:1028/1393 train_time:120500ms step_avg:118.37ms
step:1029/1393 train_time:120624ms step_avg:118.38ms
step:1030/1393 train_time:120748ms step_avg:118.38ms
step:1031/1393 train_time:120871ms step_avg:118.39ms
step:1032/1393 train_time:120994ms step_avg:118.39ms
step:1033/1393 train_time:121118ms step_avg:118.39ms
step:1034/1393 train_time:121241ms step_avg:118.40ms
step:1035/1393 train_time:121365ms step_avg:118.40ms
step:1036/1393 train_time:121488ms step_avg:118.41ms
step:1037/1393 train_time:121612ms step_avg:118.41ms
step:1038/1393 train_time:121735ms step_avg:118.42ms
step:1039/1393 train_time:121858ms step_avg:118.42ms
step:1040/1393 train_time:121982ms step_avg:118.43ms
step:1041/1393 train_time:122105ms step_avg:118.43ms
step:1042/1393 train_time:122229ms step_avg:118.44ms
step:1043/1393 train_time:122353ms step_avg:118.44ms
step:1044/1393 train_time:122479ms step_avg:118.45ms
step:1045/1393 train_time:122602ms step_avg:118.46ms
step:1046/1393 train_time:122726ms step_avg:118.46ms
step:1047/1393 train_time:122849ms step_avg:118.47ms
step:1048/1393 train_time:122973ms step_avg:118.47ms
step:1049/1393 train_time:123096ms step_avg:118.48ms
step:1050/1393 train_time:123220ms step_avg:118.48ms
step:1051/1393 train_time:123343ms step_avg:118.49ms
step:1052/1393 train_time:123467ms step_avg:118.49ms
step:1053/1393 train_time:123592ms step_avg:118.50ms
step:1054/1393 train_time:123716ms step_avg:118.50ms
step:1055/1393 train_time:123841ms step_avg:118.51ms
step:1056/1393 train_time:123964ms step_avg:118.51ms
step:1057/1393 train_time:124088ms step_avg:118.52ms
step:1058/1393 train_time:124211ms step_avg:118.52ms
step:1059/1393 train_time:124335ms step_avg:118.53ms
step:1060/1393 train_time:124459ms step_avg:118.53ms
step:1061/1393 train_time:124582ms step_avg:118.54ms
step:1062/1393 train_time:124704ms step_avg:118.54ms
step:1063/1393 train_time:124830ms step_avg:118.55ms
step:1064/1393 train_time:124954ms step_avg:118.55ms
step:1065/1393 train_time:125078ms step_avg:118.56ms
step:1066/1393 train_time:125202ms step_avg:118.56ms
step:1067/1393 train_time:125326ms step_avg:118.57ms
step:1068/1393 train_time:125451ms step_avg:118.57ms
step:1069/1393 train_time:125575ms step_avg:118.58ms
step:1070/1393 train_time:125699ms step_avg:118.58ms
step:1071/1393 train_time:125823ms step_avg:118.59ms
step:1072/1393 train_time:125947ms step_avg:118.59ms
step:1073/1393 train_time:126071ms step_avg:118.60ms
step:1074/1393 train_time:126195ms step_avg:118.60ms
step:1075/1393 train_time:126318ms step_avg:118.61ms
step:1076/1393 train_time:126442ms step_avg:118.61ms
step:1077/1393 train_time:126566ms step_avg:118.62ms
step:1078/1393 train_time:126689ms step_avg:118.62ms
step:1079/1393 train_time:126812ms step_avg:118.63ms
step:1080/1393 train_time:126938ms step_avg:118.63ms
step:1081/1393 train_time:127063ms step_avg:118.64ms
step:1082/1393 train_time:127187ms step_avg:118.64ms
step:1083/1393 train_time:127311ms step_avg:118.65ms
step:1084/1393 train_time:127436ms step_avg:118.66ms
step:1085/1393 train_time:127559ms step_avg:118.66ms
step:1086/1393 train_time:127684ms step_avg:118.67ms
step:1087/1393 train_time:127809ms step_avg:118.67ms
step:1088/1393 train_time:127931ms step_avg:118.67ms
step:1089/1393 train_time:128058ms step_avg:118.68ms
step:1090/1393 train_time:128183ms step_avg:118.69ms
step:1091/1393 train_time:128307ms step_avg:118.69ms
step:1092/1393 train_time:128433ms step_avg:118.70ms
step:1093/1393 train_time:128557ms step_avg:118.70ms
step:1094/1393 train_time:128681ms step_avg:118.71ms
step:1095/1393 train_time:128805ms step_avg:118.71ms
step:1096/1393 train_time:128929ms step_avg:118.72ms
step:1097/1393 train_time:129052ms step_avg:118.72ms
step:1098/1393 train_time:129176ms step_avg:118.73ms
step:1099/1393 train_time:129299ms step_avg:118.73ms
step:1100/1393 train_time:129422ms step_avg:118.74ms
step:1101/1393 train_time:129547ms step_avg:118.74ms
step:1102/1393 train_time:129674ms step_avg:118.75ms
step:1103/1393 train_time:129797ms step_avg:118.75ms
step:1104/1393 train_time:129921ms step_avg:118.76ms
step:1105/1393 train_time:130044ms step_avg:118.76ms
step:1106/1393 train_time:130167ms step_avg:118.77ms
step:1107/1393 train_time:130291ms step_avg:118.77ms
step:1108/1393 train_time:130416ms step_avg:118.78ms
step:1109/1393 train_time:130540ms step_avg:118.78ms
step:1110/1393 train_time:130664ms step_avg:118.79ms
step:1111/1393 train_time:130789ms step_avg:118.79ms
step:1112/1393 train_time:130914ms step_avg:118.80ms
step:1113/1393 train_time:131038ms step_avg:118.80ms
step:1114/1393 train_time:131161ms step_avg:118.81ms
step:1115/1393 train_time:131286ms step_avg:118.81ms
step:1116/1393 train_time:131409ms step_avg:118.81ms
step:1117/1393 train_time:131533ms step_avg:118.82ms
step:1118/1393 train_time:131657ms step_avg:118.82ms
step:1119/1393 train_time:131780ms step_avg:118.83ms
step:1120/1393 train_time:131906ms step_avg:118.83ms
step:1121/1393 train_time:132030ms step_avg:118.84ms
step:1122/1393 train_time:132153ms step_avg:118.84ms
step:1123/1393 train_time:132278ms step_avg:118.85ms
step:1124/1393 train_time:132401ms step_avg:118.85ms
step:1125/1393 train_time:132524ms step_avg:118.86ms
step:1125/1393 val_loss:3.3567 train_time:132647ms step_avg:118.97ms
step:1126/1393 train_time:132670ms step_avg:118.88ms
step:1127/1393 train_time:132777ms step_avg:118.87ms
step:1128/1393 train_time:132907ms step_avg:118.88ms
step:1129/1393 train_time:133032ms step_avg:118.88ms
step:1130/1393 train_time:133155ms step_avg:118.89ms
step:1131/1393 train_time:133279ms step_avg:118.89ms
step:1132/1393 train_time:133403ms step_avg:118.90ms
step:1133/1393 train_time:133527ms step_avg:118.90ms
step:1134/1393 train_time:133650ms step_avg:118.91ms
step:1135/1393 train_time:133775ms step_avg:118.91ms
step:1136/1393 train_time:133900ms step_avg:118.92ms
step:1137/1393 train_time:134023ms step_avg:118.92ms
step:1138/1393 train_time:134150ms step_avg:118.93ms
step:1139/1393 train_time:134277ms step_avg:118.93ms
step:1140/1393 train_time:134402ms step_avg:118.94ms
step:1141/1393 train_time:134528ms step_avg:118.95ms
step:1142/1393 train_time:134652ms step_avg:118.95ms
step:1143/1393 train_time:134777ms step_avg:118.96ms
step:1144/1393 train_time:134902ms step_avg:118.96ms
step:1145/1393 train_time:135027ms step_avg:118.97ms
step:1146/1393 train_time:135152ms step_avg:118.97ms
step:1147/1393 train_time:135278ms step_avg:118.98ms
step:1148/1393 train_time:135403ms step_avg:118.98ms
step:1149/1393 train_time:135528ms step_avg:118.99ms
step:1150/1393 train_time:135652ms step_avg:118.99ms
step:1151/1393 train_time:135779ms step_avg:119.00ms
step:1152/1393 train_time:135904ms step_avg:119.01ms
step:1153/1393 train_time:136028ms step_avg:119.01ms
step:1154/1393 train_time:136153ms step_avg:119.01ms
step:1155/1393 train_time:136278ms step_avg:119.02ms
step:1156/1393 train_time:136404ms step_avg:119.03ms
step:1157/1393 train_time:136534ms step_avg:119.04ms
step:1158/1393 train_time:136659ms step_avg:119.04ms
step:1159/1393 train_time:136784ms step_avg:119.05ms
step:1160/1393 train_time:136913ms step_avg:119.05ms
step:1161/1393 train_time:137038ms step_avg:119.06ms
step:1162/1393 train_time:137163ms step_avg:119.07ms
step:1163/1393 train_time:137292ms step_avg:119.07ms
step:1164/1393 train_time:137416ms step_avg:119.08ms
step:1165/1393 train_time:137541ms step_avg:119.08ms
step:1166/1393 train_time:137670ms step_avg:119.09ms
step:1167/1393 train_time:137798ms step_avg:119.10ms
step:1168/1393 train_time:137923ms step_avg:119.10ms
step:1169/1393 train_time:138048ms step_avg:119.11ms
step:1170/1393 train_time:138179ms step_avg:119.12ms
step:1171/1393 train_time:138305ms step_avg:119.13ms
step:1172/1393 train_time:138430ms step_avg:119.13ms
step:1173/1393 train_time:138554ms step_avg:119.13ms
step:1174/1393 train_time:138680ms step_avg:119.14ms
step:1175/1393 train_time:138805ms step_avg:119.15ms
step:1176/1393 train_time:138930ms step_avg:119.15ms
step:1177/1393 train_time:139056ms step_avg:119.16ms
step:1178/1393 train_time:139181ms step_avg:119.16ms
step:1179/1393 train_time:139308ms step_avg:119.17ms
step:1180/1393 train_time:139434ms step_avg:119.17ms
step:1181/1393 train_time:139559ms step_avg:119.18ms
step:1182/1393 train_time:139684ms step_avg:119.18ms
step:1183/1393 train_time:139808ms step_avg:119.19ms
step:1184/1393 train_time:139933ms step_avg:119.19ms
step:1185/1393 train_time:140064ms step_avg:119.20ms
step:1186/1393 train_time:140189ms step_avg:119.21ms
step:1187/1393 train_time:140313ms step_avg:119.21ms
step:1188/1393 train_time:140438ms step_avg:119.22ms
step:1189/1393 train_time:140563ms step_avg:119.22ms
step:1190/1393 train_time:140688ms step_avg:119.23ms
step:1191/1393 train_time:140813ms step_avg:119.23ms
step:1192/1393 train_time:140940ms step_avg:119.24ms
step:1193/1393 train_time:141068ms step_avg:119.25ms
step:1194/1393 train_time:141192ms step_avg:119.25ms
step:1195/1393 train_time:141317ms step_avg:119.25ms
step:1196/1393 train_time:141443ms step_avg:119.26ms
step:1197/1393 train_time:141572ms step_avg:119.27ms
step:1198/1393 train_time:141695ms step_avg:119.27ms
step:1199/1393 train_time:141823ms step_avg:119.28ms
step:1200/1393 train_time:141947ms step_avg:119.28ms
step:1201/1393 train_time:142072ms step_avg:119.29ms
step:1202/1393 train_time:142198ms step_avg:119.29ms
step:1203/1393 train_time:142322ms step_avg:119.30ms
step:1204/1393 train_time:142447ms step_avg:119.30ms
step:1205/1393 train_time:142572ms step_avg:119.31ms
step:1206/1393 train_time:142696ms step_avg:119.31ms
step:1207/1393 train_time:142823ms step_avg:119.32ms
step:1208/1393 train_time:142948ms step_avg:119.32ms
step:1209/1393 train_time:143073ms step_avg:119.33ms
step:1210/1393 train_time:143198ms step_avg:119.33ms
step:1211/1393 train_time:143322ms step_avg:119.34ms
step:1212/1393 train_time:143450ms step_avg:119.34ms
step:1213/1393 train_time:143573ms step_avg:119.35ms
step:1214/1393 train_time:143700ms step_avg:119.35ms
step:1215/1393 train_time:143824ms step_avg:119.36ms
step:1216/1393 train_time:143948ms step_avg:119.36ms
step:1217/1393 train_time:144073ms step_avg:119.36ms
step:1218/1393 train_time:144199ms step_avg:119.37ms
step:1219/1393 train_time:144324ms step_avg:119.37ms
step:1220/1393 train_time:144455ms step_avg:119.38ms
step:1221/1393 train_time:144579ms step_avg:119.39ms
step:1222/1393 train_time:144706ms step_avg:119.39ms
step:1223/1393 train_time:144833ms step_avg:119.40ms
step:1224/1393 train_time:144957ms step_avg:119.40ms
step:1225/1393 train_time:145082ms step_avg:119.41ms
step:1226/1393 train_time:145208ms step_avg:119.41ms
step:1227/1393 train_time:145332ms step_avg:119.42ms
step:1228/1393 train_time:145457ms step_avg:119.42ms
step:1229/1393 train_time:145583ms step_avg:119.43ms
step:1230/1393 train_time:145707ms step_avg:119.43ms
step:1231/1393 train_time:145831ms step_avg:119.44ms
step:1232/1393 train_time:145956ms step_avg:119.44ms
step:1233/1393 train_time:146081ms step_avg:119.44ms
step:1234/1393 train_time:146207ms step_avg:119.45ms
step:1235/1393 train_time:146332ms step_avg:119.46ms
step:1236/1393 train_time:146462ms step_avg:119.46ms
step:1237/1393 train_time:146586ms step_avg:119.47ms
step:1238/1393 train_time:146712ms step_avg:119.47ms
step:1239/1393 train_time:146839ms step_avg:119.48ms
step:1240/1393 train_time:146968ms step_avg:119.49ms
step:1241/1393 train_time:147092ms step_avg:119.49ms
step:1242/1393 train_time:147217ms step_avg:119.49ms
step:1243/1393 train_time:147344ms step_avg:119.50ms
step:1244/1393 train_time:147472ms step_avg:119.51ms
step:1245/1393 train_time:147597ms step_avg:119.51ms
step:1246/1393 train_time:147722ms step_avg:119.52ms
step:1247/1393 train_time:147847ms step_avg:119.52ms
step:1248/1393 train_time:147974ms step_avg:119.53ms
step:1249/1393 train_time:148099ms step_avg:119.53ms
step:1250/1393 train_time:148224ms step_avg:119.54ms
step:1250/1393 val_loss:3.3125 train_time:148351ms step_avg:119.64ms
step:1251/1393 train_time:148373ms step_avg:119.56ms
step:1252/1393 train_time:148482ms step_avg:119.55ms
step:1253/1393 train_time:148610ms step_avg:119.56ms
step:1254/1393 train_time:148736ms step_avg:119.56ms
step:1255/1393 train_time:148860ms step_avg:119.57ms
step:1256/1393 train_time:148986ms step_avg:119.57ms
step:1257/1393 train_time:149110ms step_avg:119.57ms
step:1258/1393 train_time:149235ms step_avg:119.58ms
step:1259/1393 train_time:149360ms step_avg:119.58ms
step:1260/1393 train_time:149485ms step_avg:119.59ms
step:1261/1393 train_time:149613ms step_avg:119.60ms
step:1262/1393 train_time:149740ms step_avg:119.60ms
step:1263/1393 train_time:149864ms step_avg:119.60ms
step:1264/1393 train_time:149989ms step_avg:119.61ms
step:1265/1393 train_time:150114ms step_avg:119.61ms
step:1266/1393 train_time:150239ms step_avg:119.62ms
step:1267/1393 train_time:150365ms step_avg:119.62ms
step:1268/1393 train_time:150492ms step_avg:119.63ms
step:1269/1393 train_time:150617ms step_avg:119.63ms
step:1270/1393 train_time:150744ms step_avg:119.64ms
step:1271/1393 train_time:150870ms step_avg:119.64ms
step:1272/1393 train_time:150998ms step_avg:119.65ms
step:1273/1393 train_time:151123ms step_avg:119.65ms
step:1274/1393 train_time:151251ms step_avg:119.66ms
step:1275/1393 train_time:151377ms step_avg:119.67ms
step:1276/1393 train_time:151501ms step_avg:119.67ms
step:1277/1393 train_time:151628ms step_avg:119.68ms
step:1278/1393 train_time:151754ms step_avg:119.68ms
step:1279/1393 train_time:151879ms step_avg:119.68ms
step:1280/1393 train_time:152004ms step_avg:119.69ms
step:1281/1393 train_time:152129ms step_avg:119.69ms
step:1282/1393 train_time:152254ms step_avg:119.70ms
step:1283/1393 train_time:152381ms step_avg:119.70ms
step:1284/1393 train_time:152507ms step_avg:119.71ms
step:1285/1393 train_time:152633ms step_avg:119.71ms
step:1286/1393 train_time:152758ms step_avg:119.72ms
step:1287/1393 train_time:152885ms step_avg:119.72ms
step:1288/1393 train_time:153009ms step_avg:119.73ms
step:1289/1393 train_time:153136ms step_avg:119.73ms
step:1290/1393 train_time:153261ms step_avg:119.74ms
step:1291/1393 train_time:153386ms step_avg:119.74ms
step:1292/1393 train_time:153510ms step_avg:119.74ms
step:1293/1393 train_time:153635ms step_avg:119.75ms
step:1294/1393 train_time:153762ms step_avg:119.75ms
step:1295/1393 train_time:153888ms step_avg:119.76ms
step:1296/1393 train_time:154014ms step_avg:119.76ms
step:1297/1393 train_time:154139ms step_avg:119.77ms
step:1298/1393 train_time:154264ms step_avg:119.77ms
step:1299/1393 train_time:154391ms step_avg:119.78ms
step:1300/1393 train_time:154516ms step_avg:119.78ms
step:1301/1393 train_time:154641ms step_avg:119.78ms
step:1302/1393 train_time:154766ms step_avg:119.79ms
step:1303/1393 train_time:154894ms step_avg:119.79ms
step:1304/1393 train_time:155019ms step_avg:119.80ms
step:1305/1393 train_time:155143ms step_avg:119.80ms
step:1306/1393 train_time:155270ms step_avg:119.81ms
step:1307/1393 train_time:155396ms step_avg:119.81ms
step:1308/1393 train_time:155521ms step_avg:119.82ms
step:1309/1393 train_time:155651ms step_avg:119.82ms
step:1310/1393 train_time:155777ms step_avg:119.83ms
step:1311/1393 train_time:155903ms step_avg:119.83ms
step:1312/1393 train_time:156030ms step_avg:119.84ms
step:1313/1393 train_time:156157ms step_avg:119.84ms
step:1314/1393 train_time:156285ms step_avg:119.85ms
step:1315/1393 train_time:156410ms step_avg:119.85ms
step:1316/1393 train_time:156535ms step_avg:119.86ms
step:1317/1393 train_time:156664ms step_avg:119.87ms
step:1318/1393 train_time:156789ms step_avg:119.87ms
step:1319/1393 train_time:156914ms step_avg:119.87ms
step:1320/1393 train_time:157040ms step_avg:119.88ms
step:1321/1393 train_time:157166ms step_avg:119.88ms
step:1322/1393 train_time:157289ms step_avg:119.89ms
step:1323/1393 train_time:157415ms step_avg:119.89ms
step:1324/1393 train_time:157539ms step_avg:119.89ms
step:1325/1393 train_time:157664ms step_avg:119.90ms
step:1326/1393 train_time:157790ms step_avg:119.90ms
step:1327/1393 train_time:157915ms step_avg:119.91ms
step:1328/1393 train_time:158041ms step_avg:119.91ms
step:1329/1393 train_time:158166ms step_avg:119.91ms
step:1330/1393 train_time:158292ms step_avg:119.92ms
step:1331/1393 train_time:158416ms step_avg:119.92ms
step:1332/1393 train_time:158543ms step_avg:119.93ms
step:1333/1393 train_time:158670ms step_avg:119.93ms
step:1334/1393 train_time:158795ms step_avg:119.94ms
step:1335/1393 train_time:158920ms step_avg:119.94ms
step:1336/1393 train_time:159044ms step_avg:119.94ms
step:1337/1393 train_time:159170ms step_avg:119.95ms
step:1338/1393 train_time:159297ms step_avg:119.95ms
step:1339/1393 train_time:159422ms step_avg:119.96ms
step:1340/1393 train_time:159547ms step_avg:119.96ms
step:1341/1393 train_time:159674ms step_avg:119.97ms
step:1342/1393 train_time:159800ms step_avg:119.97ms
step:1343/1393 train_time:159926ms step_avg:119.97ms
step:1344/1393 train_time:160054ms step_avg:119.98ms
step:1345/1393 train_time:160178ms step_avg:119.98ms
step:1346/1393 train_time:160305ms step_avg:119.99ms
step:1347/1393 train_time:160432ms step_avg:119.99ms
step:1348/1393 train_time:160558ms step_avg:120.00ms
step:1349/1393 train_time:160690ms step_avg:120.01ms
step:1350/1393 train_time:160815ms step_avg:120.01ms
step:1351/1393 train_time:160943ms step_avg:120.02ms
step:1352/1393 train_time:161071ms step_avg:120.02ms
step:1353/1393 train_time:161198ms step_avg:120.03ms
step:1354/1393 train_time:161328ms step_avg:120.04ms
step:1355/1393 train_time:161454ms step_avg:120.04ms
step:1356/1393 train_time:161580ms step_avg:120.04ms
step:1357/1393 train_time:161707ms step_avg:120.05ms
step:1358/1393 train_time:161833ms step_avg:120.05ms
step:1359/1393 train_time:161963ms step_avg:120.06ms
step:1360/1393 train_time:162094ms step_avg:120.07ms
step:1361/1393 train_time:162224ms step_avg:120.08ms
step:1362/1393 train_time:162351ms step_avg:120.08ms
step:1363/1393 train_time:162477ms step_avg:120.09ms
step:1364/1393 train_time:162607ms step_avg:120.09ms
step:1365/1393 train_time:162733ms step_avg:120.10ms
step:1366/1393 train_time:162858ms step_avg:120.10ms
step:1367/1393 train_time:162986ms step_avg:120.11ms
step:1368/1393 train_time:163111ms step_avg:120.11ms
step:1369/1393 train_time:163236ms step_avg:120.11ms
step:1370/1393 train_time:163362ms step_avg:120.12ms
step:1371/1393 train_time:163488ms step_avg:120.12ms
step:1372/1393 train_time:163615ms step_avg:120.13ms
step:1373/1393 train_time:163740ms step_avg:120.13ms
step:1374/1393 train_time:163865ms step_avg:120.14ms
step:1375/1393 train_time:163991ms step_avg:120.14ms
step:1375/1393 val_loss:3.2790 train_time:164117ms step_avg:120.23ms
step:1376/1393 train_time:164140ms step_avg:120.16ms
step:1377/1393 train_time:164248ms step_avg:120.15ms
step:1378/1393 train_time:164377ms step_avg:120.16ms
step:1379/1393 train_time:164507ms step_avg:120.17ms
step:1380/1393 train_time:164633ms step_avg:120.17ms
step:1381/1393 train_time:164759ms step_avg:120.17ms
step:1382/1393 train_time:164886ms step_avg:120.18ms
step:1383/1393 train_time:165014ms step_avg:120.19ms
step:1384/1393 train_time:165139ms step_avg:120.19ms
step:1385/1393 train_time:165266ms step_avg:120.19ms
step:1386/1393 train_time:165391ms step_avg:120.20ms
step:1387/1393 train_time:165517ms step_avg:120.20ms
step:1388/1393 train_time:165649ms step_avg:120.21ms
step:1389/1393 train_time:165774ms step_avg:120.21ms
step:1390/1393 train_time:165900ms step_avg:120.22ms
step:1391/1393 train_time:166026ms step_avg:120.22ms
step:1392/1393 train_time:166152ms step_avg:120.23ms
step:1393/1393 train_time:166278ms step_avg:120.23ms
step:1393/1393 val_loss:3.2756 train_time:166405ms step_avg:120.32ms
peak memory allocated: 31573 MiB reserved: 33076 MiB
