import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)
flex_kernel_options = None
if torch.cuda.get_device_name(0).endswith(("3090", "4090")):
    flex_kernel_options = {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_M1": 32, "BLOCK_N1": 64, "BLOCK_M2": 64, "BLOCK_N2": 32}

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale, kernel_options=flex_kernel_options)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor = None, sliding_window_num_blocks: Tensor = 0):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 28415).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x) if not self.training else lm_head_fp8(x, self.lm_head.weight)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)

        if target_seq is None:
            return logits

        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb-tokmon-10B/english-28416-balanced/fineweb-tokmon_train_*.bin" # input .bin to train on
    val_files = "data/fineweb-tokmon-10B/english-28416-balanced/fineweb-tokmon_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # fewer tokens but equivalent text for validation, snapped to nearest seq_len
    val_ratio = 0.99011 # equivalent token density on validation tokens to that of GPT-2
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()


def main():
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

    model = GPT(vocab_size=28416, num_layers=12, num_heads=6, model_dim=768).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    k = 1.08
    adam_params = [dict(params=head_params, lr=0.008*k), dict(params=embed_params, lr=0.6*k), dict(params=scalar_params, lr=0.04*k)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05*k, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model)
    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss = (val_loss * args.val_ratio) / val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
    )
    dist.destroy_process_group()


if __name__ == "__main__":
    main()
====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu124 compiled for CUDA 12.4
Mon Jan 20 16:54:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   36C    P0            127W /  700W |    7714MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   37C    P0            121W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   35C    P0            118W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   34C    P0            116W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   38C    P0            122W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   36C    P0            118W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   36C    P0            118W /  700W |    3212MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      9496      C   /usr/bin/python3                             3394MiB |
|    0   N/A  N/A      9497      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A      9498      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A      9499      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A      9500      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A      9501      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A      9502      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A      9503      C   /usr/bin/python3                              610MiB |
|    1   N/A  N/A      9497      C   /usr/bin/python3                             3442MiB |
|    2   N/A  N/A      9498      C   /usr/bin/python3                             3442MiB |
|    3   N/A  N/A      9499      C   /usr/bin/python3                             3442MiB |
|    4   N/A  N/A      9500      C   /usr/bin/python3                             3442MiB |
|    5   N/A  N/A      9501      C   /usr/bin/python3                             3442MiB |
|    6   N/A  N/A      9502      C   /usr/bin/python3                             3442MiB |
|    7   N/A  N/A      9503      C   /usr/bin/python3                             3202MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.1533 train_time:0ms step_avg:nanms
step:1/1393 train_time:652932ms step_avg:nanms
step:2/1393 train_time:652966ms step_avg:nanms
step:3/1393 train_time:653950ms step_avg:nanms
step:4/1393 train_time:654061ms step_avg:nanms
step:5/1393 train_time:654174ms step_avg:nanms
step:6/1393 train_time:654286ms step_avg:nanms
step:7/1393 train_time:654400ms step_avg:nanms
step:8/1393 train_time:654513ms step_avg:nanms
step:9/1393 train_time:654625ms step_avg:nanms
step:10/1393 train_time:654738ms step_avg:nanms
step:11/1393 train_time:113ms step_avg:nanms
step:12/1393 train_time:227ms step_avg:nanms
step:13/1393 train_time:341ms step_avg:113.51ms
step:14/1393 train_time:453ms step_avg:113.35ms
step:15/1393 train_time:567ms step_avg:113.36ms
step:16/1393 train_time:680ms step_avg:113.35ms
step:17/1393 train_time:792ms step_avg:113.17ms
step:18/1393 train_time:905ms step_avg:113.15ms
step:19/1393 train_time:1018ms step_avg:113.12ms
step:20/1393 train_time:1130ms step_avg:113.04ms
step:21/1393 train_time:1243ms step_avg:113.01ms
step:22/1393 train_time:1356ms step_avg:112.97ms
step:23/1393 train_time:1469ms step_avg:113.01ms
step:24/1393 train_time:1583ms step_avg:113.04ms
step:25/1393 train_time:1695ms step_avg:112.99ms
step:26/1393 train_time:1809ms step_avg:113.03ms
step:27/1393 train_time:1922ms step_avg:113.07ms
step:28/1393 train_time:2036ms step_avg:113.09ms
step:29/1393 train_time:2149ms step_avg:113.13ms
step:30/1393 train_time:2264ms step_avg:113.20ms
step:31/1393 train_time:2377ms step_avg:113.19ms
step:32/1393 train_time:2490ms step_avg:113.17ms
step:33/1393 train_time:2604ms step_avg:113.20ms
step:34/1393 train_time:2716ms step_avg:113.15ms
step:35/1393 train_time:2828ms step_avg:113.12ms
step:36/1393 train_time:2941ms step_avg:113.13ms
step:37/1393 train_time:3054ms step_avg:113.10ms
step:38/1393 train_time:3167ms step_avg:113.12ms
step:39/1393 train_time:3280ms step_avg:113.11ms
step:40/1393 train_time:3393ms step_avg:113.09ms
step:41/1393 train_time:3506ms step_avg:113.09ms
step:42/1393 train_time:3619ms step_avg:113.10ms
step:43/1393 train_time:3733ms step_avg:113.11ms
step:44/1393 train_time:3846ms step_avg:113.13ms
step:45/1393 train_time:3960ms step_avg:113.13ms
step:46/1393 train_time:4072ms step_avg:113.12ms
step:47/1393 train_time:4185ms step_avg:113.12ms
step:48/1393 train_time:4299ms step_avg:113.12ms
step:49/1393 train_time:4412ms step_avg:113.12ms
step:50/1393 train_time:4525ms step_avg:113.11ms
step:51/1393 train_time:4638ms step_avg:113.11ms
step:52/1393 train_time:4750ms step_avg:113.10ms
step:53/1393 train_time:4864ms step_avg:113.11ms
step:54/1393 train_time:4976ms step_avg:113.10ms
step:55/1393 train_time:5091ms step_avg:113.13ms
step:56/1393 train_time:5204ms step_avg:113.12ms
step:57/1393 train_time:5316ms step_avg:113.12ms
step:58/1393 train_time:5430ms step_avg:113.12ms
step:59/1393 train_time:5543ms step_avg:113.12ms
step:60/1393 train_time:5656ms step_avg:113.12ms
step:61/1393 train_time:5768ms step_avg:113.11ms
step:62/1393 train_time:5881ms step_avg:113.10ms
step:63/1393 train_time:5994ms step_avg:113.10ms
step:64/1393 train_time:6108ms step_avg:113.11ms
step:65/1393 train_time:6221ms step_avg:113.11ms
step:66/1393 train_time:6333ms step_avg:113.10ms
step:67/1393 train_time:6447ms step_avg:113.10ms
step:68/1393 train_time:6560ms step_avg:113.09ms
step:69/1393 train_time:6672ms step_avg:113.09ms
step:70/1393 train_time:6785ms step_avg:113.09ms
step:71/1393 train_time:6899ms step_avg:113.10ms
step:72/1393 train_time:7012ms step_avg:113.10ms
step:73/1393 train_time:7125ms step_avg:113.10ms
step:74/1393 train_time:7238ms step_avg:113.09ms
step:75/1393 train_time:7351ms step_avg:113.08ms
step:76/1393 train_time:7464ms step_avg:113.09ms
step:77/1393 train_time:7577ms step_avg:113.08ms
step:78/1393 train_time:7690ms step_avg:113.10ms
step:79/1393 train_time:7803ms step_avg:113.09ms
step:80/1393 train_time:7916ms step_avg:113.09ms
step:81/1393 train_time:8029ms step_avg:113.08ms
step:82/1393 train_time:8142ms step_avg:113.08ms
step:83/1393 train_time:8255ms step_avg:113.08ms
step:84/1393 train_time:8368ms step_avg:113.08ms
step:85/1393 train_time:8482ms step_avg:113.09ms
step:86/1393 train_time:8595ms step_avg:113.09ms
step:87/1393 train_time:8707ms step_avg:113.08ms
step:88/1393 train_time:8821ms step_avg:113.09ms
step:89/1393 train_time:8934ms step_avg:113.09ms
step:90/1393 train_time:9048ms step_avg:113.10ms
step:91/1393 train_time:9161ms step_avg:113.09ms
step:92/1393 train_time:9273ms step_avg:113.09ms
step:93/1393 train_time:9386ms step_avg:113.08ms
step:94/1393 train_time:9499ms step_avg:113.09ms
step:95/1393 train_time:9612ms step_avg:113.08ms
step:96/1393 train_time:9725ms step_avg:113.08ms
step:97/1393 train_time:9839ms step_avg:113.09ms
step:98/1393 train_time:9954ms step_avg:113.11ms
step:99/1393 train_time:10067ms step_avg:113.11ms
step:100/1393 train_time:10180ms step_avg:113.11ms
step:101/1393 train_time:10293ms step_avg:113.11ms
step:102/1393 train_time:10406ms step_avg:113.11ms
step:103/1393 train_time:10519ms step_avg:113.11ms
step:104/1393 train_time:10632ms step_avg:113.11ms
step:105/1393 train_time:10745ms step_avg:113.11ms
step:106/1393 train_time:10859ms step_avg:113.12ms
step:107/1393 train_time:10973ms step_avg:113.12ms
step:108/1393 train_time:11088ms step_avg:113.14ms
step:109/1393 train_time:11201ms step_avg:113.15ms
step:110/1393 train_time:11315ms step_avg:113.15ms
step:111/1393 train_time:11429ms step_avg:113.16ms
step:112/1393 train_time:11543ms step_avg:113.17ms
step:113/1393 train_time:11657ms step_avg:113.17ms
step:114/1393 train_time:11771ms step_avg:113.18ms
step:115/1393 train_time:11885ms step_avg:113.19ms
step:116/1393 train_time:11999ms step_avg:113.20ms
step:117/1393 train_time:12113ms step_avg:113.21ms
step:118/1393 train_time:12228ms step_avg:113.22ms
step:119/1393 train_time:12341ms step_avg:113.22ms
step:120/1393 train_time:12454ms step_avg:113.22ms
step:121/1393 train_time:12569ms step_avg:113.24ms
step:122/1393 train_time:12684ms step_avg:113.25ms
step:123/1393 train_time:12797ms step_avg:113.25ms
step:124/1393 train_time:12911ms step_avg:113.26ms
step:125/1393 train_time:13026ms step_avg:113.27ms
step:125/1393 val_loss:4.3312 train_time:13139ms step_avg:114.25ms
step:126/1393 train_time:13164ms step_avg:113.48ms
step:127/1393 train_time:13258ms step_avg:113.31ms
step:128/1393 train_time:13379ms step_avg:113.38ms
step:129/1393 train_time:13497ms step_avg:113.42ms
step:130/1393 train_time:13610ms step_avg:113.42ms
step:131/1393 train_time:13724ms step_avg:113.42ms
step:132/1393 train_time:13838ms step_avg:113.43ms
step:133/1393 train_time:13952ms step_avg:113.43ms
step:134/1393 train_time:14066ms step_avg:113.44ms
step:135/1393 train_time:14180ms step_avg:113.44ms
step:136/1393 train_time:14295ms step_avg:113.45ms
step:137/1393 train_time:14408ms step_avg:113.45ms
step:138/1393 train_time:14522ms step_avg:113.46ms
step:139/1393 train_time:14637ms step_avg:113.46ms
step:140/1393 train_time:14750ms step_avg:113.46ms
step:141/1393 train_time:14864ms step_avg:113.47ms
step:142/1393 train_time:14978ms step_avg:113.47ms
step:143/1393 train_time:15093ms step_avg:113.48ms
step:144/1393 train_time:15206ms step_avg:113.48ms
step:145/1393 train_time:15320ms step_avg:113.48ms
step:146/1393 train_time:15434ms step_avg:113.48ms
step:147/1393 train_time:15548ms step_avg:113.49ms
step:148/1393 train_time:15661ms step_avg:113.49ms
step:149/1393 train_time:15775ms step_avg:113.49ms
step:150/1393 train_time:15888ms step_avg:113.48ms
step:151/1393 train_time:16002ms step_avg:113.49ms
step:152/1393 train_time:16116ms step_avg:113.49ms
step:153/1393 train_time:16229ms step_avg:113.49ms
step:154/1393 train_time:16343ms step_avg:113.50ms
step:155/1393 train_time:16458ms step_avg:113.50ms
step:156/1393 train_time:16571ms step_avg:113.50ms
step:157/1393 train_time:16685ms step_avg:113.50ms
step:158/1393 train_time:16800ms step_avg:113.51ms
step:159/1393 train_time:16914ms step_avg:113.52ms
step:160/1393 train_time:17027ms step_avg:113.52ms
step:161/1393 train_time:17142ms step_avg:113.52ms
step:162/1393 train_time:17256ms step_avg:113.53ms
step:163/1393 train_time:17370ms step_avg:113.53ms
step:164/1393 train_time:17483ms step_avg:113.53ms
step:165/1393 train_time:17597ms step_avg:113.53ms
step:166/1393 train_time:17711ms step_avg:113.53ms
step:167/1393 train_time:17826ms step_avg:113.54ms
step:168/1393 train_time:17940ms step_avg:113.54ms
step:169/1393 train_time:18053ms step_avg:113.54ms
step:170/1393 train_time:18166ms step_avg:113.54ms
step:171/1393 train_time:18281ms step_avg:113.54ms
step:172/1393 train_time:18394ms step_avg:113.54ms
step:173/1393 train_time:18507ms step_avg:113.54ms
step:174/1393 train_time:18620ms step_avg:113.54ms
step:175/1393 train_time:18734ms step_avg:113.54ms
step:176/1393 train_time:18848ms step_avg:113.54ms
step:177/1393 train_time:18961ms step_avg:113.54ms
step:178/1393 train_time:19075ms step_avg:113.54ms
step:179/1393 train_time:19189ms step_avg:113.55ms
step:180/1393 train_time:19303ms step_avg:113.55ms
step:181/1393 train_time:19417ms step_avg:113.55ms
step:182/1393 train_time:19530ms step_avg:113.55ms
step:183/1393 train_time:19643ms step_avg:113.54ms
step:184/1393 train_time:19757ms step_avg:113.54ms
step:185/1393 train_time:19871ms step_avg:113.55ms
step:186/1393 train_time:19984ms step_avg:113.55ms
step:187/1393 train_time:20098ms step_avg:113.55ms
step:188/1393 train_time:20213ms step_avg:113.55ms
step:189/1393 train_time:20327ms step_avg:113.56ms
step:190/1393 train_time:20442ms step_avg:113.56ms
step:191/1393 train_time:20556ms step_avg:113.57ms
step:192/1393 train_time:20670ms step_avg:113.57ms
step:193/1393 train_time:20783ms step_avg:113.57ms
step:194/1393 train_time:20897ms step_avg:113.57ms
step:195/1393 train_time:21011ms step_avg:113.57ms
step:196/1393 train_time:21125ms step_avg:113.58ms
step:197/1393 train_time:21239ms step_avg:113.58ms
step:198/1393 train_time:21353ms step_avg:113.58ms
step:199/1393 train_time:21467ms step_avg:113.58ms
step:200/1393 train_time:21581ms step_avg:113.59ms
step:201/1393 train_time:21697ms step_avg:113.60ms
step:202/1393 train_time:21810ms step_avg:113.60ms
step:203/1393 train_time:21924ms step_avg:113.60ms
step:204/1393 train_time:22038ms step_avg:113.60ms
step:205/1393 train_time:22152ms step_avg:113.60ms
step:206/1393 train_time:22265ms step_avg:113.60ms
step:207/1393 train_time:22379ms step_avg:113.60ms
step:208/1393 train_time:22493ms step_avg:113.60ms
step:209/1393 train_time:22606ms step_avg:113.60ms
step:210/1393 train_time:22721ms step_avg:113.60ms
step:211/1393 train_time:22836ms step_avg:113.61ms
step:212/1393 train_time:22950ms step_avg:113.61ms
step:213/1393 train_time:23064ms step_avg:113.62ms
step:214/1393 train_time:23178ms step_avg:113.62ms
step:215/1393 train_time:23292ms step_avg:113.62ms
step:216/1393 train_time:23406ms step_avg:113.62ms
step:217/1393 train_time:23521ms step_avg:113.63ms
step:218/1393 train_time:23635ms step_avg:113.63ms
step:219/1393 train_time:23750ms step_avg:113.64ms
step:220/1393 train_time:23864ms step_avg:113.64ms
step:221/1393 train_time:23979ms step_avg:113.64ms
step:222/1393 train_time:24093ms step_avg:113.65ms
step:223/1393 train_time:24208ms step_avg:113.65ms
step:224/1393 train_time:24322ms step_avg:113.65ms
step:225/1393 train_time:24436ms step_avg:113.65ms
step:226/1393 train_time:24550ms step_avg:113.66ms
step:227/1393 train_time:24664ms step_avg:113.66ms
step:228/1393 train_time:24779ms step_avg:113.67ms
step:229/1393 train_time:24894ms step_avg:113.67ms
step:230/1393 train_time:25009ms step_avg:113.68ms
step:231/1393 train_time:25123ms step_avg:113.68ms
step:232/1393 train_time:25237ms step_avg:113.68ms
step:233/1393 train_time:25352ms step_avg:113.68ms
step:234/1393 train_time:25466ms step_avg:113.69ms
step:235/1393 train_time:25581ms step_avg:113.69ms
step:236/1393 train_time:25695ms step_avg:113.70ms
step:237/1393 train_time:25809ms step_avg:113.70ms
step:238/1393 train_time:25924ms step_avg:113.70ms
step:239/1393 train_time:26039ms step_avg:113.71ms
step:240/1393 train_time:26153ms step_avg:113.71ms
step:241/1393 train_time:26267ms step_avg:113.71ms
step:242/1393 train_time:26381ms step_avg:113.71ms
step:243/1393 train_time:26495ms step_avg:113.71ms
step:244/1393 train_time:26609ms step_avg:113.72ms
step:245/1393 train_time:26724ms step_avg:113.72ms
step:246/1393 train_time:26839ms step_avg:113.72ms
step:247/1393 train_time:26953ms step_avg:113.72ms
step:248/1393 train_time:27067ms step_avg:113.73ms
step:249/1393 train_time:27183ms step_avg:113.73ms
step:250/1393 train_time:27297ms step_avg:113.74ms
step:250/1393 val_loss:3.9566 train_time:27410ms step_avg:114.21ms
step:251/1393 train_time:27435ms step_avg:113.84ms
step:252/1393 train_time:27529ms step_avg:113.75ms
step:253/1393 train_time:27651ms step_avg:113.79ms
step:254/1393 train_time:27768ms step_avg:113.80ms
step:255/1393 train_time:27883ms step_avg:113.81ms
step:256/1393 train_time:27997ms step_avg:113.81ms
step:257/1393 train_time:28111ms step_avg:113.81ms
step:258/1393 train_time:28225ms step_avg:113.81ms
step:259/1393 train_time:28339ms step_avg:113.81ms
step:260/1393 train_time:28453ms step_avg:113.81ms
step:261/1393 train_time:28567ms step_avg:113.81ms
step:262/1393 train_time:28681ms step_avg:113.81ms
step:263/1393 train_time:28795ms step_avg:113.81ms
step:264/1393 train_time:28909ms step_avg:113.81ms
step:265/1393 train_time:29024ms step_avg:113.82ms
step:266/1393 train_time:29138ms step_avg:113.82ms
step:267/1393 train_time:29252ms step_avg:113.82ms
step:268/1393 train_time:29366ms step_avg:113.82ms
step:269/1393 train_time:29480ms step_avg:113.82ms
step:270/1393 train_time:29594ms step_avg:113.82ms
step:271/1393 train_time:29708ms step_avg:113.82ms
step:272/1393 train_time:29821ms step_avg:113.82ms
step:273/1393 train_time:29935ms step_avg:113.82ms
step:274/1393 train_time:30049ms step_avg:113.82ms
step:275/1393 train_time:30163ms step_avg:113.82ms
step:276/1393 train_time:30277ms step_avg:113.82ms
step:277/1393 train_time:30391ms step_avg:113.82ms
step:278/1393 train_time:30506ms step_avg:113.83ms
step:279/1393 train_time:30621ms step_avg:113.83ms
step:280/1393 train_time:30735ms step_avg:113.83ms
step:281/1393 train_time:30850ms step_avg:113.84ms
step:282/1393 train_time:30965ms step_avg:113.84ms
step:283/1393 train_time:31079ms step_avg:113.84ms
step:284/1393 train_time:31193ms step_avg:113.84ms
step:285/1393 train_time:31307ms step_avg:113.84ms
step:286/1393 train_time:31422ms step_avg:113.85ms
step:287/1393 train_time:31536ms step_avg:113.85ms
step:288/1393 train_time:31650ms step_avg:113.85ms
step:289/1393 train_time:31764ms step_avg:113.85ms
step:290/1393 train_time:31878ms step_avg:113.85ms
step:291/1393 train_time:31992ms step_avg:113.85ms
step:292/1393 train_time:32106ms step_avg:113.85ms
step:293/1393 train_time:32221ms step_avg:113.86ms
step:294/1393 train_time:32335ms step_avg:113.85ms
step:295/1393 train_time:32449ms step_avg:113.85ms
step:296/1393 train_time:32564ms step_avg:113.86ms
step:297/1393 train_time:32679ms step_avg:113.86ms
step:298/1393 train_time:32793ms step_avg:113.86ms
step:299/1393 train_time:32907ms step_avg:113.86ms
step:300/1393 train_time:33022ms step_avg:113.87ms
step:301/1393 train_time:33136ms step_avg:113.87ms
step:302/1393 train_time:33251ms step_avg:113.87ms
step:303/1393 train_time:33366ms step_avg:113.88ms
step:304/1393 train_time:33481ms step_avg:113.88ms
step:305/1393 train_time:33595ms step_avg:113.88ms
step:306/1393 train_time:33709ms step_avg:113.88ms
step:307/1393 train_time:33824ms step_avg:113.88ms
step:308/1393 train_time:33938ms step_avg:113.89ms
step:309/1393 train_time:34053ms step_avg:113.89ms
step:310/1393 train_time:34167ms step_avg:113.89ms
step:311/1393 train_time:34281ms step_avg:113.89ms
step:312/1393 train_time:34398ms step_avg:113.90ms
step:313/1393 train_time:34514ms step_avg:113.91ms
step:314/1393 train_time:34631ms step_avg:113.92ms
step:315/1393 train_time:34748ms step_avg:113.93ms
step:316/1393 train_time:34865ms step_avg:113.94ms
step:317/1393 train_time:34982ms step_avg:113.95ms
step:318/1393 train_time:35099ms step_avg:113.96ms
step:319/1393 train_time:35215ms step_avg:113.96ms
step:320/1393 train_time:35331ms step_avg:113.97ms
step:321/1393 train_time:35449ms step_avg:113.98ms
step:322/1393 train_time:35566ms step_avg:113.99ms
step:323/1393 train_time:35683ms step_avg:114.00ms
step:324/1393 train_time:35800ms step_avg:114.01ms
step:325/1393 train_time:35916ms step_avg:114.02ms
step:326/1393 train_time:36033ms step_avg:114.03ms
step:327/1393 train_time:36151ms step_avg:114.04ms
step:328/1393 train_time:36269ms step_avg:114.05ms
step:329/1393 train_time:36387ms step_avg:114.06ms
step:330/1393 train_time:36504ms step_avg:114.07ms
step:331/1393 train_time:36620ms step_avg:114.08ms
step:332/1393 train_time:36737ms step_avg:114.09ms
step:333/1393 train_time:36853ms step_avg:114.10ms
step:334/1393 train_time:36971ms step_avg:114.11ms
step:335/1393 train_time:37087ms step_avg:114.12ms
step:336/1393 train_time:37203ms step_avg:114.12ms
step:337/1393 train_time:37320ms step_avg:114.13ms
step:338/1393 train_time:37436ms step_avg:114.14ms
step:339/1393 train_time:37553ms step_avg:114.14ms
step:340/1393 train_time:37671ms step_avg:114.15ms
step:341/1393 train_time:37788ms step_avg:114.16ms
step:342/1393 train_time:37905ms step_avg:114.17ms
step:343/1393 train_time:38021ms step_avg:114.18ms
step:344/1393 train_time:38138ms step_avg:114.19ms
step:345/1393 train_time:38255ms step_avg:114.19ms
step:346/1393 train_time:38371ms step_avg:114.20ms
step:347/1393 train_time:38487ms step_avg:114.21ms
step:348/1393 train_time:38604ms step_avg:114.21ms
step:349/1393 train_time:38721ms step_avg:114.22ms
step:350/1393 train_time:38837ms step_avg:114.23ms
step:351/1393 train_time:38954ms step_avg:114.24ms
step:352/1393 train_time:39071ms step_avg:114.24ms
step:353/1393 train_time:39188ms step_avg:114.25ms
step:354/1393 train_time:39305ms step_avg:114.26ms
step:355/1393 train_time:39422ms step_avg:114.27ms
step:356/1393 train_time:39539ms step_avg:114.27ms
step:357/1393 train_time:39655ms step_avg:114.28ms
step:358/1393 train_time:39772ms step_avg:114.29ms
step:359/1393 train_time:39889ms step_avg:114.30ms
step:360/1393 train_time:40007ms step_avg:114.30ms
step:361/1393 train_time:40124ms step_avg:114.31ms
step:362/1393 train_time:40241ms step_avg:114.32ms
step:363/1393 train_time:40358ms step_avg:114.33ms
step:364/1393 train_time:40475ms step_avg:114.33ms
step:365/1393 train_time:40591ms step_avg:114.34ms
step:366/1393 train_time:40709ms step_avg:114.35ms
step:367/1393 train_time:40825ms step_avg:114.36ms
step:368/1393 train_time:40942ms step_avg:114.36ms
step:369/1393 train_time:41059ms step_avg:114.37ms
step:370/1393 train_time:41176ms step_avg:114.38ms
step:371/1393 train_time:41293ms step_avg:114.38ms
step:372/1393 train_time:41410ms step_avg:114.39ms
step:373/1393 train_time:41527ms step_avg:114.40ms
step:374/1393 train_time:41645ms step_avg:114.41ms
step:375/1393 train_time:41762ms step_avg:114.42ms
step:375/1393 val_loss:3.7723 train_time:41877ms step_avg:114.73ms
step:376/1393 train_time:41901ms step_avg:114.48ms
step:377/1393 train_time:41998ms step_avg:114.44ms
step:378/1393 train_time:42124ms step_avg:114.47ms
step:379/1393 train_time:42244ms step_avg:114.48ms
step:380/1393 train_time:42361ms step_avg:114.49ms
step:381/1393 train_time:42477ms step_avg:114.49ms
step:382/1393 train_time:42594ms step_avg:114.50ms
step:383/1393 train_time:42710ms step_avg:114.50ms
step:384/1393 train_time:42827ms step_avg:114.51ms
step:385/1393 train_time:42944ms step_avg:114.52ms
step:386/1393 train_time:43061ms step_avg:114.52ms
step:387/1393 train_time:43178ms step_avg:114.53ms
step:388/1393 train_time:43295ms step_avg:114.54ms
step:389/1393 train_time:43411ms step_avg:114.54ms
step:390/1393 train_time:43528ms step_avg:114.55ms
step:391/1393 train_time:43646ms step_avg:114.56ms
step:392/1393 train_time:43763ms step_avg:114.56ms
step:393/1393 train_time:43880ms step_avg:114.57ms
step:394/1393 train_time:43998ms step_avg:114.58ms
step:395/1393 train_time:44114ms step_avg:114.58ms
step:396/1393 train_time:44231ms step_avg:114.59ms
step:397/1393 train_time:44348ms step_avg:114.59ms
step:398/1393 train_time:44465ms step_avg:114.60ms
step:399/1393 train_time:44582ms step_avg:114.61ms
step:400/1393 train_time:44698ms step_avg:114.61ms
step:401/1393 train_time:44816ms step_avg:114.62ms
step:402/1393 train_time:44932ms step_avg:114.62ms
step:403/1393 train_time:45049ms step_avg:114.63ms
step:404/1393 train_time:45167ms step_avg:114.64ms
step:405/1393 train_time:45284ms step_avg:114.64ms
step:406/1393 train_time:45402ms step_avg:114.65ms
step:407/1393 train_time:45519ms step_avg:114.66ms
step:408/1393 train_time:45635ms step_avg:114.66ms
step:409/1393 train_time:45752ms step_avg:114.67ms
step:410/1393 train_time:45869ms step_avg:114.67ms
step:411/1393 train_time:45986ms step_avg:114.68ms
step:412/1393 train_time:46103ms step_avg:114.68ms
step:413/1393 train_time:46220ms step_avg:114.69ms
step:414/1393 train_time:46337ms step_avg:114.70ms
step:415/1393 train_time:46454ms step_avg:114.70ms
step:416/1393 train_time:46571ms step_avg:114.71ms
step:417/1393 train_time:46688ms step_avg:114.71ms
step:418/1393 train_time:46805ms step_avg:114.72ms
step:419/1393 train_time:46923ms step_avg:114.73ms
step:420/1393 train_time:47040ms step_avg:114.73ms
step:421/1393 train_time:47157ms step_avg:114.74ms
step:422/1393 train_time:47274ms step_avg:114.74ms
step:423/1393 train_time:47392ms step_avg:114.75ms
step:424/1393 train_time:47509ms step_avg:114.76ms
step:425/1393 train_time:47626ms step_avg:114.76ms
step:426/1393 train_time:47743ms step_avg:114.77ms
step:427/1393 train_time:47860ms step_avg:114.77ms
step:428/1393 train_time:47977ms step_avg:114.78ms
step:429/1393 train_time:48095ms step_avg:114.79ms
step:430/1393 train_time:48212ms step_avg:114.79ms
step:431/1393 train_time:48329ms step_avg:114.80ms
step:432/1393 train_time:48446ms step_avg:114.80ms
step:433/1393 train_time:48563ms step_avg:114.81ms
step:434/1393 train_time:48682ms step_avg:114.82ms
step:435/1393 train_time:48799ms step_avg:114.82ms
step:436/1393 train_time:48916ms step_avg:114.83ms
step:437/1393 train_time:49034ms step_avg:114.83ms
step:438/1393 train_time:49151ms step_avg:114.84ms
step:439/1393 train_time:49268ms step_avg:114.84ms
step:440/1393 train_time:49385ms step_avg:114.85ms
step:441/1393 train_time:49502ms step_avg:114.85ms
step:442/1393 train_time:49619ms step_avg:114.86ms
step:443/1393 train_time:49737ms step_avg:114.87ms
step:444/1393 train_time:49855ms step_avg:114.87ms
step:445/1393 train_time:49973ms step_avg:114.88ms
step:446/1393 train_time:50090ms step_avg:114.89ms
step:447/1393 train_time:50208ms step_avg:114.89ms
step:448/1393 train_time:50325ms step_avg:114.90ms
step:449/1393 train_time:50443ms step_avg:114.90ms
step:450/1393 train_time:50560ms step_avg:114.91ms
step:451/1393 train_time:50678ms step_avg:114.92ms
step:452/1393 train_time:50794ms step_avg:114.92ms
step:453/1393 train_time:50911ms step_avg:114.92ms
step:454/1393 train_time:51028ms step_avg:114.93ms
step:455/1393 train_time:51145ms step_avg:114.93ms
step:456/1393 train_time:51262ms step_avg:114.94ms
step:457/1393 train_time:51379ms step_avg:114.94ms
step:458/1393 train_time:51496ms step_avg:114.95ms
step:459/1393 train_time:51613ms step_avg:114.95ms
step:460/1393 train_time:51731ms step_avg:114.96ms
step:461/1393 train_time:51848ms step_avg:114.96ms
step:462/1393 train_time:51965ms step_avg:114.97ms
step:463/1393 train_time:52084ms step_avg:114.97ms
step:464/1393 train_time:52201ms step_avg:114.98ms
step:465/1393 train_time:52319ms step_avg:114.99ms
step:466/1393 train_time:52437ms step_avg:114.99ms
step:467/1393 train_time:52554ms step_avg:115.00ms
step:468/1393 train_time:52672ms step_avg:115.00ms
step:469/1393 train_time:52789ms step_avg:115.01ms
step:470/1393 train_time:52906ms step_avg:115.01ms
step:471/1393 train_time:53024ms step_avg:115.02ms
step:472/1393 train_time:53142ms step_avg:115.03ms
step:473/1393 train_time:53259ms step_avg:115.03ms
step:474/1393 train_time:53376ms step_avg:115.03ms
step:475/1393 train_time:53494ms step_avg:115.04ms
step:476/1393 train_time:53611ms step_avg:115.04ms
step:477/1393 train_time:53727ms step_avg:115.05ms
step:478/1393 train_time:53844ms step_avg:115.05ms
step:479/1393 train_time:53961ms step_avg:115.06ms
step:480/1393 train_time:54078ms step_avg:115.06ms
step:481/1393 train_time:54195ms step_avg:115.06ms
step:482/1393 train_time:54313ms step_avg:115.07ms
step:483/1393 train_time:54430ms step_avg:115.07ms
step:484/1393 train_time:54547ms step_avg:115.08ms
step:485/1393 train_time:54665ms step_avg:115.08ms
step:486/1393 train_time:54783ms step_avg:115.09ms
step:487/1393 train_time:54901ms step_avg:115.10ms
step:488/1393 train_time:55019ms step_avg:115.10ms
step:489/1393 train_time:55136ms step_avg:115.11ms
step:490/1393 train_time:55254ms step_avg:115.11ms
step:491/1393 train_time:55371ms step_avg:115.12ms
step:492/1393 train_time:55488ms step_avg:115.12ms
step:493/1393 train_time:55605ms step_avg:115.12ms
step:494/1393 train_time:55723ms step_avg:115.13ms
step:495/1393 train_time:55841ms step_avg:115.14ms
step:496/1393 train_time:55959ms step_avg:115.14ms
step:497/1393 train_time:56077ms step_avg:115.15ms
step:498/1393 train_time:56194ms step_avg:115.15ms
step:499/1393 train_time:56311ms step_avg:115.16ms
step:500/1393 train_time:56428ms step_avg:115.16ms
step:500/1393 val_loss:3.6588 train_time:56544ms step_avg:115.39ms
step:501/1393 train_time:56568ms step_avg:115.21ms
step:502/1393 train_time:56664ms step_avg:115.17ms
step:503/1393 train_time:56792ms step_avg:115.20ms
step:504/1393 train_time:56913ms step_avg:115.21ms
step:505/1393 train_time:57030ms step_avg:115.21ms
step:506/1393 train_time:57147ms step_avg:115.21ms
step:507/1393 train_time:57264ms step_avg:115.22ms
step:508/1393 train_time:57380ms step_avg:115.22ms
step:509/1393 train_time:57498ms step_avg:115.23ms
step:510/1393 train_time:57615ms step_avg:115.23ms
step:511/1393 train_time:57733ms step_avg:115.24ms
step:512/1393 train_time:57850ms step_avg:115.24ms
step:513/1393 train_time:57967ms step_avg:115.24ms
step:514/1393 train_time:58084ms step_avg:115.25ms
step:515/1393 train_time:58202ms step_avg:115.25ms
step:516/1393 train_time:58319ms step_avg:115.26ms
step:517/1393 train_time:58436ms step_avg:115.26ms
step:518/1393 train_time:58558ms step_avg:115.27ms
step:519/1393 train_time:58677ms step_avg:115.28ms
step:520/1393 train_time:58796ms step_avg:115.29ms
step:521/1393 train_time:58915ms step_avg:115.29ms
step:522/1393 train_time:59035ms step_avg:115.30ms
step:523/1393 train_time:59158ms step_avg:115.32ms
step:524/1393 train_time:59279ms step_avg:115.33ms
step:525/1393 train_time:59398ms step_avg:115.34ms
step:526/1393 train_time:59517ms step_avg:115.34ms
step:527/1393 train_time:59636ms step_avg:115.35ms
step:528/1393 train_time:59756ms step_avg:115.36ms
step:529/1393 train_time:59876ms step_avg:115.37ms
step:530/1393 train_time:59995ms step_avg:115.38ms
step:531/1393 train_time:60114ms step_avg:115.38ms
step:532/1393 train_time:60234ms step_avg:115.39ms
step:533/1393 train_time:60353ms step_avg:115.40ms
step:534/1393 train_time:60473ms step_avg:115.41ms
step:535/1393 train_time:60593ms step_avg:115.42ms
step:536/1393 train_time:60712ms step_avg:115.42ms
step:537/1393 train_time:60832ms step_avg:115.43ms
step:538/1393 train_time:60951ms step_avg:115.44ms
step:539/1393 train_time:61071ms step_avg:115.45ms
step:540/1393 train_time:61190ms step_avg:115.45ms
step:541/1393 train_time:61311ms step_avg:115.46ms
step:542/1393 train_time:61431ms step_avg:115.47ms
step:543/1393 train_time:61551ms step_avg:115.48ms
step:544/1393 train_time:61671ms step_avg:115.49ms
step:545/1393 train_time:61790ms step_avg:115.50ms
step:546/1393 train_time:61911ms step_avg:115.50ms
step:547/1393 train_time:62030ms step_avg:115.51ms
step:548/1393 train_time:62151ms step_avg:115.52ms
step:549/1393 train_time:62271ms step_avg:115.53ms
step:550/1393 train_time:62391ms step_avg:115.54ms
step:551/1393 train_time:62510ms step_avg:115.55ms
step:552/1393 train_time:62630ms step_avg:115.55ms
step:553/1393 train_time:62751ms step_avg:115.56ms
step:554/1393 train_time:62870ms step_avg:115.57ms
step:555/1393 train_time:62990ms step_avg:115.58ms
step:556/1393 train_time:63110ms step_avg:115.59ms
step:557/1393 train_time:63230ms step_avg:115.59ms
step:558/1393 train_time:63349ms step_avg:115.60ms
step:559/1393 train_time:63468ms step_avg:115.61ms
step:560/1393 train_time:63587ms step_avg:115.61ms
step:561/1393 train_time:63706ms step_avg:115.62ms
step:562/1393 train_time:63825ms step_avg:115.62ms
step:563/1393 train_time:63944ms step_avg:115.63ms
step:564/1393 train_time:64063ms step_avg:115.64ms
step:565/1393 train_time:64183ms step_avg:115.64ms
step:566/1393 train_time:64302ms step_avg:115.65ms
step:567/1393 train_time:64421ms step_avg:115.66ms
step:568/1393 train_time:64539ms step_avg:115.66ms
step:569/1393 train_time:64659ms step_avg:115.67ms
step:570/1393 train_time:64779ms step_avg:115.68ms
step:571/1393 train_time:64899ms step_avg:115.68ms
step:572/1393 train_time:65017ms step_avg:115.69ms
step:573/1393 train_time:65137ms step_avg:115.70ms
step:574/1393 train_time:65258ms step_avg:115.71ms
step:575/1393 train_time:65379ms step_avg:115.72ms
step:576/1393 train_time:65498ms step_avg:115.72ms
step:577/1393 train_time:65618ms step_avg:115.73ms
step:578/1393 train_time:65738ms step_avg:115.74ms
step:579/1393 train_time:65857ms step_avg:115.74ms
step:580/1393 train_time:65976ms step_avg:115.75ms
step:581/1393 train_time:66096ms step_avg:115.75ms
step:582/1393 train_time:66214ms step_avg:115.76ms
step:583/1393 train_time:66335ms step_avg:115.77ms
step:584/1393 train_time:66456ms step_avg:115.78ms
step:585/1393 train_time:66576ms step_avg:115.78ms
step:586/1393 train_time:66695ms step_avg:115.79ms
step:587/1393 train_time:66814ms step_avg:115.80ms
step:588/1393 train_time:66934ms step_avg:115.80ms
step:589/1393 train_time:67054ms step_avg:115.81ms
step:590/1393 train_time:67174ms step_avg:115.82ms
step:591/1393 train_time:67295ms step_avg:115.83ms
step:592/1393 train_time:67414ms step_avg:115.83ms
step:593/1393 train_time:67536ms step_avg:115.84ms
step:594/1393 train_time:67655ms step_avg:115.85ms
step:595/1393 train_time:67777ms step_avg:115.86ms
step:596/1393 train_time:67896ms step_avg:115.86ms
step:597/1393 train_time:68016ms step_avg:115.87ms
step:598/1393 train_time:68136ms step_avg:115.88ms
step:599/1393 train_time:68256ms step_avg:115.88ms
step:600/1393 train_time:68375ms step_avg:115.89ms
step:601/1393 train_time:68494ms step_avg:115.90ms
step:602/1393 train_time:68613ms step_avg:115.90ms
step:603/1393 train_time:68733ms step_avg:115.91ms
step:604/1393 train_time:68853ms step_avg:115.91ms
step:605/1393 train_time:68973ms step_avg:115.92ms
step:606/1393 train_time:69092ms step_avg:115.93ms
step:607/1393 train_time:69212ms step_avg:115.93ms
step:608/1393 train_time:69331ms step_avg:115.94ms
step:609/1393 train_time:69451ms step_avg:115.94ms
step:610/1393 train_time:69572ms step_avg:115.95ms
step:611/1393 train_time:69692ms step_avg:115.96ms
step:612/1393 train_time:69813ms step_avg:115.97ms
step:613/1393 train_time:69933ms step_avg:115.98ms
step:614/1393 train_time:70052ms step_avg:115.98ms
step:615/1393 train_time:70172ms step_avg:115.99ms
step:616/1393 train_time:70292ms step_avg:115.99ms
step:617/1393 train_time:70412ms step_avg:116.00ms
step:618/1393 train_time:70531ms step_avg:116.01ms
step:619/1393 train_time:70651ms step_avg:116.01ms
step:620/1393 train_time:70772ms step_avg:116.02ms
step:621/1393 train_time:70892ms step_avg:116.03ms
step:622/1393 train_time:71013ms step_avg:116.03ms
step:623/1393 train_time:71132ms step_avg:116.04ms
step:624/1393 train_time:71253ms step_avg:116.05ms
step:625/1393 train_time:71372ms step_avg:116.05ms
step:625/1393 val_loss:3.5773 train_time:71490ms step_avg:116.24ms
step:626/1393 train_time:71515ms step_avg:116.10ms
step:627/1393 train_time:71613ms step_avg:116.07ms
step:628/1393 train_time:71739ms step_avg:116.08ms
step:629/1393 train_time:71861ms step_avg:116.09ms
step:630/1393 train_time:71981ms step_avg:116.10ms
step:631/1393 train_time:72100ms step_avg:116.10ms
step:632/1393 train_time:72219ms step_avg:116.11ms
step:633/1393 train_time:72339ms step_avg:116.11ms
step:634/1393 train_time:72459ms step_avg:116.12ms
step:635/1393 train_time:72579ms step_avg:116.13ms
step:636/1393 train_time:72699ms step_avg:116.13ms
step:637/1393 train_time:72819ms step_avg:116.14ms
step:638/1393 train_time:72938ms step_avg:116.14ms
step:639/1393 train_time:73058ms step_avg:116.15ms
step:640/1393 train_time:73178ms step_avg:116.16ms
step:641/1393 train_time:73299ms step_avg:116.16ms
step:642/1393 train_time:73418ms step_avg:116.17ms
step:643/1393 train_time:73537ms step_avg:116.17ms
step:644/1393 train_time:73657ms step_avg:116.18ms
step:645/1393 train_time:73777ms step_avg:116.18ms
step:646/1393 train_time:73897ms step_avg:116.19ms
step:647/1393 train_time:74017ms step_avg:116.20ms
step:648/1393 train_time:74137ms step_avg:116.20ms
step:649/1393 train_time:74257ms step_avg:116.21ms
step:650/1393 train_time:74376ms step_avg:116.21ms
step:651/1393 train_time:74496ms step_avg:116.22ms
step:652/1393 train_time:74615ms step_avg:116.22ms
step:653/1393 train_time:74734ms step_avg:116.23ms
step:654/1393 train_time:74854ms step_avg:116.23ms
step:655/1393 train_time:74974ms step_avg:116.24ms
step:656/1393 train_time:75094ms step_avg:116.24ms
step:657/1393 train_time:75214ms step_avg:116.25ms
step:658/1393 train_time:75334ms step_avg:116.26ms
step:659/1393 train_time:75453ms step_avg:116.26ms
step:660/1393 train_time:75573ms step_avg:116.27ms
step:661/1393 train_time:75693ms step_avg:116.27ms
step:662/1393 train_time:75812ms step_avg:116.28ms
step:663/1393 train_time:75932ms step_avg:116.28ms
step:664/1393 train_time:76052ms step_avg:116.29ms
step:665/1393 train_time:76172ms step_avg:116.29ms
step:666/1393 train_time:76292ms step_avg:116.30ms
step:667/1393 train_time:76412ms step_avg:116.30ms
step:668/1393 train_time:76533ms step_avg:116.31ms
step:669/1393 train_time:76653ms step_avg:116.32ms
step:670/1393 train_time:76774ms step_avg:116.32ms
step:671/1393 train_time:76894ms step_avg:116.33ms
step:672/1393 train_time:77013ms step_avg:116.33ms
step:673/1393 train_time:77132ms step_avg:116.34ms
step:674/1393 train_time:77252ms step_avg:116.34ms
step:675/1393 train_time:77372ms step_avg:116.35ms
step:676/1393 train_time:77492ms step_avg:116.35ms
step:677/1393 train_time:77613ms step_avg:116.36ms
step:678/1393 train_time:77732ms step_avg:116.37ms
step:679/1393 train_time:77852ms step_avg:116.37ms
step:680/1393 train_time:77972ms step_avg:116.38ms
step:681/1393 train_time:78092ms step_avg:116.38ms
step:682/1393 train_time:78212ms step_avg:116.39ms
step:683/1393 train_time:78331ms step_avg:116.39ms
step:684/1393 train_time:78450ms step_avg:116.40ms
step:685/1393 train_time:78570ms step_avg:116.40ms
step:686/1393 train_time:78689ms step_avg:116.40ms
step:687/1393 train_time:78811ms step_avg:116.41ms
step:688/1393 train_time:78930ms step_avg:116.42ms
step:689/1393 train_time:79052ms step_avg:116.42ms
step:690/1393 train_time:79172ms step_avg:116.43ms
step:691/1393 train_time:79291ms step_avg:116.43ms
step:692/1393 train_time:79411ms step_avg:116.44ms
step:693/1393 train_time:79530ms step_avg:116.44ms
step:694/1393 train_time:79650ms step_avg:116.45ms
step:695/1393 train_time:79770ms step_avg:116.45ms
step:696/1393 train_time:79890ms step_avg:116.46ms
step:697/1393 train_time:80010ms step_avg:116.46ms
step:698/1393 train_time:80130ms step_avg:116.47ms
step:699/1393 train_time:80250ms step_avg:116.47ms
step:700/1393 train_time:80369ms step_avg:116.48ms
step:701/1393 train_time:80490ms step_avg:116.48ms
step:702/1393 train_time:80609ms step_avg:116.49ms
step:703/1393 train_time:80728ms step_avg:116.49ms
step:704/1393 train_time:80848ms step_avg:116.50ms
step:705/1393 train_time:80969ms step_avg:116.50ms
step:706/1393 train_time:81089ms step_avg:116.51ms
step:707/1393 train_time:81210ms step_avg:116.51ms
step:708/1393 train_time:81330ms step_avg:116.52ms
step:709/1393 train_time:81451ms step_avg:116.53ms
step:710/1393 train_time:81572ms step_avg:116.53ms
step:711/1393 train_time:81692ms step_avg:116.54ms
step:712/1393 train_time:81813ms step_avg:116.54ms
step:713/1393 train_time:81932ms step_avg:116.55ms
step:714/1393 train_time:82051ms step_avg:116.55ms
step:715/1393 train_time:82172ms step_avg:116.56ms
step:716/1393 train_time:82292ms step_avg:116.56ms
step:717/1393 train_time:82413ms step_avg:116.57ms
step:718/1393 train_time:82532ms step_avg:116.57ms
step:719/1393 train_time:82652ms step_avg:116.58ms
step:720/1393 train_time:82773ms step_avg:116.58ms
step:721/1393 train_time:82893ms step_avg:116.59ms
step:722/1393 train_time:83012ms step_avg:116.59ms
step:723/1393 train_time:83131ms step_avg:116.59ms
step:724/1393 train_time:83252ms step_avg:116.60ms
step:725/1393 train_time:83373ms step_avg:116.61ms
step:726/1393 train_time:83495ms step_avg:116.61ms
step:727/1393 train_time:83617ms step_avg:116.62ms
step:728/1393 train_time:83737ms step_avg:116.63ms
step:729/1393 train_time:83862ms step_avg:116.64ms
step:730/1393 train_time:83984ms step_avg:116.65ms
step:731/1393 train_time:84106ms step_avg:116.65ms
step:732/1393 train_time:84228ms step_avg:116.66ms
step:733/1393 train_time:84350ms step_avg:116.67ms
step:734/1393 train_time:84472ms step_avg:116.67ms
step:735/1393 train_time:84593ms step_avg:116.68ms
step:736/1393 train_time:84714ms step_avg:116.69ms
step:737/1393 train_time:84835ms step_avg:116.69ms
step:738/1393 train_time:84957ms step_avg:116.70ms
step:739/1393 train_time:85078ms step_avg:116.70ms
step:740/1393 train_time:85201ms step_avg:116.71ms
step:741/1393 train_time:85323ms step_avg:116.72ms
step:742/1393 train_time:85444ms step_avg:116.73ms
step:743/1393 train_time:85565ms step_avg:116.73ms
step:744/1393 train_time:85687ms step_avg:116.74ms
step:745/1393 train_time:85808ms step_avg:116.75ms
step:746/1393 train_time:85931ms step_avg:116.75ms
step:747/1393 train_time:86053ms step_avg:116.76ms
step:748/1393 train_time:86174ms step_avg:116.77ms
step:749/1393 train_time:86294ms step_avg:116.77ms
step:750/1393 train_time:86416ms step_avg:116.78ms
step:750/1393 val_loss:3.5237 train_time:86535ms step_avg:116.94ms
step:751/1393 train_time:86560ms step_avg:116.82ms
step:752/1393 train_time:86662ms step_avg:116.80ms
step:753/1393 train_time:86790ms step_avg:116.81ms
step:754/1393 train_time:86914ms step_avg:116.82ms
step:755/1393 train_time:87036ms step_avg:116.83ms
step:756/1393 train_time:87158ms step_avg:116.83ms
step:757/1393 train_time:87280ms step_avg:116.84ms
step:758/1393 train_time:87401ms step_avg:116.85ms
step:759/1393 train_time:87522ms step_avg:116.85ms
step:760/1393 train_time:87644ms step_avg:116.86ms
step:761/1393 train_time:87767ms step_avg:116.87ms
step:762/1393 train_time:87889ms step_avg:116.87ms
step:763/1393 train_time:88010ms step_avg:116.88ms
step:764/1393 train_time:88131ms step_avg:116.89ms
step:765/1393 train_time:88254ms step_avg:116.89ms
step:766/1393 train_time:88376ms step_avg:116.90ms
step:767/1393 train_time:88498ms step_avg:116.91ms
step:768/1393 train_time:88619ms step_avg:116.91ms
step:769/1393 train_time:88740ms step_avg:116.92ms
step:770/1393 train_time:88861ms step_avg:116.92ms
step:771/1393 train_time:88982ms step_avg:116.93ms
step:772/1393 train_time:89104ms step_avg:116.93ms
step:773/1393 train_time:89226ms step_avg:116.94ms
step:774/1393 train_time:89348ms step_avg:116.95ms
step:775/1393 train_time:89469ms step_avg:116.95ms
step:776/1393 train_time:89592ms step_avg:116.96ms
step:777/1393 train_time:89714ms step_avg:116.97ms
step:778/1393 train_time:89837ms step_avg:116.97ms
step:779/1393 train_time:89958ms step_avg:116.98ms
step:780/1393 train_time:90080ms step_avg:116.99ms
step:781/1393 train_time:90201ms step_avg:116.99ms
step:782/1393 train_time:90323ms step_avg:117.00ms
step:783/1393 train_time:90445ms step_avg:117.01ms
step:784/1393 train_time:90568ms step_avg:117.01ms
step:785/1393 train_time:90689ms step_avg:117.02ms
step:786/1393 train_time:90811ms step_avg:117.02ms
step:787/1393 train_time:90933ms step_avg:117.03ms
step:788/1393 train_time:91055ms step_avg:117.04ms
step:789/1393 train_time:91177ms step_avg:117.04ms
step:790/1393 train_time:91298ms step_avg:117.05ms
step:791/1393 train_time:91419ms step_avg:117.05ms
step:792/1393 train_time:91541ms step_avg:117.06ms
step:793/1393 train_time:91663ms step_avg:117.07ms
step:794/1393 train_time:91785ms step_avg:117.07ms
step:795/1393 train_time:91906ms step_avg:117.08ms
step:796/1393 train_time:92028ms step_avg:117.08ms
step:797/1393 train_time:92149ms step_avg:117.09ms
step:798/1393 train_time:92270ms step_avg:117.09ms
step:799/1393 train_time:92393ms step_avg:117.10ms
step:800/1393 train_time:92515ms step_avg:117.11ms
step:801/1393 train_time:92637ms step_avg:117.11ms
step:802/1393 train_time:92758ms step_avg:117.12ms
step:803/1393 train_time:92879ms step_avg:117.12ms
step:804/1393 train_time:93000ms step_avg:117.13ms
step:805/1393 train_time:93120ms step_avg:117.13ms
step:806/1393 train_time:93243ms step_avg:117.14ms
step:807/1393 train_time:93364ms step_avg:117.14ms
step:808/1393 train_time:93486ms step_avg:117.15ms
step:809/1393 train_time:93607ms step_avg:117.16ms
step:810/1393 train_time:93728ms step_avg:117.16ms
step:811/1393 train_time:93851ms step_avg:117.17ms
step:812/1393 train_time:93973ms step_avg:117.17ms
step:813/1393 train_time:94095ms step_avg:117.18ms
step:814/1393 train_time:94217ms step_avg:117.18ms
step:815/1393 train_time:94338ms step_avg:117.19ms
step:816/1393 train_time:94459ms step_avg:117.20ms
step:817/1393 train_time:94581ms step_avg:117.20ms
step:818/1393 train_time:94703ms step_avg:117.21ms
step:819/1393 train_time:94824ms step_avg:117.21ms
step:820/1393 train_time:94947ms step_avg:117.22ms
step:821/1393 train_time:95069ms step_avg:117.22ms
step:822/1393 train_time:95191ms step_avg:117.23ms
step:823/1393 train_time:95313ms step_avg:117.24ms
step:824/1393 train_time:95435ms step_avg:117.24ms
step:825/1393 train_time:95558ms step_avg:117.25ms
step:826/1393 train_time:95679ms step_avg:117.25ms
step:827/1393 train_time:95800ms step_avg:117.26ms
step:828/1393 train_time:95922ms step_avg:117.26ms
step:829/1393 train_time:96043ms step_avg:117.27ms
step:830/1393 train_time:96165ms step_avg:117.27ms
step:831/1393 train_time:96287ms step_avg:117.28ms
step:832/1393 train_time:96409ms step_avg:117.29ms
step:833/1393 train_time:96531ms step_avg:117.29ms
step:834/1393 train_time:96653ms step_avg:117.30ms
step:835/1393 train_time:96775ms step_avg:117.30ms
step:836/1393 train_time:96896ms step_avg:117.31ms
step:837/1393 train_time:97017ms step_avg:117.31ms
step:838/1393 train_time:97139ms step_avg:117.32ms
step:839/1393 train_time:97262ms step_avg:117.32ms
step:840/1393 train_time:97384ms step_avg:117.33ms
step:841/1393 train_time:97506ms step_avg:117.34ms
step:842/1393 train_time:97628ms step_avg:117.34ms
step:843/1393 train_time:97749ms step_avg:117.35ms
step:844/1393 train_time:97871ms step_avg:117.35ms
step:845/1393 train_time:97993ms step_avg:117.36ms
step:846/1393 train_time:98115ms step_avg:117.36ms
step:847/1393 train_time:98237ms step_avg:117.37ms
step:848/1393 train_time:98358ms step_avg:117.37ms
step:849/1393 train_time:98480ms step_avg:117.38ms
step:850/1393 train_time:98602ms step_avg:117.38ms
step:851/1393 train_time:98723ms step_avg:117.39ms
step:852/1393 train_time:98845ms step_avg:117.39ms
step:853/1393 train_time:98966ms step_avg:117.40ms
step:854/1393 train_time:99089ms step_avg:117.40ms
step:855/1393 train_time:99211ms step_avg:117.41ms
step:856/1393 train_time:99333ms step_avg:117.41ms
step:857/1393 train_time:99456ms step_avg:117.42ms
step:858/1393 train_time:99578ms step_avg:117.43ms
step:859/1393 train_time:99700ms step_avg:117.43ms
step:860/1393 train_time:99822ms step_avg:117.44ms
step:861/1393 train_time:99944ms step_avg:117.44ms
step:862/1393 train_time:100067ms step_avg:117.45ms
step:863/1393 train_time:100188ms step_avg:117.45ms
step:864/1393 train_time:100311ms step_avg:117.46ms
step:865/1393 train_time:100433ms step_avg:117.47ms
step:866/1393 train_time:100555ms step_avg:117.47ms
step:867/1393 train_time:100676ms step_avg:117.48ms
step:868/1393 train_time:100798ms step_avg:117.48ms
step:869/1393 train_time:100919ms step_avg:117.48ms
step:870/1393 train_time:101042ms step_avg:117.49ms
step:871/1393 train_time:101164ms step_avg:117.50ms
step:872/1393 train_time:101286ms step_avg:117.50ms
step:873/1393 train_time:101409ms step_avg:117.51ms
step:874/1393 train_time:101530ms step_avg:117.51ms
step:875/1393 train_time:101652ms step_avg:117.52ms
step:875/1393 val_loss:3.4725 train_time:101773ms step_avg:117.66ms
step:876/1393 train_time:101797ms step_avg:117.55ms
step:877/1393 train_time:101900ms step_avg:117.53ms
step:878/1393 train_time:102029ms step_avg:117.55ms
step:879/1393 train_time:102153ms step_avg:117.55ms
step:880/1393 train_time:102275ms step_avg:117.56ms
step:881/1393 train_time:102397ms step_avg:117.56ms
step:882/1393 train_time:102518ms step_avg:117.57ms
step:883/1393 train_time:102640ms step_avg:117.57ms
step:884/1393 train_time:102761ms step_avg:117.58ms
step:885/1393 train_time:102884ms step_avg:117.58ms
step:886/1393 train_time:103006ms step_avg:117.59ms
step:887/1393 train_time:103128ms step_avg:117.59ms
step:888/1393 train_time:103249ms step_avg:117.60ms
step:889/1393 train_time:103372ms step_avg:117.60ms
step:890/1393 train_time:103494ms step_avg:117.61ms
step:891/1393 train_time:103616ms step_avg:117.61ms
step:892/1393 train_time:103738ms step_avg:117.62ms
step:893/1393 train_time:103862ms step_avg:117.62ms
step:894/1393 train_time:103987ms step_avg:117.63ms
step:895/1393 train_time:104108ms step_avg:117.64ms
step:896/1393 train_time:104229ms step_avg:117.64ms
step:897/1393 train_time:104352ms step_avg:117.65ms
step:898/1393 train_time:104474ms step_avg:117.65ms
step:899/1393 train_time:104596ms step_avg:117.66ms
step:900/1393 train_time:104718ms step_avg:117.66ms
step:901/1393 train_time:104840ms step_avg:117.67ms
step:902/1393 train_time:104963ms step_avg:117.67ms
step:903/1393 train_time:105085ms step_avg:117.68ms
step:904/1393 train_time:105207ms step_avg:117.68ms
step:905/1393 train_time:105329ms step_avg:117.69ms
step:906/1393 train_time:105452ms step_avg:117.69ms
step:907/1393 train_time:105574ms step_avg:117.70ms
step:908/1393 train_time:105696ms step_avg:117.70ms
step:909/1393 train_time:105817ms step_avg:117.71ms
step:910/1393 train_time:105940ms step_avg:117.71ms
step:911/1393 train_time:106062ms step_avg:117.72ms
step:912/1393 train_time:106184ms step_avg:117.72ms
step:913/1393 train_time:106305ms step_avg:117.72ms
step:914/1393 train_time:106429ms step_avg:117.73ms
step:915/1393 train_time:106552ms step_avg:117.74ms
step:916/1393 train_time:106673ms step_avg:117.74ms
step:917/1393 train_time:106795ms step_avg:117.75ms
step:918/1393 train_time:106917ms step_avg:117.75ms
step:919/1393 train_time:107040ms step_avg:117.76ms
step:920/1393 train_time:107161ms step_avg:117.76ms
step:921/1393 train_time:107283ms step_avg:117.76ms
step:922/1393 train_time:107404ms step_avg:117.77ms
step:923/1393 train_time:107526ms step_avg:117.77ms
step:924/1393 train_time:107649ms step_avg:117.78ms
step:925/1393 train_time:107770ms step_avg:117.78ms
step:926/1393 train_time:107894ms step_avg:117.79ms
step:927/1393 train_time:108016ms step_avg:117.79ms
step:928/1393 train_time:108139ms step_avg:117.80ms
step:929/1393 train_time:108262ms step_avg:117.80ms
step:930/1393 train_time:108385ms step_avg:117.81ms
step:931/1393 train_time:108509ms step_avg:117.82ms
step:932/1393 train_time:108632ms step_avg:117.82ms
step:933/1393 train_time:108757ms step_avg:117.83ms
step:934/1393 train_time:108880ms step_avg:117.84ms
step:935/1393 train_time:109004ms step_avg:117.84ms
step:936/1393 train_time:109127ms step_avg:117.85ms
step:937/1393 train_time:109251ms step_avg:117.85ms
step:938/1393 train_time:109374ms step_avg:117.86ms
step:939/1393 train_time:109497ms step_avg:117.87ms
step:940/1393 train_time:109621ms step_avg:117.87ms
step:941/1393 train_time:109743ms step_avg:117.88ms
step:942/1393 train_time:109867ms step_avg:117.88ms
step:943/1393 train_time:109992ms step_avg:117.89ms
step:944/1393 train_time:110115ms step_avg:117.90ms
step:945/1393 train_time:110239ms step_avg:117.90ms
step:946/1393 train_time:110363ms step_avg:117.91ms
step:947/1393 train_time:110487ms step_avg:117.92ms
step:948/1393 train_time:110611ms step_avg:117.92ms
step:949/1393 train_time:110734ms step_avg:117.93ms
step:950/1393 train_time:110860ms step_avg:117.94ms
step:951/1393 train_time:110983ms step_avg:117.94ms
step:952/1393 train_time:111107ms step_avg:117.95ms
step:953/1393 train_time:111231ms step_avg:117.95ms
step:954/1393 train_time:111353ms step_avg:117.96ms
step:955/1393 train_time:111477ms step_avg:117.97ms
step:956/1393 train_time:111600ms step_avg:117.97ms
step:957/1393 train_time:111723ms step_avg:117.98ms
step:958/1393 train_time:111846ms step_avg:117.98ms
step:959/1393 train_time:111970ms step_avg:117.99ms
step:960/1393 train_time:112093ms step_avg:117.99ms
step:961/1393 train_time:112217ms step_avg:118.00ms
step:962/1393 train_time:112341ms step_avg:118.01ms
step:963/1393 train_time:112464ms step_avg:118.01ms
step:964/1393 train_time:112588ms step_avg:118.02ms
step:965/1393 train_time:112711ms step_avg:118.02ms
step:966/1393 train_time:112833ms step_avg:118.03ms
step:967/1393 train_time:112957ms step_avg:118.03ms
step:968/1393 train_time:113080ms step_avg:118.04ms
step:969/1393 train_time:113202ms step_avg:118.04ms
step:970/1393 train_time:113325ms step_avg:118.05ms
step:971/1393 train_time:113449ms step_avg:118.05ms
step:972/1393 train_time:113575ms step_avg:118.06ms
step:973/1393 train_time:113699ms step_avg:118.07ms
step:974/1393 train_time:113822ms step_avg:118.07ms
step:975/1393 train_time:113945ms step_avg:118.08ms
step:976/1393 train_time:114071ms step_avg:118.09ms
step:977/1393 train_time:114196ms step_avg:118.09ms
step:978/1393 train_time:114320ms step_avg:118.10ms
step:979/1393 train_time:114443ms step_avg:118.10ms
step:980/1393 train_time:114567ms step_avg:118.11ms
step:981/1393 train_time:114689ms step_avg:118.11ms
step:982/1393 train_time:114812ms step_avg:118.12ms
step:983/1393 train_time:114934ms step_avg:118.12ms
step:984/1393 train_time:115059ms step_avg:118.13ms
step:985/1393 train_time:115181ms step_avg:118.13ms
step:986/1393 train_time:115304ms step_avg:118.14ms
step:987/1393 train_time:115427ms step_avg:118.14ms
step:988/1393 train_time:115551ms step_avg:118.15ms
step:989/1393 train_time:115675ms step_avg:118.16ms
step:990/1393 train_time:115799ms step_avg:118.16ms
step:991/1393 train_time:115922ms step_avg:118.17ms
step:992/1393 train_time:116044ms step_avg:118.17ms
step:993/1393 train_time:116168ms step_avg:118.18ms
step:994/1393 train_time:116294ms step_avg:118.19ms
step:995/1393 train_time:116419ms step_avg:118.19ms
step:996/1393 train_time:116542ms step_avg:118.20ms
step:997/1393 train_time:116665ms step_avg:118.20ms
step:998/1393 train_time:116789ms step_avg:118.21ms
step:999/1393 train_time:116912ms step_avg:118.21ms
step:1000/1393 train_time:117037ms step_avg:118.22ms
step:1000/1393 val_loss:3.4133 train_time:117159ms step_avg:118.34ms
step:1001/1393 train_time:117184ms step_avg:118.25ms
step:1002/1393 train_time:117288ms step_avg:118.23ms
step:1003/1393 train_time:117421ms step_avg:118.25ms
step:1004/1393 train_time:117545ms step_avg:118.25ms
step:1005/1393 train_time:117669ms step_avg:118.26ms
step:1006/1393 train_time:117793ms step_avg:118.27ms
step:1007/1393 train_time:117916ms step_avg:118.27ms
step:1008/1393 train_time:118040ms step_avg:118.28ms
step:1009/1393 train_time:118165ms step_avg:118.28ms
step:1010/1393 train_time:118289ms step_avg:118.29ms
step:1011/1393 train_time:118413ms step_avg:118.29ms
step:1012/1393 train_time:118536ms step_avg:118.30ms
step:1013/1393 train_time:118660ms step_avg:118.30ms
step:1014/1393 train_time:118784ms step_avg:118.31ms
step:1015/1393 train_time:118907ms step_avg:118.32ms
step:1016/1393 train_time:119029ms step_avg:118.32ms
step:1017/1393 train_time:119153ms step_avg:118.32ms
step:1018/1393 train_time:119275ms step_avg:118.33ms
step:1019/1393 train_time:119399ms step_avg:118.33ms
step:1020/1393 train_time:119524ms step_avg:118.34ms
step:1021/1393 train_time:119648ms step_avg:118.35ms
step:1022/1393 train_time:119775ms step_avg:118.35ms
step:1023/1393 train_time:119899ms step_avg:118.36ms
step:1024/1393 train_time:120023ms step_avg:118.37ms
step:1025/1393 train_time:120148ms step_avg:118.37ms
step:1026/1393 train_time:120272ms step_avg:118.38ms
step:1027/1393 train_time:120395ms step_avg:118.38ms
step:1028/1393 train_time:120519ms step_avg:118.39ms
step:1029/1393 train_time:120644ms step_avg:118.39ms
step:1030/1393 train_time:120767ms step_avg:118.40ms
step:1031/1393 train_time:120892ms step_avg:118.41ms
step:1032/1393 train_time:121014ms step_avg:118.41ms
step:1033/1393 train_time:121137ms step_avg:118.41ms
step:1034/1393 train_time:121261ms step_avg:118.42ms
step:1035/1393 train_time:121385ms step_avg:118.42ms
step:1036/1393 train_time:121509ms step_avg:118.43ms
step:1037/1393 train_time:121633ms step_avg:118.43ms
step:1038/1393 train_time:121756ms step_avg:118.44ms
step:1039/1393 train_time:121880ms step_avg:118.44ms
step:1040/1393 train_time:122004ms step_avg:118.45ms
step:1041/1393 train_time:122126ms step_avg:118.45ms
step:1042/1393 train_time:122251ms step_avg:118.46ms
step:1043/1393 train_time:122376ms step_avg:118.47ms
step:1044/1393 train_time:122501ms step_avg:118.47ms
step:1045/1393 train_time:122625ms step_avg:118.48ms
step:1046/1393 train_time:122749ms step_avg:118.48ms
step:1047/1393 train_time:122871ms step_avg:118.49ms
step:1048/1393 train_time:122996ms step_avg:118.49ms
step:1049/1393 train_time:123120ms step_avg:118.50ms
step:1050/1393 train_time:123244ms step_avg:118.50ms
step:1051/1393 train_time:123368ms step_avg:118.51ms
step:1052/1393 train_time:123492ms step_avg:118.51ms
step:1053/1393 train_time:123617ms step_avg:118.52ms
step:1054/1393 train_time:123740ms step_avg:118.53ms
step:1055/1393 train_time:123866ms step_avg:118.53ms
step:1056/1393 train_time:123989ms step_avg:118.54ms
step:1057/1393 train_time:124112ms step_avg:118.54ms
step:1058/1393 train_time:124235ms step_avg:118.54ms
step:1059/1393 train_time:124359ms step_avg:118.55ms
step:1060/1393 train_time:124483ms step_avg:118.56ms
step:1061/1393 train_time:124606ms step_avg:118.56ms
step:1062/1393 train_time:124729ms step_avg:118.56ms
step:1063/1393 train_time:124855ms step_avg:118.57ms
step:1064/1393 train_time:124979ms step_avg:118.58ms
step:1065/1393 train_time:125103ms step_avg:118.58ms
step:1066/1393 train_time:125228ms step_avg:118.59ms
step:1067/1393 train_time:125354ms step_avg:118.59ms
step:1068/1393 train_time:125479ms step_avg:118.60ms
step:1069/1393 train_time:125603ms step_avg:118.61ms
step:1070/1393 train_time:125726ms step_avg:118.61ms
step:1071/1393 train_time:125851ms step_avg:118.62ms
step:1072/1393 train_time:125974ms step_avg:118.62ms
step:1073/1393 train_time:126098ms step_avg:118.62ms
step:1074/1393 train_time:126222ms step_avg:118.63ms
step:1075/1393 train_time:126345ms step_avg:118.63ms
step:1076/1393 train_time:126469ms step_avg:118.64ms
step:1077/1393 train_time:126593ms step_avg:118.64ms
step:1078/1393 train_time:126717ms step_avg:118.65ms
step:1079/1393 train_time:126841ms step_avg:118.65ms
step:1080/1393 train_time:126967ms step_avg:118.66ms
step:1081/1393 train_time:127091ms step_avg:118.67ms
step:1082/1393 train_time:127216ms step_avg:118.67ms
step:1083/1393 train_time:127339ms step_avg:118.68ms
step:1084/1393 train_time:127463ms step_avg:118.68ms
step:1085/1393 train_time:127586ms step_avg:118.68ms
step:1086/1393 train_time:127713ms step_avg:118.69ms
step:1087/1393 train_time:127836ms step_avg:118.70ms
step:1088/1393 train_time:127959ms step_avg:118.70ms
step:1089/1393 train_time:128085ms step_avg:118.71ms
step:1090/1393 train_time:128210ms step_avg:118.71ms
step:1091/1393 train_time:128334ms step_avg:118.72ms
step:1092/1393 train_time:128461ms step_avg:118.73ms
step:1093/1393 train_time:128584ms step_avg:118.73ms
step:1094/1393 train_time:128709ms step_avg:118.73ms
step:1095/1393 train_time:128833ms step_avg:118.74ms
step:1096/1393 train_time:128957ms step_avg:118.74ms
step:1097/1393 train_time:129080ms step_avg:118.75ms
step:1098/1393 train_time:129204ms step_avg:118.75ms
step:1099/1393 train_time:129328ms step_avg:118.76ms
step:1100/1393 train_time:129451ms step_avg:118.76ms
step:1101/1393 train_time:129578ms step_avg:118.77ms
step:1102/1393 train_time:129704ms step_avg:118.78ms
step:1103/1393 train_time:129827ms step_avg:118.78ms
step:1104/1393 train_time:129951ms step_avg:118.79ms
step:1105/1393 train_time:130074ms step_avg:118.79ms
step:1106/1393 train_time:130197ms step_avg:118.79ms
step:1107/1393 train_time:130321ms step_avg:118.80ms
step:1108/1393 train_time:130446ms step_avg:118.80ms
step:1109/1393 train_time:130571ms step_avg:118.81ms
step:1110/1393 train_time:130696ms step_avg:118.81ms
step:1111/1393 train_time:130820ms step_avg:118.82ms
step:1112/1393 train_time:130945ms step_avg:118.82ms
step:1113/1393 train_time:131068ms step_avg:118.83ms
step:1114/1393 train_time:131193ms step_avg:118.83ms
step:1115/1393 train_time:131318ms step_avg:118.84ms
step:1116/1393 train_time:131443ms step_avg:118.84ms
step:1117/1393 train_time:131567ms step_avg:118.85ms
step:1118/1393 train_time:131691ms step_avg:118.85ms
step:1119/1393 train_time:131814ms step_avg:118.86ms
step:1120/1393 train_time:131939ms step_avg:118.86ms
step:1121/1393 train_time:132062ms step_avg:118.87ms
step:1122/1393 train_time:132186ms step_avg:118.87ms
step:1123/1393 train_time:132310ms step_avg:118.88ms
step:1124/1393 train_time:132434ms step_avg:118.88ms
step:1125/1393 train_time:132557ms step_avg:118.88ms
step:1125/1393 val_loss:3.3635 train_time:132678ms step_avg:118.99ms
step:1126/1393 train_time:132703ms step_avg:118.91ms
step:1127/1393 train_time:132809ms step_avg:118.90ms
step:1128/1393 train_time:132936ms step_avg:118.91ms
step:1129/1393 train_time:133062ms step_avg:118.91ms
step:1130/1393 train_time:133187ms step_avg:118.92ms
step:1131/1393 train_time:133312ms step_avg:118.92ms
step:1132/1393 train_time:133435ms step_avg:118.93ms
step:1133/1393 train_time:133559ms step_avg:118.93ms
step:1134/1393 train_time:133683ms step_avg:118.93ms
step:1135/1393 train_time:133809ms step_avg:118.94ms
step:1136/1393 train_time:133934ms step_avg:118.95ms
step:1137/1393 train_time:134058ms step_avg:118.95ms
step:1138/1393 train_time:134184ms step_avg:118.96ms
step:1139/1393 train_time:134311ms step_avg:118.96ms
step:1140/1393 train_time:134436ms step_avg:118.97ms
step:1141/1393 train_time:134561ms step_avg:118.98ms
step:1142/1393 train_time:134686ms step_avg:118.98ms
step:1143/1393 train_time:134811ms step_avg:118.99ms
step:1144/1393 train_time:134935ms step_avg:118.99ms
step:1145/1393 train_time:135060ms step_avg:119.00ms
step:1146/1393 train_time:135184ms step_avg:119.00ms
step:1147/1393 train_time:135310ms step_avg:119.01ms
step:1148/1393 train_time:135435ms step_avg:119.01ms
step:1149/1393 train_time:135559ms step_avg:119.02ms
step:1150/1393 train_time:135684ms step_avg:119.02ms
step:1151/1393 train_time:135810ms step_avg:119.03ms
step:1152/1393 train_time:135935ms step_avg:119.03ms
step:1153/1393 train_time:136059ms step_avg:119.04ms
step:1154/1393 train_time:136184ms step_avg:119.04ms
step:1155/1393 train_time:136309ms step_avg:119.05ms
step:1156/1393 train_time:136434ms step_avg:119.05ms
step:1157/1393 train_time:136564ms step_avg:119.06ms
step:1158/1393 train_time:136688ms step_avg:119.07ms
step:1159/1393 train_time:136814ms step_avg:119.07ms
step:1160/1393 train_time:136943ms step_avg:119.08ms
step:1161/1393 train_time:137068ms step_avg:119.09ms
step:1162/1393 train_time:137192ms step_avg:119.09ms
step:1163/1393 train_time:137320ms step_avg:119.10ms
step:1164/1393 train_time:137445ms step_avg:119.10ms
step:1165/1393 train_time:137571ms step_avg:119.11ms
step:1166/1393 train_time:137699ms step_avg:119.12ms
step:1167/1393 train_time:137825ms step_avg:119.12ms
step:1168/1393 train_time:137950ms step_avg:119.13ms
step:1169/1393 train_time:138076ms step_avg:119.13ms
step:1170/1393 train_time:138206ms step_avg:119.14ms
step:1171/1393 train_time:138332ms step_avg:119.15ms
step:1172/1393 train_time:138456ms step_avg:119.15ms
step:1173/1393 train_time:138581ms step_avg:119.16ms
step:1174/1393 train_time:138707ms step_avg:119.16ms
step:1175/1393 train_time:138832ms step_avg:119.17ms
step:1176/1393 train_time:138958ms step_avg:119.17ms
step:1177/1393 train_time:139083ms step_avg:119.18ms
step:1178/1393 train_time:139208ms step_avg:119.19ms
step:1179/1393 train_time:139335ms step_avg:119.19ms
step:1180/1393 train_time:139462ms step_avg:119.20ms
step:1181/1393 train_time:139588ms step_avg:119.20ms
step:1182/1393 train_time:139714ms step_avg:119.21ms
step:1183/1393 train_time:139838ms step_avg:119.21ms
step:1184/1393 train_time:139963ms step_avg:119.22ms
step:1185/1393 train_time:140093ms step_avg:119.23ms
step:1186/1393 train_time:140219ms step_avg:119.23ms
step:1187/1393 train_time:140343ms step_avg:119.24ms
step:1188/1393 train_time:140468ms step_avg:119.24ms
step:1189/1393 train_time:140592ms step_avg:119.25ms
step:1190/1393 train_time:140717ms step_avg:119.25ms
step:1191/1393 train_time:140843ms step_avg:119.26ms
step:1192/1393 train_time:140970ms step_avg:119.26ms
step:1193/1393 train_time:141096ms step_avg:119.27ms
step:1194/1393 train_time:141221ms step_avg:119.27ms
step:1195/1393 train_time:141346ms step_avg:119.28ms
step:1196/1393 train_time:141471ms step_avg:119.28ms
step:1197/1393 train_time:141601ms step_avg:119.29ms
step:1198/1393 train_time:141724ms step_avg:119.30ms
step:1199/1393 train_time:141850ms step_avg:119.30ms
step:1200/1393 train_time:141974ms step_avg:119.31ms
step:1201/1393 train_time:142098ms step_avg:119.31ms
step:1202/1393 train_time:142224ms step_avg:119.32ms
step:1203/1393 train_time:142348ms step_avg:119.32ms
step:1204/1393 train_time:142473ms step_avg:119.32ms
step:1205/1393 train_time:142599ms step_avg:119.33ms
step:1206/1393 train_time:142724ms step_avg:119.33ms
step:1207/1393 train_time:142850ms step_avg:119.34ms
step:1208/1393 train_time:142975ms step_avg:119.34ms
step:1209/1393 train_time:143101ms step_avg:119.35ms
step:1210/1393 train_time:143226ms step_avg:119.36ms
step:1211/1393 train_time:143351ms step_avg:119.36ms
step:1212/1393 train_time:143477ms step_avg:119.37ms
step:1213/1393 train_time:143603ms step_avg:119.37ms
step:1214/1393 train_time:143729ms step_avg:119.38ms
step:1215/1393 train_time:143853ms step_avg:119.38ms
step:1216/1393 train_time:143978ms step_avg:119.38ms
step:1217/1393 train_time:144102ms step_avg:119.39ms
step:1218/1393 train_time:144228ms step_avg:119.39ms
step:1219/1393 train_time:144351ms step_avg:119.40ms
step:1220/1393 train_time:144481ms step_avg:119.41ms
step:1221/1393 train_time:144606ms step_avg:119.41ms
step:1222/1393 train_time:144733ms step_avg:119.42ms
step:1223/1393 train_time:144860ms step_avg:119.42ms
step:1224/1393 train_time:144985ms step_avg:119.43ms
step:1225/1393 train_time:145111ms step_avg:119.43ms
step:1226/1393 train_time:145236ms step_avg:119.44ms
step:1227/1393 train_time:145361ms step_avg:119.44ms
step:1228/1393 train_time:145485ms step_avg:119.45ms
step:1229/1393 train_time:145612ms step_avg:119.45ms
step:1230/1393 train_time:145736ms step_avg:119.46ms
step:1231/1393 train_time:145861ms step_avg:119.46ms
step:1232/1393 train_time:145986ms step_avg:119.46ms
step:1233/1393 train_time:146111ms step_avg:119.47ms
step:1234/1393 train_time:146236ms step_avg:119.47ms
step:1235/1393 train_time:146362ms step_avg:119.48ms
step:1236/1393 train_time:146490ms step_avg:119.49ms
step:1237/1393 train_time:146615ms step_avg:119.49ms
step:1238/1393 train_time:146739ms step_avg:119.49ms
step:1239/1393 train_time:146865ms step_avg:119.50ms
step:1240/1393 train_time:146994ms step_avg:119.51ms
step:1241/1393 train_time:147118ms step_avg:119.51ms
step:1242/1393 train_time:147244ms step_avg:119.52ms
step:1243/1393 train_time:147371ms step_avg:119.52ms
step:1244/1393 train_time:147498ms step_avg:119.53ms
step:1245/1393 train_time:147623ms step_avg:119.53ms
step:1246/1393 train_time:147748ms step_avg:119.54ms
step:1247/1393 train_time:147873ms step_avg:119.54ms
step:1248/1393 train_time:147999ms step_avg:119.55ms
step:1249/1393 train_time:148124ms step_avg:119.55ms
step:1250/1393 train_time:148249ms step_avg:119.56ms
step:1250/1393 val_loss:3.3188 train_time:148376ms step_avg:119.66ms
step:1251/1393 train_time:148400ms step_avg:119.58ms
step:1252/1393 train_time:148510ms step_avg:119.57ms
step:1253/1393 train_time:148639ms step_avg:119.58ms
step:1254/1393 train_time:148764ms step_avg:119.59ms
step:1255/1393 train_time:148890ms step_avg:119.59ms
step:1256/1393 train_time:149016ms step_avg:119.60ms
step:1257/1393 train_time:149140ms step_avg:119.60ms
step:1258/1393 train_time:149264ms step_avg:119.60ms
step:1259/1393 train_time:149390ms step_avg:119.61ms
step:1260/1393 train_time:149515ms step_avg:119.61ms
step:1261/1393 train_time:149644ms step_avg:119.62ms
step:1262/1393 train_time:149771ms step_avg:119.63ms
step:1263/1393 train_time:149895ms step_avg:119.63ms
step:1264/1393 train_time:150019ms step_avg:119.63ms
step:1265/1393 train_time:150145ms step_avg:119.64ms
step:1266/1393 train_time:150270ms step_avg:119.64ms
step:1267/1393 train_time:150395ms step_avg:119.65ms
step:1268/1393 train_time:150522ms step_avg:119.65ms
step:1269/1393 train_time:150648ms step_avg:119.66ms
step:1270/1393 train_time:150774ms step_avg:119.66ms
step:1271/1393 train_time:150901ms step_avg:119.67ms
step:1272/1393 train_time:151029ms step_avg:119.67ms
step:1273/1393 train_time:151154ms step_avg:119.68ms
step:1274/1393 train_time:151281ms step_avg:119.68ms
step:1275/1393 train_time:151407ms step_avg:119.69ms
step:1276/1393 train_time:151532ms step_avg:119.69ms
step:1277/1393 train_time:151658ms step_avg:119.70ms
step:1278/1393 train_time:151785ms step_avg:119.70ms
step:1279/1393 train_time:151910ms step_avg:119.71ms
step:1280/1393 train_time:152034ms step_avg:119.71ms
step:1281/1393 train_time:152158ms step_avg:119.72ms
step:1282/1393 train_time:152283ms step_avg:119.72ms
step:1283/1393 train_time:152410ms step_avg:119.73ms
step:1284/1393 train_time:152536ms step_avg:119.73ms
step:1285/1393 train_time:152663ms step_avg:119.74ms
step:1286/1393 train_time:152788ms step_avg:119.74ms
step:1287/1393 train_time:152915ms step_avg:119.75ms
step:1288/1393 train_time:153039ms step_avg:119.75ms
step:1289/1393 train_time:153165ms step_avg:119.75ms
step:1290/1393 train_time:153291ms step_avg:119.76ms
step:1291/1393 train_time:153415ms step_avg:119.76ms
step:1292/1393 train_time:153540ms step_avg:119.77ms
step:1293/1393 train_time:153665ms step_avg:119.77ms
step:1294/1393 train_time:153791ms step_avg:119.77ms
step:1295/1393 train_time:153918ms step_avg:119.78ms
step:1296/1393 train_time:154043ms step_avg:119.78ms
step:1297/1393 train_time:154168ms step_avg:119.79ms
step:1298/1393 train_time:154292ms step_avg:119.79ms
step:1299/1393 train_time:154420ms step_avg:119.80ms
step:1300/1393 train_time:154545ms step_avg:119.80ms
step:1301/1393 train_time:154670ms step_avg:119.81ms
step:1302/1393 train_time:154794ms step_avg:119.81ms
step:1303/1393 train_time:154923ms step_avg:119.82ms
step:1304/1393 train_time:155049ms step_avg:119.82ms
step:1305/1393 train_time:155173ms step_avg:119.82ms
step:1306/1393 train_time:155300ms step_avg:119.83ms
step:1307/1393 train_time:155427ms step_avg:119.84ms
step:1308/1393 train_time:155552ms step_avg:119.84ms
step:1309/1393 train_time:155682ms step_avg:119.85ms
step:1310/1393 train_time:155809ms step_avg:119.85ms
step:1311/1393 train_time:155934ms step_avg:119.86ms
step:1312/1393 train_time:156063ms step_avg:119.86ms
step:1313/1393 train_time:156190ms step_avg:119.87ms
step:1314/1393 train_time:156316ms step_avg:119.87ms
step:1315/1393 train_time:156442ms step_avg:119.88ms
step:1316/1393 train_time:156566ms step_avg:119.88ms
step:1317/1393 train_time:156694ms step_avg:119.89ms
step:1318/1393 train_time:156820ms step_avg:119.89ms
step:1319/1393 train_time:156945ms step_avg:119.90ms
step:1320/1393 train_time:157071ms step_avg:119.90ms
step:1321/1393 train_time:157197ms step_avg:119.91ms
step:1322/1393 train_time:157321ms step_avg:119.91ms
step:1323/1393 train_time:157447ms step_avg:119.91ms
step:1324/1393 train_time:157571ms step_avg:119.92ms
step:1325/1393 train_time:157695ms step_avg:119.92ms
step:1326/1393 train_time:157822ms step_avg:119.93ms
step:1327/1393 train_time:157947ms step_avg:119.93ms
step:1328/1393 train_time:158073ms step_avg:119.93ms
step:1329/1393 train_time:158198ms step_avg:119.94ms
step:1330/1393 train_time:158324ms step_avg:119.94ms
step:1331/1393 train_time:158448ms step_avg:119.95ms
step:1332/1393 train_time:158574ms step_avg:119.95ms
step:1333/1393 train_time:158701ms step_avg:119.96ms
step:1334/1393 train_time:158826ms step_avg:119.96ms
step:1335/1393 train_time:158951ms step_avg:119.96ms
step:1336/1393 train_time:159075ms step_avg:119.97ms
step:1337/1393 train_time:159200ms step_avg:119.97ms
step:1338/1393 train_time:159328ms step_avg:119.98ms
step:1339/1393 train_time:159452ms step_avg:119.98ms
step:1340/1393 train_time:159576ms step_avg:119.98ms
step:1341/1393 train_time:159703ms step_avg:119.99ms
step:1342/1393 train_time:159828ms step_avg:119.99ms
step:1343/1393 train_time:159955ms step_avg:120.00ms
step:1344/1393 train_time:160083ms step_avg:120.00ms
step:1345/1393 train_time:160207ms step_avg:120.01ms
step:1346/1393 train_time:160335ms step_avg:120.01ms
step:1347/1393 train_time:160462ms step_avg:120.02ms
step:1348/1393 train_time:160588ms step_avg:120.02ms
step:1349/1393 train_time:160720ms step_avg:120.03ms
step:1350/1393 train_time:160845ms step_avg:120.03ms
step:1351/1393 train_time:160974ms step_avg:120.04ms
step:1352/1393 train_time:161102ms step_avg:120.05ms
step:1353/1393 train_time:161229ms step_avg:120.05ms
step:1354/1393 train_time:161358ms step_avg:120.06ms
step:1355/1393 train_time:161484ms step_avg:120.06ms
step:1356/1393 train_time:161610ms step_avg:120.07ms
step:1357/1393 train_time:161737ms step_avg:120.07ms
step:1358/1393 train_time:161864ms step_avg:120.08ms
step:1359/1393 train_time:161994ms step_avg:120.08ms
step:1360/1393 train_time:162124ms step_avg:120.09ms
step:1361/1393 train_time:162254ms step_avg:120.10ms
step:1362/1393 train_time:162382ms step_avg:120.11ms
step:1363/1393 train_time:162510ms step_avg:120.11ms
step:1364/1393 train_time:162641ms step_avg:120.12ms
step:1365/1393 train_time:162766ms step_avg:120.12ms
step:1366/1393 train_time:162892ms step_avg:120.13ms
step:1367/1393 train_time:163019ms step_avg:120.13ms
step:1368/1393 train_time:163144ms step_avg:120.14ms
step:1369/1393 train_time:163270ms step_avg:120.14ms
step:1370/1393 train_time:163398ms step_avg:120.15ms
step:1371/1393 train_time:163522ms step_avg:120.15ms
step:1372/1393 train_time:163649ms step_avg:120.15ms
step:1373/1393 train_time:163774ms step_avg:120.16ms
step:1374/1393 train_time:163901ms step_avg:120.16ms
step:1375/1393 train_time:164027ms step_avg:120.17ms
step:1375/1393 val_loss:3.2853 train_time:164152ms step_avg:120.26ms
step:1376/1393 train_time:164176ms step_avg:120.19ms
step:1377/1393 train_time:164286ms step_avg:120.18ms
step:1378/1393 train_time:164414ms step_avg:120.19ms
step:1379/1393 train_time:164544ms step_avg:120.19ms
step:1380/1393 train_time:164670ms step_avg:120.20ms
step:1381/1393 train_time:164798ms step_avg:120.20ms
step:1382/1393 train_time:164925ms step_avg:120.21ms
step:1383/1393 train_time:165052ms step_avg:120.21ms
step:1384/1393 train_time:165177ms step_avg:120.22ms
step:1385/1393 train_time:165304ms step_avg:120.22ms
step:1386/1393 train_time:165430ms step_avg:120.23ms
step:1387/1393 train_time:165556ms step_avg:120.23ms
step:1388/1393 train_time:165688ms step_avg:120.24ms
step:1389/1393 train_time:165813ms step_avg:120.24ms
step:1390/1393 train_time:165939ms step_avg:120.25ms
step:1391/1393 train_time:166066ms step_avg:120.25ms
step:1392/1393 train_time:166192ms step_avg:120.26ms
step:1393/1393 train_time:166318ms step_avg:120.26ms
step:1393/1393 val_loss:3.2816 train_time:166444ms step_avg:120.35ms
peak memory allocated: 31573 MiB reserved: 32976 MiB
