import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)
flex_kernel_options = None
if torch.cuda.get_device_name(0).endswith(("3090", "4090")):
    flex_kernel_options = {"BLOCK_M": 64, "BLOCK_N": 64, "BLOCK_M1": 32, "BLOCK_N1": 64, "BLOCK_M2": 64, "BLOCK_N2": 32}

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale, kernel_options=flex_kernel_options)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_idx) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor = None, sliding_window_num_blocks: Tensor = 0):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 28415).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x) if not self.training else lm_head_fp8(x, self.lm_head.weight)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)

        if target_seq is None:
            return logits

        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb-tokmon-10B/english-28416-balanced/fineweb-tokmon_train_*.bin" # input .bin to train on
    val_files = "data/fineweb-tokmon-10B/english-28416-balanced/fineweb-tokmon_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # fewer tokens but equivalent text for validation, snapped to nearest seq_len
    val_ratio = 0.99011 # equivalent token density on validation tokens to that of GPT-2
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()


def main():
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

    model = GPT(vocab_size=28416, num_layers=12, num_heads=6, model_dim=768).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    k = 1.08
    adam_params = [dict(params=head_params, lr=0.008*k), dict(params=embed_params, lr=0.6*k), dict(params=scalar_params, lr=0.04*k)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05*k, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model)
    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss = (val_loss * args.val_ratio) / val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
    )
    dist.destroy_process_group()


if __name__ == "__main__":
    main()
====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu124 compiled for CUDA 12.4
Mon Jan 20 17:27:49 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:61:00.0 Off |                    0 |
| N/A   39C    P0            132W /  700W |    7714MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:62:00.0 Off |                    0 |
| N/A   42C    P0            123W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:63:00.0 Off |                    0 |
| N/A   43C    P0            123W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   38C    P0            120W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:6A:00.0 Off |                    0 |
| N/A   36C    P0            117W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:6B:00.0 Off |                    0 |
| N/A   44C    P0            126W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:6C:00.0 Off |                    0 |
| N/A   41C    P0            121W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:6D:00.0 Off |                    0 |
| N/A   38C    P0            120W /  700W |    3212MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     43162      C   /usr/bin/python3                             3394MiB |
|    0   N/A  N/A     43163      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     43164      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     43165      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     43166      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     43167      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     43168      C   /usr/bin/python3                              610MiB |
|    0   N/A  N/A     43169      C   /usr/bin/python3                              610MiB |
|    1   N/A  N/A     43163      C   /usr/bin/python3                             3442MiB |
|    2   N/A  N/A     43164      C   /usr/bin/python3                             3442MiB |
|    3   N/A  N/A     43165      C   /usr/bin/python3                             3442MiB |
|    4   N/A  N/A     43166      C   /usr/bin/python3                             3442MiB |
|    5   N/A  N/A     43167      C   /usr/bin/python3                             3442MiB |
|    6   N/A  N/A     43168      C   /usr/bin/python3                             3442MiB |
|    7   N/A  N/A     43169      C   /usr/bin/python3                             3202MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.1533 train_time:0ms step_avg:nanms
step:1/1393 train_time:22425ms step_avg:nanms
step:2/1393 train_time:22461ms step_avg:nanms
step:3/1393 train_time:23383ms step_avg:nanms
step:4/1393 train_time:23495ms step_avg:nanms
step:5/1393 train_time:23608ms step_avg:nanms
step:6/1393 train_time:23721ms step_avg:nanms
step:7/1393 train_time:23833ms step_avg:nanms
step:8/1393 train_time:23946ms step_avg:nanms
step:9/1393 train_time:24058ms step_avg:nanms
step:10/1393 train_time:24172ms step_avg:nanms
step:11/1393 train_time:113ms step_avg:nanms
step:12/1393 train_time:226ms step_avg:nanms
step:13/1393 train_time:340ms step_avg:113.41ms
step:14/1393 train_time:453ms step_avg:113.35ms
step:15/1393 train_time:567ms step_avg:113.30ms
step:16/1393 train_time:679ms step_avg:113.22ms
step:17/1393 train_time:793ms step_avg:113.23ms
step:18/1393 train_time:906ms step_avg:113.22ms
step:19/1393 train_time:1020ms step_avg:113.33ms
step:20/1393 train_time:1133ms step_avg:113.25ms
step:21/1393 train_time:1246ms step_avg:113.23ms
step:22/1393 train_time:1359ms step_avg:113.27ms
step:23/1393 train_time:1473ms step_avg:113.28ms
step:24/1393 train_time:1585ms step_avg:113.25ms
step:25/1393 train_time:1699ms step_avg:113.30ms
step:26/1393 train_time:1813ms step_avg:113.29ms
step:27/1393 train_time:1926ms step_avg:113.31ms
step:28/1393 train_time:2040ms step_avg:113.31ms
step:29/1393 train_time:2153ms step_avg:113.31ms
step:30/1393 train_time:2266ms step_avg:113.32ms
step:31/1393 train_time:2380ms step_avg:113.35ms
step:32/1393 train_time:2494ms step_avg:113.38ms
step:33/1393 train_time:2607ms step_avg:113.36ms
step:34/1393 train_time:2721ms step_avg:113.38ms
step:35/1393 train_time:2834ms step_avg:113.36ms
step:36/1393 train_time:2947ms step_avg:113.36ms
step:37/1393 train_time:3060ms step_avg:113.33ms
step:38/1393 train_time:3174ms step_avg:113.35ms
step:39/1393 train_time:3287ms step_avg:113.33ms
step:40/1393 train_time:3400ms step_avg:113.33ms
step:41/1393 train_time:3513ms step_avg:113.32ms
step:42/1393 train_time:3627ms step_avg:113.35ms
step:43/1393 train_time:3741ms step_avg:113.37ms
step:44/1393 train_time:3855ms step_avg:113.39ms
step:45/1393 train_time:3969ms step_avg:113.40ms
step:46/1393 train_time:4082ms step_avg:113.38ms
step:47/1393 train_time:4195ms step_avg:113.37ms
step:48/1393 train_time:4307ms step_avg:113.35ms
step:49/1393 train_time:4420ms step_avg:113.34ms
step:50/1393 train_time:4532ms step_avg:113.31ms
step:51/1393 train_time:4646ms step_avg:113.31ms
step:52/1393 train_time:4759ms step_avg:113.30ms
step:53/1393 train_time:4871ms step_avg:113.29ms
step:54/1393 train_time:4984ms step_avg:113.28ms
step:55/1393 train_time:5098ms step_avg:113.29ms
step:56/1393 train_time:5211ms step_avg:113.28ms
step:57/1393 train_time:5324ms step_avg:113.28ms
step:58/1393 train_time:5437ms step_avg:113.28ms
step:59/1393 train_time:5551ms step_avg:113.28ms
step:60/1393 train_time:5664ms step_avg:113.27ms
step:61/1393 train_time:5776ms step_avg:113.26ms
step:62/1393 train_time:5890ms step_avg:113.27ms
step:63/1393 train_time:6002ms step_avg:113.25ms
step:64/1393 train_time:6115ms step_avg:113.24ms
step:65/1393 train_time:6227ms step_avg:113.23ms
step:66/1393 train_time:6340ms step_avg:113.21ms
step:67/1393 train_time:6453ms step_avg:113.20ms
step:68/1393 train_time:6565ms step_avg:113.20ms
step:69/1393 train_time:6678ms step_avg:113.18ms
step:70/1393 train_time:6791ms step_avg:113.18ms
step:71/1393 train_time:6904ms step_avg:113.17ms
step:72/1393 train_time:7017ms step_avg:113.18ms
step:73/1393 train_time:7130ms step_avg:113.18ms
step:74/1393 train_time:7244ms step_avg:113.19ms
step:75/1393 train_time:7357ms step_avg:113.19ms
step:76/1393 train_time:7470ms step_avg:113.19ms
step:77/1393 train_time:7584ms step_avg:113.19ms
step:78/1393 train_time:7697ms step_avg:113.19ms
step:79/1393 train_time:7810ms step_avg:113.19ms
step:80/1393 train_time:7923ms step_avg:113.19ms
step:81/1393 train_time:8036ms step_avg:113.18ms
step:82/1393 train_time:8149ms step_avg:113.18ms
step:83/1393 train_time:8261ms step_avg:113.17ms
step:84/1393 train_time:8374ms step_avg:113.16ms
step:85/1393 train_time:8487ms step_avg:113.16ms
step:86/1393 train_time:8601ms step_avg:113.17ms
step:87/1393 train_time:8714ms step_avg:113.17ms
step:88/1393 train_time:8827ms step_avg:113.17ms
step:89/1393 train_time:8940ms step_avg:113.16ms
step:90/1393 train_time:9054ms step_avg:113.17ms
step:91/1393 train_time:9166ms step_avg:113.16ms
step:92/1393 train_time:9280ms step_avg:113.17ms
step:93/1393 train_time:9393ms step_avg:113.17ms
step:94/1393 train_time:9506ms step_avg:113.16ms
step:95/1393 train_time:9619ms step_avg:113.16ms
step:96/1393 train_time:9731ms step_avg:113.15ms
step:97/1393 train_time:9844ms step_avg:113.15ms
step:98/1393 train_time:9958ms step_avg:113.16ms
step:99/1393 train_time:10071ms step_avg:113.16ms
step:100/1393 train_time:10183ms step_avg:113.15ms
step:101/1393 train_time:10296ms step_avg:113.15ms
step:102/1393 train_time:10409ms step_avg:113.15ms
step:103/1393 train_time:10521ms step_avg:113.13ms
step:104/1393 train_time:10635ms step_avg:113.14ms
step:105/1393 train_time:10748ms step_avg:113.14ms
step:106/1393 train_time:10862ms step_avg:113.14ms
step:107/1393 train_time:10976ms step_avg:113.15ms
step:108/1393 train_time:11089ms step_avg:113.16ms
step:109/1393 train_time:11203ms step_avg:113.16ms
step:110/1393 train_time:11317ms step_avg:113.17ms
step:111/1393 train_time:11430ms step_avg:113.17ms
step:112/1393 train_time:11544ms step_avg:113.18ms
step:113/1393 train_time:11657ms step_avg:113.18ms
step:114/1393 train_time:11771ms step_avg:113.18ms
step:115/1393 train_time:11885ms step_avg:113.19ms
step:116/1393 train_time:11999ms step_avg:113.19ms
step:117/1393 train_time:12112ms step_avg:113.20ms
step:118/1393 train_time:12226ms step_avg:113.20ms
step:119/1393 train_time:12340ms step_avg:113.21ms
step:120/1393 train_time:12454ms step_avg:113.22ms
step:121/1393 train_time:12568ms step_avg:113.22ms
step:122/1393 train_time:12682ms step_avg:113.23ms
step:123/1393 train_time:12796ms step_avg:113.24ms
step:124/1393 train_time:12910ms step_avg:113.24ms
step:125/1393 train_time:13024ms step_avg:113.25ms
step:125/1393 val_loss:4.3331 train_time:13136ms step_avg:114.23ms
step:126/1393 train_time:13159ms step_avg:113.44ms
step:127/1393 train_time:13253ms step_avg:113.27ms
step:128/1393 train_time:13374ms step_avg:113.34ms
step:129/1393 train_time:13491ms step_avg:113.37ms
step:130/1393 train_time:13605ms step_avg:113.37ms
step:131/1393 train_time:13719ms step_avg:113.38ms
step:132/1393 train_time:13833ms step_avg:113.38ms
step:133/1393 train_time:13948ms step_avg:113.40ms
step:134/1393 train_time:14061ms step_avg:113.40ms
step:135/1393 train_time:14175ms step_avg:113.40ms
step:136/1393 train_time:14289ms step_avg:113.41ms
step:137/1393 train_time:14403ms step_avg:113.41ms
step:138/1393 train_time:14517ms step_avg:113.41ms
step:139/1393 train_time:14631ms step_avg:113.42ms
step:140/1393 train_time:14745ms step_avg:113.42ms
step:141/1393 train_time:14858ms step_avg:113.42ms
step:142/1393 train_time:14973ms step_avg:113.43ms
step:143/1393 train_time:15088ms step_avg:113.44ms
step:144/1393 train_time:15201ms step_avg:113.44ms
step:145/1393 train_time:15315ms step_avg:113.45ms
step:146/1393 train_time:15429ms step_avg:113.45ms
step:147/1393 train_time:15542ms step_avg:113.45ms
step:148/1393 train_time:15656ms step_avg:113.45ms
step:149/1393 train_time:15769ms step_avg:113.45ms
step:150/1393 train_time:15883ms step_avg:113.45ms
step:151/1393 train_time:15997ms step_avg:113.46ms
step:152/1393 train_time:16111ms step_avg:113.46ms
step:153/1393 train_time:16225ms step_avg:113.46ms
step:154/1393 train_time:16338ms step_avg:113.46ms
step:155/1393 train_time:16452ms step_avg:113.46ms
step:156/1393 train_time:16566ms step_avg:113.47ms
step:157/1393 train_time:16680ms step_avg:113.47ms
step:158/1393 train_time:16794ms step_avg:113.48ms
step:159/1393 train_time:16909ms step_avg:113.48ms
step:160/1393 train_time:17023ms step_avg:113.48ms
step:161/1393 train_time:17136ms step_avg:113.48ms
step:162/1393 train_time:17250ms step_avg:113.49ms
step:163/1393 train_time:17363ms step_avg:113.49ms
step:164/1393 train_time:17478ms step_avg:113.49ms
step:165/1393 train_time:17591ms step_avg:113.49ms
step:166/1393 train_time:17704ms step_avg:113.49ms
step:167/1393 train_time:17819ms step_avg:113.49ms
step:168/1393 train_time:17932ms step_avg:113.50ms
step:169/1393 train_time:18047ms step_avg:113.50ms
step:170/1393 train_time:18160ms step_avg:113.50ms
step:171/1393 train_time:18273ms step_avg:113.50ms
step:172/1393 train_time:18387ms step_avg:113.50ms
step:173/1393 train_time:18501ms step_avg:113.50ms
step:174/1393 train_time:18615ms step_avg:113.51ms
step:175/1393 train_time:18729ms step_avg:113.51ms
step:176/1393 train_time:18842ms step_avg:113.51ms
step:177/1393 train_time:18956ms step_avg:113.51ms
step:178/1393 train_time:19070ms step_avg:113.51ms
step:179/1393 train_time:19183ms step_avg:113.51ms
step:180/1393 train_time:19297ms step_avg:113.51ms
step:181/1393 train_time:19410ms step_avg:113.51ms
step:182/1393 train_time:19524ms step_avg:113.51ms
step:183/1393 train_time:19638ms step_avg:113.51ms
step:184/1393 train_time:19751ms step_avg:113.51ms
step:185/1393 train_time:19864ms step_avg:113.51ms
step:186/1393 train_time:19977ms step_avg:113.51ms
step:187/1393 train_time:20091ms step_avg:113.51ms
step:188/1393 train_time:20205ms step_avg:113.51ms
step:189/1393 train_time:20318ms step_avg:113.51ms
step:190/1393 train_time:20433ms step_avg:113.52ms
step:191/1393 train_time:20547ms step_avg:113.52ms
step:192/1393 train_time:20661ms step_avg:113.52ms
step:193/1393 train_time:20775ms step_avg:113.52ms
step:194/1393 train_time:20889ms step_avg:113.53ms
step:195/1393 train_time:21003ms step_avg:113.53ms
step:196/1393 train_time:21116ms step_avg:113.53ms
step:197/1393 train_time:21230ms step_avg:113.53ms
step:198/1393 train_time:21343ms step_avg:113.53ms
step:199/1393 train_time:21457ms step_avg:113.53ms
step:200/1393 train_time:21570ms step_avg:113.53ms
step:201/1393 train_time:21684ms step_avg:113.53ms
step:202/1393 train_time:21798ms step_avg:113.53ms
step:203/1393 train_time:21913ms step_avg:113.54ms
step:204/1393 train_time:22025ms step_avg:113.53ms
step:205/1393 train_time:22139ms step_avg:113.53ms
step:206/1393 train_time:22252ms step_avg:113.53ms
step:207/1393 train_time:22366ms step_avg:113.53ms
step:208/1393 train_time:22479ms step_avg:113.53ms
step:209/1393 train_time:22593ms step_avg:113.53ms
step:210/1393 train_time:22707ms step_avg:113.54ms
step:211/1393 train_time:22822ms step_avg:113.54ms
step:212/1393 train_time:22936ms step_avg:113.54ms
step:213/1393 train_time:23050ms step_avg:113.55ms
step:214/1393 train_time:23164ms step_avg:113.55ms
step:215/1393 train_time:23278ms step_avg:113.55ms
step:216/1393 train_time:23393ms step_avg:113.56ms
step:217/1393 train_time:23507ms step_avg:113.56ms
step:218/1393 train_time:23622ms step_avg:113.57ms
step:219/1393 train_time:23736ms step_avg:113.57ms
step:220/1393 train_time:23850ms step_avg:113.57ms
step:221/1393 train_time:23964ms step_avg:113.58ms
step:222/1393 train_time:24078ms step_avg:113.58ms
step:223/1393 train_time:24192ms step_avg:113.58ms
step:224/1393 train_time:24306ms step_avg:113.58ms
step:225/1393 train_time:24420ms step_avg:113.58ms
step:226/1393 train_time:24535ms step_avg:113.59ms
step:227/1393 train_time:24649ms step_avg:113.59ms
step:228/1393 train_time:24763ms step_avg:113.59ms
step:229/1393 train_time:24878ms step_avg:113.60ms
step:230/1393 train_time:24992ms step_avg:113.60ms
step:231/1393 train_time:25106ms step_avg:113.60ms
step:232/1393 train_time:25220ms step_avg:113.60ms
step:233/1393 train_time:25335ms step_avg:113.61ms
step:234/1393 train_time:25449ms step_avg:113.61ms
step:235/1393 train_time:25563ms step_avg:113.61ms
step:236/1393 train_time:25677ms step_avg:113.62ms
step:237/1393 train_time:25792ms step_avg:113.62ms
step:238/1393 train_time:25907ms step_avg:113.63ms
step:239/1393 train_time:26021ms step_avg:113.63ms
step:240/1393 train_time:26136ms step_avg:113.63ms
step:241/1393 train_time:26249ms step_avg:113.63ms
step:242/1393 train_time:26364ms step_avg:113.64ms
step:243/1393 train_time:26478ms step_avg:113.64ms
step:244/1393 train_time:26593ms step_avg:113.64ms
step:245/1393 train_time:26707ms step_avg:113.65ms
step:246/1393 train_time:26822ms step_avg:113.65ms
step:247/1393 train_time:26937ms step_avg:113.66ms
step:248/1393 train_time:27051ms step_avg:113.66ms
step:249/1393 train_time:27165ms step_avg:113.66ms
step:250/1393 train_time:27280ms step_avg:113.67ms
step:250/1393 val_loss:3.9405 train_time:27393ms step_avg:114.14ms
step:251/1393 train_time:27416ms step_avg:113.76ms
step:252/1393 train_time:27511ms step_avg:113.68ms
step:253/1393 train_time:27634ms step_avg:113.72ms
step:254/1393 train_time:27751ms step_avg:113.73ms
step:255/1393 train_time:27866ms step_avg:113.74ms
step:256/1393 train_time:27980ms step_avg:113.74ms
step:257/1393 train_time:28094ms step_avg:113.74ms
step:258/1393 train_time:28209ms step_avg:113.74ms
step:259/1393 train_time:28323ms step_avg:113.75ms
step:260/1393 train_time:28437ms step_avg:113.75ms
step:261/1393 train_time:28551ms step_avg:113.75ms
step:262/1393 train_time:28665ms step_avg:113.75ms
step:263/1393 train_time:28780ms step_avg:113.75ms
step:264/1393 train_time:28894ms step_avg:113.76ms
step:265/1393 train_time:29008ms step_avg:113.76ms
step:266/1393 train_time:29122ms step_avg:113.76ms
step:267/1393 train_time:29237ms step_avg:113.76ms
step:268/1393 train_time:29351ms step_avg:113.76ms
step:269/1393 train_time:29465ms step_avg:113.77ms
step:270/1393 train_time:29580ms step_avg:113.77ms
step:271/1393 train_time:29694ms step_avg:113.77ms
step:272/1393 train_time:29809ms step_avg:113.77ms
step:273/1393 train_time:29923ms step_avg:113.78ms
step:274/1393 train_time:30037ms step_avg:113.78ms
step:275/1393 train_time:30152ms step_avg:113.78ms
step:276/1393 train_time:30266ms step_avg:113.78ms
step:277/1393 train_time:30380ms step_avg:113.78ms
step:278/1393 train_time:30494ms step_avg:113.78ms
step:279/1393 train_time:30608ms step_avg:113.79ms
step:280/1393 train_time:30723ms step_avg:113.79ms
step:281/1393 train_time:30837ms step_avg:113.79ms
step:282/1393 train_time:30951ms step_avg:113.79ms
step:283/1393 train_time:31066ms step_avg:113.79ms
step:284/1393 train_time:31180ms step_avg:113.79ms
step:285/1393 train_time:31293ms step_avg:113.79ms
step:286/1393 train_time:31407ms step_avg:113.79ms
step:287/1393 train_time:31523ms step_avg:113.80ms
step:288/1393 train_time:31637ms step_avg:113.80ms
step:289/1393 train_time:31751ms step_avg:113.80ms
step:290/1393 train_time:31865ms step_avg:113.80ms
step:291/1393 train_time:31979ms step_avg:113.80ms
step:292/1393 train_time:32092ms step_avg:113.80ms
step:293/1393 train_time:32206ms step_avg:113.80ms
step:294/1393 train_time:32321ms step_avg:113.81ms
step:295/1393 train_time:32435ms step_avg:113.81ms
step:296/1393 train_time:32549ms step_avg:113.81ms
step:297/1393 train_time:32664ms step_avg:113.81ms
step:298/1393 train_time:32778ms step_avg:113.81ms
step:299/1393 train_time:32893ms step_avg:113.82ms
step:300/1393 train_time:33007ms step_avg:113.82ms
step:301/1393 train_time:33122ms step_avg:113.82ms
step:302/1393 train_time:33236ms step_avg:113.82ms
step:303/1393 train_time:33351ms step_avg:113.83ms
step:304/1393 train_time:33465ms step_avg:113.83ms
step:305/1393 train_time:33579ms step_avg:113.83ms
step:306/1393 train_time:33693ms step_avg:113.83ms
step:307/1393 train_time:33807ms step_avg:113.83ms
step:308/1393 train_time:33922ms step_avg:113.83ms
step:309/1393 train_time:34036ms step_avg:113.83ms
step:310/1393 train_time:34150ms step_avg:113.83ms
step:311/1393 train_time:34264ms step_avg:113.83ms
step:312/1393 train_time:34381ms step_avg:113.84ms
step:313/1393 train_time:34498ms step_avg:113.86ms
step:314/1393 train_time:34616ms step_avg:113.87ms
step:315/1393 train_time:34732ms step_avg:113.88ms
step:316/1393 train_time:34849ms step_avg:113.89ms
step:317/1393 train_time:34966ms step_avg:113.90ms
step:318/1393 train_time:35082ms step_avg:113.90ms
step:319/1393 train_time:35199ms step_avg:113.91ms
step:320/1393 train_time:35317ms step_avg:113.93ms
step:321/1393 train_time:35434ms step_avg:113.93ms
step:322/1393 train_time:35550ms step_avg:113.94ms
step:323/1393 train_time:35667ms step_avg:113.95ms
step:324/1393 train_time:35783ms step_avg:113.96ms
step:325/1393 train_time:35900ms step_avg:113.97ms
step:326/1393 train_time:36018ms step_avg:113.98ms
step:327/1393 train_time:36134ms step_avg:113.99ms
step:328/1393 train_time:36251ms step_avg:114.00ms
step:329/1393 train_time:36368ms step_avg:114.01ms
step:330/1393 train_time:36486ms step_avg:114.02ms
step:331/1393 train_time:36603ms step_avg:114.03ms
step:332/1393 train_time:36720ms step_avg:114.04ms
step:333/1393 train_time:36838ms step_avg:114.05ms
step:334/1393 train_time:36955ms step_avg:114.06ms
step:335/1393 train_time:37072ms step_avg:114.07ms
step:336/1393 train_time:37188ms step_avg:114.07ms
step:337/1393 train_time:37305ms step_avg:114.08ms
step:338/1393 train_time:37422ms step_avg:114.09ms
step:339/1393 train_time:37539ms step_avg:114.10ms
step:340/1393 train_time:37655ms step_avg:114.11ms
step:341/1393 train_time:37772ms step_avg:114.12ms
step:342/1393 train_time:37889ms step_avg:114.12ms
step:343/1393 train_time:38006ms step_avg:114.13ms
step:344/1393 train_time:38123ms step_avg:114.14ms
step:345/1393 train_time:38240ms step_avg:114.15ms
step:346/1393 train_time:38357ms step_avg:114.16ms
step:347/1393 train_time:38474ms step_avg:114.17ms
step:348/1393 train_time:38591ms step_avg:114.17ms
step:349/1393 train_time:38707ms step_avg:114.18ms
step:350/1393 train_time:38824ms step_avg:114.19ms
step:351/1393 train_time:38942ms step_avg:114.20ms
step:352/1393 train_time:39059ms step_avg:114.21ms
step:353/1393 train_time:39175ms step_avg:114.21ms
step:354/1393 train_time:39292ms step_avg:114.22ms
step:355/1393 train_time:39409ms step_avg:114.23ms
step:356/1393 train_time:39526ms step_avg:114.24ms
step:357/1393 train_time:39645ms step_avg:114.25ms
step:358/1393 train_time:39762ms step_avg:114.26ms
step:359/1393 train_time:39879ms step_avg:114.27ms
step:360/1393 train_time:39996ms step_avg:114.27ms
step:361/1393 train_time:40113ms step_avg:114.28ms
step:362/1393 train_time:40229ms step_avg:114.29ms
step:363/1393 train_time:40346ms step_avg:114.29ms
step:364/1393 train_time:40463ms step_avg:114.30ms
step:365/1393 train_time:40580ms step_avg:114.31ms
step:366/1393 train_time:40698ms step_avg:114.32ms
step:367/1393 train_time:40815ms step_avg:114.33ms
step:368/1393 train_time:40932ms step_avg:114.33ms
step:369/1393 train_time:41048ms step_avg:114.34ms
step:370/1393 train_time:41165ms step_avg:114.35ms
step:371/1393 train_time:41281ms step_avg:114.35ms
step:372/1393 train_time:41399ms step_avg:114.36ms
step:373/1393 train_time:41516ms step_avg:114.37ms
step:374/1393 train_time:41632ms step_avg:114.37ms
step:375/1393 train_time:41749ms step_avg:114.38ms
step:375/1393 val_loss:3.7544 train_time:41864ms step_avg:114.70ms
step:376/1393 train_time:41887ms step_avg:114.45ms
step:377/1393 train_time:41984ms step_avg:114.40ms
step:378/1393 train_time:42109ms step_avg:114.43ms
step:379/1393 train_time:42228ms step_avg:114.44ms
step:380/1393 train_time:42345ms step_avg:114.45ms
step:381/1393 train_time:42462ms step_avg:114.45ms
step:382/1393 train_time:42579ms step_avg:114.46ms
step:383/1393 train_time:42696ms step_avg:114.47ms
step:384/1393 train_time:42813ms step_avg:114.47ms
step:385/1393 train_time:42931ms step_avg:114.48ms
step:386/1393 train_time:43048ms step_avg:114.49ms
step:387/1393 train_time:43165ms step_avg:114.50ms
step:388/1393 train_time:43282ms step_avg:114.50ms
step:389/1393 train_time:43399ms step_avg:114.51ms
step:390/1393 train_time:43517ms step_avg:114.52ms
step:391/1393 train_time:43634ms step_avg:114.52ms
step:392/1393 train_time:43751ms step_avg:114.53ms
step:393/1393 train_time:43868ms step_avg:114.54ms
step:394/1393 train_time:43984ms step_avg:114.54ms
step:395/1393 train_time:44101ms step_avg:114.55ms
step:396/1393 train_time:44218ms step_avg:114.55ms
step:397/1393 train_time:44335ms step_avg:114.56ms
step:398/1393 train_time:44452ms step_avg:114.57ms
step:399/1393 train_time:44569ms step_avg:114.57ms
step:400/1393 train_time:44686ms step_avg:114.58ms
step:401/1393 train_time:44803ms step_avg:114.59ms
step:402/1393 train_time:44920ms step_avg:114.59ms
step:403/1393 train_time:45037ms step_avg:114.60ms
step:404/1393 train_time:45154ms step_avg:114.60ms
step:405/1393 train_time:45271ms step_avg:114.61ms
step:406/1393 train_time:45388ms step_avg:114.62ms
step:407/1393 train_time:45505ms step_avg:114.62ms
step:408/1393 train_time:45622ms step_avg:114.63ms
step:409/1393 train_time:45739ms step_avg:114.63ms
step:410/1393 train_time:45856ms step_avg:114.64ms
step:411/1393 train_time:45973ms step_avg:114.65ms
step:412/1393 train_time:46089ms step_avg:114.65ms
step:413/1393 train_time:46206ms step_avg:114.66ms
step:414/1393 train_time:46322ms step_avg:114.66ms
step:415/1393 train_time:46440ms step_avg:114.67ms
step:416/1393 train_time:46558ms step_avg:114.67ms
step:417/1393 train_time:46675ms step_avg:114.68ms
step:418/1393 train_time:46793ms step_avg:114.69ms
step:419/1393 train_time:46911ms step_avg:114.70ms
step:420/1393 train_time:47029ms step_avg:114.70ms
step:421/1393 train_time:47146ms step_avg:114.71ms
step:422/1393 train_time:47263ms step_avg:114.72ms
step:423/1393 train_time:47380ms step_avg:114.72ms
step:424/1393 train_time:47498ms step_avg:114.73ms
step:425/1393 train_time:47615ms step_avg:114.74ms
step:426/1393 train_time:47734ms step_avg:114.74ms
step:427/1393 train_time:47852ms step_avg:114.75ms
step:428/1393 train_time:47970ms step_avg:114.76ms
step:429/1393 train_time:48087ms step_avg:114.77ms
step:430/1393 train_time:48204ms step_avg:114.77ms
step:431/1393 train_time:48321ms step_avg:114.78ms
step:432/1393 train_time:48439ms step_avg:114.78ms
step:433/1393 train_time:48555ms step_avg:114.79ms
step:434/1393 train_time:48673ms step_avg:114.79ms
step:435/1393 train_time:48791ms step_avg:114.80ms
step:436/1393 train_time:48908ms step_avg:114.81ms
step:437/1393 train_time:49025ms step_avg:114.81ms
step:438/1393 train_time:49143ms step_avg:114.82ms
step:439/1393 train_time:49260ms step_avg:114.83ms
step:440/1393 train_time:49378ms step_avg:114.83ms
step:441/1393 train_time:49495ms step_avg:114.84ms
step:442/1393 train_time:49613ms step_avg:114.84ms
step:443/1393 train_time:49730ms step_avg:114.85ms
step:444/1393 train_time:49847ms step_avg:114.86ms
step:445/1393 train_time:49965ms step_avg:114.86ms
step:446/1393 train_time:50082ms step_avg:114.87ms
step:447/1393 train_time:50199ms step_avg:114.87ms
step:448/1393 train_time:50317ms step_avg:114.88ms
step:449/1393 train_time:50434ms step_avg:114.88ms
step:450/1393 train_time:50552ms step_avg:114.89ms
step:451/1393 train_time:50670ms step_avg:114.90ms
step:452/1393 train_time:50787ms step_avg:114.90ms
step:453/1393 train_time:50904ms step_avg:114.91ms
step:454/1393 train_time:51021ms step_avg:114.91ms
step:455/1393 train_time:51139ms step_avg:114.92ms
step:456/1393 train_time:51256ms step_avg:114.92ms
step:457/1393 train_time:51374ms step_avg:114.93ms
step:458/1393 train_time:51491ms step_avg:114.94ms
step:459/1393 train_time:51609ms step_avg:114.94ms
step:460/1393 train_time:51727ms step_avg:114.95ms
step:461/1393 train_time:51844ms step_avg:114.95ms
step:462/1393 train_time:51962ms step_avg:114.96ms
step:463/1393 train_time:52079ms step_avg:114.96ms
step:464/1393 train_time:52196ms step_avg:114.97ms
step:465/1393 train_time:52313ms step_avg:114.97ms
step:466/1393 train_time:52431ms step_avg:114.98ms
step:467/1393 train_time:52549ms step_avg:114.99ms
step:468/1393 train_time:52666ms step_avg:114.99ms
step:469/1393 train_time:52783ms step_avg:115.00ms
step:470/1393 train_time:52901ms step_avg:115.00ms
step:471/1393 train_time:53018ms step_avg:115.01ms
step:472/1393 train_time:53135ms step_avg:115.01ms
step:473/1393 train_time:53252ms step_avg:115.01ms
step:474/1393 train_time:53369ms step_avg:115.02ms
step:475/1393 train_time:53487ms step_avg:115.02ms
step:476/1393 train_time:53604ms step_avg:115.03ms
step:477/1393 train_time:53721ms step_avg:115.03ms
step:478/1393 train_time:53839ms step_avg:115.04ms
step:479/1393 train_time:53957ms step_avg:115.05ms
step:480/1393 train_time:54074ms step_avg:115.05ms
step:481/1393 train_time:54193ms step_avg:115.06ms
step:482/1393 train_time:54311ms step_avg:115.07ms
step:483/1393 train_time:54428ms step_avg:115.07ms
step:484/1393 train_time:54546ms step_avg:115.07ms
step:485/1393 train_time:54663ms step_avg:115.08ms
step:486/1393 train_time:54780ms step_avg:115.08ms
step:487/1393 train_time:54897ms step_avg:115.09ms
step:488/1393 train_time:55014ms step_avg:115.09ms
step:489/1393 train_time:55132ms step_avg:115.10ms
step:490/1393 train_time:55249ms step_avg:115.10ms
step:491/1393 train_time:55367ms step_avg:115.11ms
step:492/1393 train_time:55484ms step_avg:115.11ms
step:493/1393 train_time:55601ms step_avg:115.12ms
step:494/1393 train_time:55719ms step_avg:115.12ms
step:495/1393 train_time:55836ms step_avg:115.13ms
step:496/1393 train_time:55954ms step_avg:115.13ms
step:497/1393 train_time:56071ms step_avg:115.14ms
step:498/1393 train_time:56189ms step_avg:115.14ms
step:499/1393 train_time:56306ms step_avg:115.15ms
step:500/1393 train_time:56424ms step_avg:115.15ms
step:500/1393 val_loss:3.6464 train_time:56540ms step_avg:115.39ms
step:501/1393 train_time:56563ms step_avg:115.20ms
step:502/1393 train_time:56660ms step_avg:115.16ms
step:503/1393 train_time:56787ms step_avg:115.19ms
step:504/1393 train_time:56908ms step_avg:115.20ms
step:505/1393 train_time:57025ms step_avg:115.20ms
step:506/1393 train_time:57142ms step_avg:115.21ms
step:507/1393 train_time:57260ms step_avg:115.21ms
step:508/1393 train_time:57377ms step_avg:115.22ms
step:509/1393 train_time:57495ms step_avg:115.22ms
step:510/1393 train_time:57612ms step_avg:115.22ms
step:511/1393 train_time:57730ms step_avg:115.23ms
step:512/1393 train_time:57847ms step_avg:115.23ms
step:513/1393 train_time:57964ms step_avg:115.24ms
step:514/1393 train_time:58082ms step_avg:115.24ms
step:515/1393 train_time:58200ms step_avg:115.25ms
step:516/1393 train_time:58318ms step_avg:115.25ms
step:517/1393 train_time:58436ms step_avg:115.26ms
step:518/1393 train_time:58555ms step_avg:115.26ms
step:519/1393 train_time:58673ms step_avg:115.27ms
step:520/1393 train_time:58793ms step_avg:115.28ms
step:521/1393 train_time:58913ms step_avg:115.29ms
step:522/1393 train_time:59032ms step_avg:115.30ms
step:523/1393 train_time:59152ms step_avg:115.31ms
step:524/1393 train_time:59272ms step_avg:115.31ms
step:525/1393 train_time:59391ms step_avg:115.32ms
step:526/1393 train_time:59511ms step_avg:115.33ms
step:527/1393 train_time:59629ms step_avg:115.34ms
step:528/1393 train_time:59750ms step_avg:115.35ms
step:529/1393 train_time:59870ms step_avg:115.36ms
step:530/1393 train_time:59991ms step_avg:115.37ms
step:531/1393 train_time:60110ms step_avg:115.37ms
step:532/1393 train_time:60230ms step_avg:115.38ms
step:533/1393 train_time:60352ms step_avg:115.40ms
step:534/1393 train_time:60471ms step_avg:115.40ms
step:535/1393 train_time:60591ms step_avg:115.41ms
step:536/1393 train_time:60710ms step_avg:115.42ms
step:537/1393 train_time:60830ms step_avg:115.43ms
step:538/1393 train_time:60951ms step_avg:115.44ms
step:539/1393 train_time:61070ms step_avg:115.44ms
step:540/1393 train_time:61189ms step_avg:115.45ms
step:541/1393 train_time:61309ms step_avg:115.46ms
step:542/1393 train_time:61429ms step_avg:115.47ms
step:543/1393 train_time:61550ms step_avg:115.48ms
step:544/1393 train_time:61670ms step_avg:115.49ms
step:545/1393 train_time:61791ms step_avg:115.50ms
step:546/1393 train_time:61910ms step_avg:115.50ms
step:547/1393 train_time:62030ms step_avg:115.51ms
step:548/1393 train_time:62149ms step_avg:115.52ms
step:549/1393 train_time:62269ms step_avg:115.53ms
step:550/1393 train_time:62387ms step_avg:115.53ms
step:551/1393 train_time:62507ms step_avg:115.54ms
step:552/1393 train_time:62628ms step_avg:115.55ms
step:553/1393 train_time:62749ms step_avg:115.56ms
step:554/1393 train_time:62869ms step_avg:115.57ms
step:555/1393 train_time:62989ms step_avg:115.58ms
step:556/1393 train_time:63109ms step_avg:115.58ms
step:557/1393 train_time:63229ms step_avg:115.59ms
step:558/1393 train_time:63350ms step_avg:115.60ms
step:559/1393 train_time:63469ms step_avg:115.61ms
step:560/1393 train_time:63588ms step_avg:115.61ms
step:561/1393 train_time:63708ms step_avg:115.62ms
step:562/1393 train_time:63827ms step_avg:115.63ms
step:563/1393 train_time:63947ms step_avg:115.64ms
step:564/1393 train_time:64067ms step_avg:115.64ms
step:565/1393 train_time:64187ms step_avg:115.65ms
step:566/1393 train_time:64306ms step_avg:115.66ms
step:567/1393 train_time:64426ms step_avg:115.67ms
step:568/1393 train_time:64546ms step_avg:115.67ms
step:569/1393 train_time:64667ms step_avg:115.68ms
step:570/1393 train_time:64785ms step_avg:115.69ms
step:571/1393 train_time:64905ms step_avg:115.70ms
step:572/1393 train_time:65024ms step_avg:115.70ms
step:573/1393 train_time:65144ms step_avg:115.71ms
step:574/1393 train_time:65264ms step_avg:115.72ms
step:575/1393 train_time:65385ms step_avg:115.73ms
step:576/1393 train_time:65504ms step_avg:115.73ms
step:577/1393 train_time:65623ms step_avg:115.74ms
step:578/1393 train_time:65744ms step_avg:115.75ms
step:579/1393 train_time:65864ms step_avg:115.75ms
step:580/1393 train_time:65984ms step_avg:115.76ms
step:581/1393 train_time:66104ms step_avg:115.77ms
step:582/1393 train_time:66223ms step_avg:115.77ms
step:583/1393 train_time:66344ms step_avg:115.78ms
step:584/1393 train_time:66463ms step_avg:115.79ms
step:585/1393 train_time:66582ms step_avg:115.80ms
step:586/1393 train_time:66702ms step_avg:115.80ms
step:587/1393 train_time:66821ms step_avg:115.81ms
step:588/1393 train_time:66940ms step_avg:115.81ms
step:589/1393 train_time:67060ms step_avg:115.82ms
step:590/1393 train_time:67179ms step_avg:115.83ms
step:591/1393 train_time:67298ms step_avg:115.83ms
step:592/1393 train_time:67418ms step_avg:115.84ms
step:593/1393 train_time:67537ms step_avg:115.84ms
step:594/1393 train_time:67656ms step_avg:115.85ms
step:595/1393 train_time:67776ms step_avg:115.86ms
step:596/1393 train_time:67896ms step_avg:115.86ms
step:597/1393 train_time:68015ms step_avg:115.87ms
step:598/1393 train_time:68136ms step_avg:115.88ms
step:599/1393 train_time:68255ms step_avg:115.88ms
step:600/1393 train_time:68374ms step_avg:115.89ms
step:601/1393 train_time:68494ms step_avg:115.90ms
step:602/1393 train_time:68614ms step_avg:115.90ms
step:603/1393 train_time:68733ms step_avg:115.91ms
step:604/1393 train_time:68853ms step_avg:115.91ms
step:605/1393 train_time:68972ms step_avg:115.92ms
step:606/1393 train_time:69091ms step_avg:115.93ms
step:607/1393 train_time:69211ms step_avg:115.93ms
step:608/1393 train_time:69330ms step_avg:115.94ms
step:609/1393 train_time:69450ms step_avg:115.94ms
step:610/1393 train_time:69570ms step_avg:115.95ms
step:611/1393 train_time:69690ms step_avg:115.96ms
step:612/1393 train_time:69810ms step_avg:115.96ms
step:613/1393 train_time:69930ms step_avg:115.97ms
step:614/1393 train_time:70052ms step_avg:115.98ms
step:615/1393 train_time:70172ms step_avg:115.99ms
step:616/1393 train_time:70291ms step_avg:115.99ms
step:617/1393 train_time:70410ms step_avg:116.00ms
step:618/1393 train_time:70530ms step_avg:116.00ms
step:619/1393 train_time:70650ms step_avg:116.01ms
step:620/1393 train_time:70771ms step_avg:116.02ms
step:621/1393 train_time:70890ms step_avg:116.02ms
step:622/1393 train_time:71010ms step_avg:116.03ms
step:623/1393 train_time:71130ms step_avg:116.04ms
step:624/1393 train_time:71250ms step_avg:116.04ms
step:625/1393 train_time:71371ms step_avg:116.05ms
step:625/1393 val_loss:3.5628 train_time:71490ms step_avg:116.24ms
step:626/1393 train_time:71513ms step_avg:116.09ms
step:627/1393 train_time:71612ms step_avg:116.07ms
step:628/1393 train_time:71738ms step_avg:116.08ms
step:629/1393 train_time:71859ms step_avg:116.09ms
step:630/1393 train_time:71979ms step_avg:116.10ms
step:631/1393 train_time:72099ms step_avg:116.10ms
step:632/1393 train_time:72219ms step_avg:116.11ms
step:633/1393 train_time:72339ms step_avg:116.11ms
step:634/1393 train_time:72459ms step_avg:116.12ms
step:635/1393 train_time:72579ms step_avg:116.13ms
step:636/1393 train_time:72700ms step_avg:116.13ms
step:637/1393 train_time:72820ms step_avg:116.14ms
step:638/1393 train_time:72940ms step_avg:116.15ms
step:639/1393 train_time:73060ms step_avg:116.15ms
step:640/1393 train_time:73180ms step_avg:116.16ms
step:641/1393 train_time:73300ms step_avg:116.17ms
step:642/1393 train_time:73421ms step_avg:116.17ms
step:643/1393 train_time:73540ms step_avg:116.18ms
step:644/1393 train_time:73659ms step_avg:116.18ms
step:645/1393 train_time:73779ms step_avg:116.19ms
step:646/1393 train_time:73901ms step_avg:116.20ms
step:647/1393 train_time:74020ms step_avg:116.20ms
step:648/1393 train_time:74140ms step_avg:116.21ms
step:649/1393 train_time:74260ms step_avg:116.21ms
step:650/1393 train_time:74381ms step_avg:116.22ms
step:651/1393 train_time:74502ms step_avg:116.23ms
step:652/1393 train_time:74621ms step_avg:116.23ms
step:653/1393 train_time:74740ms step_avg:116.24ms
step:654/1393 train_time:74861ms step_avg:116.24ms
step:655/1393 train_time:74980ms step_avg:116.25ms
step:656/1393 train_time:75100ms step_avg:116.25ms
step:657/1393 train_time:75220ms step_avg:116.26ms
step:658/1393 train_time:75340ms step_avg:116.27ms
step:659/1393 train_time:75461ms step_avg:116.27ms
step:660/1393 train_time:75580ms step_avg:116.28ms
step:661/1393 train_time:75701ms step_avg:116.28ms
step:662/1393 train_time:75820ms step_avg:116.29ms
step:663/1393 train_time:75940ms step_avg:116.29ms
step:664/1393 train_time:76060ms step_avg:116.30ms
step:665/1393 train_time:76179ms step_avg:116.30ms
step:666/1393 train_time:76299ms step_avg:116.31ms
step:667/1393 train_time:76419ms step_avg:116.32ms
step:668/1393 train_time:76540ms step_avg:116.32ms
step:669/1393 train_time:76659ms step_avg:116.33ms
step:670/1393 train_time:76779ms step_avg:116.33ms
step:671/1393 train_time:76900ms step_avg:116.34ms
step:672/1393 train_time:77020ms step_avg:116.34ms
step:673/1393 train_time:77139ms step_avg:116.35ms
step:674/1393 train_time:77260ms step_avg:116.35ms
step:675/1393 train_time:77380ms step_avg:116.36ms
step:676/1393 train_time:77499ms step_avg:116.37ms
step:677/1393 train_time:77620ms step_avg:116.37ms
step:678/1393 train_time:77740ms step_avg:116.38ms
step:679/1393 train_time:77860ms step_avg:116.38ms
step:680/1393 train_time:77981ms step_avg:116.39ms
step:681/1393 train_time:78101ms step_avg:116.39ms
step:682/1393 train_time:78221ms step_avg:116.40ms
step:683/1393 train_time:78341ms step_avg:116.41ms
step:684/1393 train_time:78462ms step_avg:116.41ms
step:685/1393 train_time:78581ms step_avg:116.42ms
step:686/1393 train_time:78701ms step_avg:116.42ms
step:687/1393 train_time:78821ms step_avg:116.43ms
step:688/1393 train_time:78942ms step_avg:116.43ms
step:689/1393 train_time:79063ms step_avg:116.44ms
step:690/1393 train_time:79182ms step_avg:116.44ms
step:691/1393 train_time:79302ms step_avg:116.45ms
step:692/1393 train_time:79421ms step_avg:116.45ms
step:693/1393 train_time:79541ms step_avg:116.46ms
step:694/1393 train_time:79661ms step_avg:116.46ms
step:695/1393 train_time:79782ms step_avg:116.47ms
step:696/1393 train_time:79902ms step_avg:116.48ms
step:697/1393 train_time:80021ms step_avg:116.48ms
step:698/1393 train_time:80141ms step_avg:116.48ms
step:699/1393 train_time:80263ms step_avg:116.49ms
step:700/1393 train_time:80382ms step_avg:116.50ms
step:701/1393 train_time:80502ms step_avg:116.50ms
step:702/1393 train_time:80622ms step_avg:116.51ms
step:703/1393 train_time:80742ms step_avg:116.51ms
step:704/1393 train_time:80863ms step_avg:116.52ms
step:705/1393 train_time:80984ms step_avg:116.52ms
step:706/1393 train_time:81104ms step_avg:116.53ms
step:707/1393 train_time:81223ms step_avg:116.53ms
step:708/1393 train_time:81344ms step_avg:116.54ms
step:709/1393 train_time:81465ms step_avg:116.55ms
step:710/1393 train_time:81586ms step_avg:116.55ms
step:711/1393 train_time:81706ms step_avg:116.56ms
step:712/1393 train_time:81826ms step_avg:116.56ms
step:713/1393 train_time:81946ms step_avg:116.57ms
step:714/1393 train_time:82066ms step_avg:116.57ms
step:715/1393 train_time:82186ms step_avg:116.58ms
step:716/1393 train_time:82306ms step_avg:116.58ms
step:717/1393 train_time:82425ms step_avg:116.58ms
step:718/1393 train_time:82545ms step_avg:116.59ms
step:719/1393 train_time:82665ms step_avg:116.59ms
step:720/1393 train_time:82785ms step_avg:116.60ms
step:721/1393 train_time:82905ms step_avg:116.60ms
step:722/1393 train_time:83024ms step_avg:116.61ms
step:723/1393 train_time:83145ms step_avg:116.61ms
step:724/1393 train_time:83266ms step_avg:116.62ms
step:725/1393 train_time:83389ms step_avg:116.63ms
step:726/1393 train_time:83510ms step_avg:116.63ms
step:727/1393 train_time:83631ms step_avg:116.64ms
step:728/1393 train_time:83753ms step_avg:116.65ms
step:729/1393 train_time:83874ms step_avg:116.65ms
step:730/1393 train_time:83998ms step_avg:116.66ms
step:731/1393 train_time:84119ms step_avg:116.67ms
step:732/1393 train_time:84241ms step_avg:116.68ms
step:733/1393 train_time:84362ms step_avg:116.68ms
step:734/1393 train_time:84484ms step_avg:116.69ms
step:735/1393 train_time:84604ms step_avg:116.70ms
step:736/1393 train_time:84727ms step_avg:116.70ms
step:737/1393 train_time:84848ms step_avg:116.71ms
step:738/1393 train_time:84970ms step_avg:116.72ms
step:739/1393 train_time:85091ms step_avg:116.72ms
step:740/1393 train_time:85214ms step_avg:116.73ms
step:741/1393 train_time:85335ms step_avg:116.74ms
step:742/1393 train_time:85456ms step_avg:116.74ms
step:743/1393 train_time:85578ms step_avg:116.75ms
step:744/1393 train_time:85700ms step_avg:116.76ms
step:745/1393 train_time:85822ms step_avg:116.76ms
step:746/1393 train_time:85943ms step_avg:116.77ms
step:747/1393 train_time:86066ms step_avg:116.78ms
step:748/1393 train_time:86187ms step_avg:116.78ms
step:749/1393 train_time:86309ms step_avg:116.79ms
step:750/1393 train_time:86430ms step_avg:116.80ms
step:750/1393 val_loss:3.5140 train_time:86550ms step_avg:116.96ms
step:751/1393 train_time:86572ms step_avg:116.83ms
step:752/1393 train_time:86677ms step_avg:116.81ms
step:753/1393 train_time:86805ms step_avg:116.83ms
step:754/1393 train_time:86930ms step_avg:116.84ms
step:755/1393 train_time:87051ms step_avg:116.85ms
step:756/1393 train_time:87172ms step_avg:116.85ms
step:757/1393 train_time:87294ms step_avg:116.86ms
step:758/1393 train_time:87416ms step_avg:116.87ms
step:759/1393 train_time:87537ms step_avg:116.87ms
step:760/1393 train_time:87658ms step_avg:116.88ms
step:761/1393 train_time:87781ms step_avg:116.89ms
step:762/1393 train_time:87902ms step_avg:116.89ms
step:763/1393 train_time:88024ms step_avg:116.90ms
step:764/1393 train_time:88146ms step_avg:116.90ms
step:765/1393 train_time:88270ms step_avg:116.91ms
step:766/1393 train_time:88392ms step_avg:116.92ms
step:767/1393 train_time:88512ms step_avg:116.93ms
step:768/1393 train_time:88634ms step_avg:116.93ms
step:769/1393 train_time:88756ms step_avg:116.94ms
step:770/1393 train_time:88877ms step_avg:116.94ms
step:771/1393 train_time:88998ms step_avg:116.95ms
step:772/1393 train_time:89120ms step_avg:116.95ms
step:773/1393 train_time:89241ms step_avg:116.96ms
step:774/1393 train_time:89363ms step_avg:116.97ms
step:775/1393 train_time:89485ms step_avg:116.97ms
step:776/1393 train_time:89607ms step_avg:116.98ms
step:777/1393 train_time:89732ms step_avg:116.99ms
step:778/1393 train_time:89853ms step_avg:117.00ms
step:779/1393 train_time:89974ms step_avg:117.00ms
step:780/1393 train_time:90096ms step_avg:117.01ms
step:781/1393 train_time:90218ms step_avg:117.01ms
step:782/1393 train_time:90339ms step_avg:117.02ms
step:783/1393 train_time:90461ms step_avg:117.03ms
step:784/1393 train_time:90584ms step_avg:117.03ms
step:785/1393 train_time:90706ms step_avg:117.04ms
step:786/1393 train_time:90829ms step_avg:117.05ms
step:787/1393 train_time:90952ms step_avg:117.06ms
step:788/1393 train_time:91073ms step_avg:117.06ms
step:789/1393 train_time:91195ms step_avg:117.07ms
step:790/1393 train_time:91317ms step_avg:117.07ms
step:791/1393 train_time:91438ms step_avg:117.08ms
step:792/1393 train_time:91559ms step_avg:117.08ms
step:793/1393 train_time:91681ms step_avg:117.09ms
step:794/1393 train_time:91804ms step_avg:117.10ms
step:795/1393 train_time:91926ms step_avg:117.10ms
step:796/1393 train_time:92048ms step_avg:117.11ms
step:797/1393 train_time:92170ms step_avg:117.12ms
step:798/1393 train_time:92292ms step_avg:117.12ms
step:799/1393 train_time:92413ms step_avg:117.13ms
step:800/1393 train_time:92534ms step_avg:117.13ms
step:801/1393 train_time:92657ms step_avg:117.14ms
step:802/1393 train_time:92778ms step_avg:117.14ms
step:803/1393 train_time:92899ms step_avg:117.15ms
step:804/1393 train_time:93022ms step_avg:117.16ms
step:805/1393 train_time:93144ms step_avg:117.16ms
step:806/1393 train_time:93265ms step_avg:117.17ms
step:807/1393 train_time:93386ms step_avg:117.17ms
step:808/1393 train_time:93508ms step_avg:117.18ms
step:809/1393 train_time:93630ms step_avg:117.18ms
step:810/1393 train_time:93752ms step_avg:117.19ms
step:811/1393 train_time:93873ms step_avg:117.19ms
step:812/1393 train_time:93994ms step_avg:117.20ms
step:813/1393 train_time:94115ms step_avg:117.20ms
step:814/1393 train_time:94237ms step_avg:117.21ms
step:815/1393 train_time:94359ms step_avg:117.22ms
step:816/1393 train_time:94481ms step_avg:117.22ms
step:817/1393 train_time:94604ms step_avg:117.23ms
step:818/1393 train_time:94725ms step_avg:117.23ms
step:819/1393 train_time:94848ms step_avg:117.24ms
step:820/1393 train_time:94971ms step_avg:117.25ms
step:821/1393 train_time:95092ms step_avg:117.25ms
step:822/1393 train_time:95214ms step_avg:117.26ms
step:823/1393 train_time:95336ms step_avg:117.26ms
step:824/1393 train_time:95457ms step_avg:117.27ms
step:825/1393 train_time:95578ms step_avg:117.27ms
step:826/1393 train_time:95699ms step_avg:117.28ms
step:827/1393 train_time:95822ms step_avg:117.28ms
step:828/1393 train_time:95943ms step_avg:117.29ms
step:829/1393 train_time:96065ms step_avg:117.30ms
step:830/1393 train_time:96189ms step_avg:117.30ms
step:831/1393 train_time:96311ms step_avg:117.31ms
step:832/1393 train_time:96434ms step_avg:117.32ms
step:833/1393 train_time:96555ms step_avg:117.32ms
step:834/1393 train_time:96678ms step_avg:117.33ms
step:835/1393 train_time:96800ms step_avg:117.33ms
step:836/1393 train_time:96922ms step_avg:117.34ms
step:837/1393 train_time:97044ms step_avg:117.34ms
step:838/1393 train_time:97165ms step_avg:117.35ms
step:839/1393 train_time:97287ms step_avg:117.36ms
step:840/1393 train_time:97411ms step_avg:117.36ms
step:841/1393 train_time:97533ms step_avg:117.37ms
step:842/1393 train_time:97654ms step_avg:117.37ms
step:843/1393 train_time:97775ms step_avg:117.38ms
step:844/1393 train_time:97898ms step_avg:117.38ms
step:845/1393 train_time:98020ms step_avg:117.39ms
step:846/1393 train_time:98142ms step_avg:117.40ms
step:847/1393 train_time:98264ms step_avg:117.40ms
step:848/1393 train_time:98385ms step_avg:117.40ms
step:849/1393 train_time:98507ms step_avg:117.41ms
step:850/1393 train_time:98630ms step_avg:117.42ms
step:851/1393 train_time:98752ms step_avg:117.42ms
step:852/1393 train_time:98874ms step_avg:117.43ms
step:853/1393 train_time:98995ms step_avg:117.43ms
step:854/1393 train_time:99119ms step_avg:117.44ms
step:855/1393 train_time:99241ms step_avg:117.45ms
step:856/1393 train_time:99362ms step_avg:117.45ms
step:857/1393 train_time:99485ms step_avg:117.46ms
step:858/1393 train_time:99607ms step_avg:117.46ms
step:859/1393 train_time:99730ms step_avg:117.47ms
step:860/1393 train_time:99852ms step_avg:117.47ms
step:861/1393 train_time:99974ms step_avg:117.48ms
step:862/1393 train_time:100097ms step_avg:117.49ms
step:863/1393 train_time:100219ms step_avg:117.49ms
step:864/1393 train_time:100341ms step_avg:117.50ms
step:865/1393 train_time:100463ms step_avg:117.50ms
step:866/1393 train_time:100585ms step_avg:117.51ms
step:867/1393 train_time:100707ms step_avg:117.51ms
step:868/1393 train_time:100830ms step_avg:117.52ms
step:869/1393 train_time:100953ms step_avg:117.52ms
step:870/1393 train_time:101075ms step_avg:117.53ms
step:871/1393 train_time:101197ms step_avg:117.53ms
step:872/1393 train_time:101320ms step_avg:117.54ms
step:873/1393 train_time:101443ms step_avg:117.55ms
step:874/1393 train_time:101564ms step_avg:117.55ms
step:875/1393 train_time:101686ms step_avg:117.56ms
step:875/1393 val_loss:3.4652 train_time:101807ms step_avg:117.70ms
step:876/1393 train_time:101830ms step_avg:117.59ms
step:877/1393 train_time:101933ms step_avg:117.57ms
step:878/1393 train_time:102062ms step_avg:117.58ms
step:879/1393 train_time:102184ms step_avg:117.59ms
step:880/1393 train_time:102308ms step_avg:117.60ms
step:881/1393 train_time:102429ms step_avg:117.60ms
step:882/1393 train_time:102550ms step_avg:117.60ms
step:883/1393 train_time:102671ms step_avg:117.61ms
step:884/1393 train_time:102793ms step_avg:117.61ms
step:885/1393 train_time:102917ms step_avg:117.62ms
step:886/1393 train_time:103039ms step_avg:117.62ms
step:887/1393 train_time:103160ms step_avg:117.63ms
step:888/1393 train_time:103282ms step_avg:117.63ms
step:889/1393 train_time:103404ms step_avg:117.64ms
step:890/1393 train_time:103525ms step_avg:117.64ms
step:891/1393 train_time:103647ms step_avg:117.65ms
step:892/1393 train_time:103768ms step_avg:117.65ms
step:893/1393 train_time:103893ms step_avg:117.66ms
step:894/1393 train_time:104016ms step_avg:117.67ms
step:895/1393 train_time:104137ms step_avg:117.67ms
step:896/1393 train_time:104259ms step_avg:117.67ms
step:897/1393 train_time:104382ms step_avg:117.68ms
step:898/1393 train_time:104505ms step_avg:117.69ms
step:899/1393 train_time:104627ms step_avg:117.69ms
step:900/1393 train_time:104748ms step_avg:117.69ms
step:901/1393 train_time:104870ms step_avg:117.70ms
step:902/1393 train_time:104993ms step_avg:117.71ms
step:903/1393 train_time:105115ms step_avg:117.71ms
step:904/1393 train_time:105237ms step_avg:117.71ms
step:905/1393 train_time:105359ms step_avg:117.72ms
step:906/1393 train_time:105483ms step_avg:117.73ms
step:907/1393 train_time:105605ms step_avg:117.73ms
step:908/1393 train_time:105726ms step_avg:117.73ms
step:909/1393 train_time:105848ms step_avg:117.74ms
step:910/1393 train_time:105969ms step_avg:117.74ms
step:911/1393 train_time:106090ms step_avg:117.75ms
step:912/1393 train_time:106214ms step_avg:117.75ms
step:913/1393 train_time:106337ms step_avg:117.76ms
step:914/1393 train_time:106460ms step_avg:117.77ms
step:915/1393 train_time:106582ms step_avg:117.77ms
step:916/1393 train_time:106703ms step_avg:117.77ms
step:917/1393 train_time:106825ms step_avg:117.78ms
step:918/1393 train_time:106946ms step_avg:117.78ms
step:919/1393 train_time:107069ms step_avg:117.79ms
step:920/1393 train_time:107191ms step_avg:117.79ms
step:921/1393 train_time:107314ms step_avg:117.80ms
step:922/1393 train_time:107437ms step_avg:117.80ms
step:923/1393 train_time:107560ms step_avg:117.81ms
step:924/1393 train_time:107681ms step_avg:117.81ms
step:925/1393 train_time:107803ms step_avg:117.82ms
step:926/1393 train_time:107927ms step_avg:117.82ms
step:927/1393 train_time:108049ms step_avg:117.83ms
step:928/1393 train_time:108171ms step_avg:117.83ms
step:929/1393 train_time:108293ms step_avg:117.84ms
step:930/1393 train_time:108416ms step_avg:117.84ms
step:931/1393 train_time:108541ms step_avg:117.85ms
step:932/1393 train_time:108664ms step_avg:117.86ms
step:933/1393 train_time:108789ms step_avg:117.86ms
step:934/1393 train_time:108912ms step_avg:117.87ms
step:935/1393 train_time:109037ms step_avg:117.88ms
step:936/1393 train_time:109159ms step_avg:117.88ms
step:937/1393 train_time:109283ms step_avg:117.89ms
step:938/1393 train_time:109407ms step_avg:117.90ms
step:939/1393 train_time:109529ms step_avg:117.90ms
step:940/1393 train_time:109653ms step_avg:117.91ms
step:941/1393 train_time:109776ms step_avg:117.91ms
step:942/1393 train_time:109898ms step_avg:117.92ms
step:943/1393 train_time:110024ms step_avg:117.92ms
step:944/1393 train_time:110148ms step_avg:117.93ms
step:945/1393 train_time:110272ms step_avg:117.94ms
step:946/1393 train_time:110396ms step_avg:117.94ms
step:947/1393 train_time:110520ms step_avg:117.95ms
step:948/1393 train_time:110643ms step_avg:117.96ms
step:949/1393 train_time:110767ms step_avg:117.96ms
step:950/1393 train_time:110894ms step_avg:117.97ms
step:951/1393 train_time:111017ms step_avg:117.98ms
step:952/1393 train_time:111140ms step_avg:117.98ms
step:953/1393 train_time:111264ms step_avg:117.99ms
step:954/1393 train_time:111386ms step_avg:117.99ms
step:955/1393 train_time:111511ms step_avg:118.00ms
step:956/1393 train_time:111634ms step_avg:118.01ms
step:957/1393 train_time:111757ms step_avg:118.01ms
step:958/1393 train_time:111881ms step_avg:118.02ms
step:959/1393 train_time:112006ms step_avg:118.02ms
step:960/1393 train_time:112129ms step_avg:118.03ms
step:961/1393 train_time:112252ms step_avg:118.04ms
step:962/1393 train_time:112376ms step_avg:118.04ms
step:963/1393 train_time:112498ms step_avg:118.05ms
step:964/1393 train_time:112621ms step_avg:118.05ms
step:965/1393 train_time:112745ms step_avg:118.06ms
step:966/1393 train_time:112869ms step_avg:118.06ms
step:967/1393 train_time:112991ms step_avg:118.07ms
step:968/1393 train_time:113115ms step_avg:118.07ms
step:969/1393 train_time:113237ms step_avg:118.08ms
step:970/1393 train_time:113360ms step_avg:118.08ms
step:971/1393 train_time:113485ms step_avg:118.09ms
step:972/1393 train_time:113611ms step_avg:118.10ms
step:973/1393 train_time:113735ms step_avg:118.10ms
step:974/1393 train_time:113858ms step_avg:118.11ms
step:975/1393 train_time:113981ms step_avg:118.12ms
step:976/1393 train_time:114107ms step_avg:118.12ms
step:977/1393 train_time:114232ms step_avg:118.13ms
step:978/1393 train_time:114357ms step_avg:118.14ms
step:979/1393 train_time:114480ms step_avg:118.14ms
step:980/1393 train_time:114604ms step_avg:118.15ms
step:981/1393 train_time:114726ms step_avg:118.15ms
step:982/1393 train_time:114849ms step_avg:118.16ms
step:983/1393 train_time:114972ms step_avg:118.16ms
step:984/1393 train_time:115095ms step_avg:118.17ms
step:985/1393 train_time:115218ms step_avg:118.17ms
step:986/1393 train_time:115341ms step_avg:118.18ms
step:987/1393 train_time:115464ms step_avg:118.18ms
step:988/1393 train_time:115588ms step_avg:118.19ms
step:989/1393 train_time:115711ms step_avg:118.19ms
step:990/1393 train_time:115835ms step_avg:118.20ms
step:991/1393 train_time:115958ms step_avg:118.20ms
step:992/1393 train_time:116082ms step_avg:118.21ms
step:993/1393 train_time:116206ms step_avg:118.22ms
step:994/1393 train_time:116334ms step_avg:118.23ms
step:995/1393 train_time:116457ms step_avg:118.23ms
step:996/1393 train_time:116580ms step_avg:118.24ms
step:997/1393 train_time:116703ms step_avg:118.24ms
step:998/1393 train_time:116828ms step_avg:118.25ms
step:999/1393 train_time:116951ms step_avg:118.25ms
step:1000/1393 train_time:117074ms step_avg:118.26ms
step:1000/1393 val_loss:3.4084 train_time:117196ms step_avg:118.38ms
step:1001/1393 train_time:117219ms step_avg:118.28ms
step:1002/1393 train_time:117326ms step_avg:118.27ms
step:1003/1393 train_time:117455ms step_avg:118.28ms
step:1004/1393 train_time:117580ms step_avg:118.29ms
step:1005/1393 train_time:117703ms step_avg:118.29ms
step:1006/1393 train_time:117827ms step_avg:118.30ms
step:1007/1393 train_time:117950ms step_avg:118.30ms
step:1008/1393 train_time:118073ms step_avg:118.31ms
step:1009/1393 train_time:118197ms step_avg:118.32ms
step:1010/1393 train_time:118321ms step_avg:118.32ms
step:1011/1393 train_time:118445ms step_avg:118.33ms
step:1012/1393 train_time:118568ms step_avg:118.33ms
step:1013/1393 train_time:118692ms step_avg:118.34ms
step:1014/1393 train_time:118816ms step_avg:118.34ms
step:1015/1393 train_time:118940ms step_avg:118.35ms
step:1016/1393 train_time:119063ms step_avg:118.35ms
step:1017/1393 train_time:119186ms step_avg:118.36ms
step:1018/1393 train_time:119309ms step_avg:118.36ms
step:1019/1393 train_time:119433ms step_avg:118.37ms
step:1020/1393 train_time:119557ms step_avg:118.37ms
step:1021/1393 train_time:119680ms step_avg:118.38ms
step:1022/1393 train_time:119806ms step_avg:118.38ms
step:1023/1393 train_time:119930ms step_avg:118.39ms
step:1024/1393 train_time:120053ms step_avg:118.39ms
step:1025/1393 train_time:120176ms step_avg:118.40ms
step:1026/1393 train_time:120299ms step_avg:118.40ms
step:1027/1393 train_time:120423ms step_avg:118.41ms
step:1028/1393 train_time:120546ms step_avg:118.41ms
step:1029/1393 train_time:120671ms step_avg:118.42ms
step:1030/1393 train_time:120794ms step_avg:118.43ms
step:1031/1393 train_time:120919ms step_avg:118.43ms
step:1032/1393 train_time:121042ms step_avg:118.44ms
step:1033/1393 train_time:121165ms step_avg:118.44ms
step:1034/1393 train_time:121289ms step_avg:118.45ms
step:1035/1393 train_time:121413ms step_avg:118.45ms
step:1036/1393 train_time:121537ms step_avg:118.46ms
step:1037/1393 train_time:121660ms step_avg:118.46ms
step:1038/1393 train_time:121783ms step_avg:118.47ms
step:1039/1393 train_time:121906ms step_avg:118.47ms
step:1040/1393 train_time:122029ms step_avg:118.48ms
step:1041/1393 train_time:122152ms step_avg:118.48ms
step:1042/1393 train_time:122276ms step_avg:118.48ms
step:1043/1393 train_time:122401ms step_avg:118.49ms
step:1044/1393 train_time:122527ms step_avg:118.50ms
step:1045/1393 train_time:122651ms step_avg:118.50ms
step:1046/1393 train_time:122774ms step_avg:118.51ms
step:1047/1393 train_time:122897ms step_avg:118.51ms
step:1048/1393 train_time:123021ms step_avg:118.52ms
step:1049/1393 train_time:123144ms step_avg:118.52ms
step:1050/1393 train_time:123268ms step_avg:118.53ms
step:1051/1393 train_time:123393ms step_avg:118.53ms
step:1052/1393 train_time:123517ms step_avg:118.54ms
step:1053/1393 train_time:123641ms step_avg:118.54ms
step:1054/1393 train_time:123764ms step_avg:118.55ms
step:1055/1393 train_time:123889ms step_avg:118.55ms
step:1056/1393 train_time:124012ms step_avg:118.56ms
step:1057/1393 train_time:124136ms step_avg:118.56ms
step:1058/1393 train_time:124259ms step_avg:118.57ms
step:1059/1393 train_time:124384ms step_avg:118.57ms
step:1060/1393 train_time:124508ms step_avg:118.58ms
step:1061/1393 train_time:124630ms step_avg:118.58ms
step:1062/1393 train_time:124754ms step_avg:118.59ms
step:1063/1393 train_time:124880ms step_avg:118.59ms
step:1064/1393 train_time:125003ms step_avg:118.60ms
step:1065/1393 train_time:125128ms step_avg:118.61ms
step:1066/1393 train_time:125251ms step_avg:118.61ms
step:1067/1393 train_time:125375ms step_avg:118.61ms
step:1068/1393 train_time:125501ms step_avg:118.62ms
step:1069/1393 train_time:125624ms step_avg:118.63ms
step:1070/1393 train_time:125748ms step_avg:118.63ms
step:1071/1393 train_time:125872ms step_avg:118.64ms
step:1072/1393 train_time:125996ms step_avg:118.64ms
step:1073/1393 train_time:126121ms step_avg:118.65ms
step:1074/1393 train_time:126246ms step_avg:118.65ms
step:1075/1393 train_time:126369ms step_avg:118.66ms
step:1076/1393 train_time:126492ms step_avg:118.66ms
step:1077/1393 train_time:126616ms step_avg:118.67ms
step:1078/1393 train_time:126740ms step_avg:118.67ms
step:1079/1393 train_time:126863ms step_avg:118.67ms
step:1080/1393 train_time:126989ms step_avg:118.68ms
step:1081/1393 train_time:127114ms step_avg:118.69ms
step:1082/1393 train_time:127239ms step_avg:118.69ms
step:1083/1393 train_time:127362ms step_avg:118.70ms
step:1084/1393 train_time:127485ms step_avg:118.70ms
step:1085/1393 train_time:127609ms step_avg:118.71ms
step:1086/1393 train_time:127735ms step_avg:118.71ms
step:1087/1393 train_time:127859ms step_avg:118.72ms
step:1088/1393 train_time:127981ms step_avg:118.72ms
step:1089/1393 train_time:128108ms step_avg:118.73ms
step:1090/1393 train_time:128234ms step_avg:118.74ms
step:1091/1393 train_time:128359ms step_avg:118.74ms
step:1092/1393 train_time:128486ms step_avg:118.75ms
step:1093/1393 train_time:128609ms step_avg:118.75ms
step:1094/1393 train_time:128734ms step_avg:118.76ms
step:1095/1393 train_time:128858ms step_avg:118.76ms
step:1096/1393 train_time:128981ms step_avg:118.77ms
step:1097/1393 train_time:129104ms step_avg:118.77ms
step:1098/1393 train_time:129228ms step_avg:118.78ms
step:1099/1393 train_time:129352ms step_avg:118.78ms
step:1100/1393 train_time:129474ms step_avg:118.78ms
step:1101/1393 train_time:129600ms step_avg:118.79ms
step:1102/1393 train_time:129727ms step_avg:118.80ms
step:1103/1393 train_time:129850ms step_avg:118.80ms
step:1104/1393 train_time:129973ms step_avg:118.81ms
step:1105/1393 train_time:130096ms step_avg:118.81ms
step:1106/1393 train_time:130219ms step_avg:118.81ms
step:1107/1393 train_time:130344ms step_avg:118.82ms
step:1108/1393 train_time:130469ms step_avg:118.82ms
step:1109/1393 train_time:130593ms step_avg:118.83ms
step:1110/1393 train_time:130718ms step_avg:118.83ms
step:1111/1393 train_time:130842ms step_avg:118.84ms
step:1112/1393 train_time:130967ms step_avg:118.84ms
step:1113/1393 train_time:131092ms step_avg:118.85ms
step:1114/1393 train_time:131215ms step_avg:118.85ms
step:1115/1393 train_time:131339ms step_avg:118.86ms
step:1116/1393 train_time:131463ms step_avg:118.86ms
step:1117/1393 train_time:131588ms step_avg:118.87ms
step:1118/1393 train_time:131712ms step_avg:118.87ms
step:1119/1393 train_time:131836ms step_avg:118.88ms
step:1120/1393 train_time:131960ms step_avg:118.88ms
step:1121/1393 train_time:132084ms step_avg:118.89ms
step:1122/1393 train_time:132208ms step_avg:118.89ms
step:1123/1393 train_time:132333ms step_avg:118.90ms
step:1124/1393 train_time:132455ms step_avg:118.90ms
step:1125/1393 train_time:132579ms step_avg:118.91ms
step:1125/1393 val_loss:3.3588 train_time:132701ms step_avg:119.01ms
step:1126/1393 train_time:132724ms step_avg:118.93ms
step:1127/1393 train_time:132829ms step_avg:118.92ms
step:1128/1393 train_time:132959ms step_avg:118.93ms
step:1129/1393 train_time:133084ms step_avg:118.93ms
step:1130/1393 train_time:133209ms step_avg:118.94ms
step:1131/1393 train_time:133333ms step_avg:118.94ms
step:1132/1393 train_time:133457ms step_avg:118.95ms
step:1133/1393 train_time:133580ms step_avg:118.95ms
step:1134/1393 train_time:133704ms step_avg:118.95ms
step:1135/1393 train_time:133829ms step_avg:118.96ms
step:1136/1393 train_time:133954ms step_avg:118.96ms
step:1137/1393 train_time:134078ms step_avg:118.97ms
step:1138/1393 train_time:134205ms step_avg:118.98ms
step:1139/1393 train_time:134332ms step_avg:118.98ms
step:1140/1393 train_time:134457ms step_avg:118.99ms
step:1141/1393 train_time:134582ms step_avg:118.99ms
step:1142/1393 train_time:134707ms step_avg:119.00ms
step:1143/1393 train_time:134833ms step_avg:119.00ms
step:1144/1393 train_time:134958ms step_avg:119.01ms
step:1145/1393 train_time:135084ms step_avg:119.02ms
step:1146/1393 train_time:135208ms step_avg:119.02ms
step:1147/1393 train_time:135334ms step_avg:119.03ms
step:1148/1393 train_time:135459ms step_avg:119.03ms
step:1149/1393 train_time:135585ms step_avg:119.04ms
step:1150/1393 train_time:135709ms step_avg:119.04ms
step:1151/1393 train_time:135835ms step_avg:119.05ms
step:1152/1393 train_time:135959ms step_avg:119.05ms
step:1153/1393 train_time:136084ms step_avg:119.06ms
step:1154/1393 train_time:136209ms step_avg:119.06ms
step:1155/1393 train_time:136335ms step_avg:119.07ms
step:1156/1393 train_time:136460ms step_avg:119.07ms
step:1157/1393 train_time:136590ms step_avg:119.08ms
step:1158/1393 train_time:136714ms step_avg:119.09ms
step:1159/1393 train_time:136841ms step_avg:119.10ms
step:1160/1393 train_time:136970ms step_avg:119.10ms
step:1161/1393 train_time:137094ms step_avg:119.11ms
step:1162/1393 train_time:137219ms step_avg:119.11ms
step:1163/1393 train_time:137346ms step_avg:119.12ms
step:1164/1393 train_time:137470ms step_avg:119.13ms
step:1165/1393 train_time:137597ms step_avg:119.13ms
step:1166/1393 train_time:137725ms step_avg:119.14ms
step:1167/1393 train_time:137852ms step_avg:119.15ms
step:1168/1393 train_time:137978ms step_avg:119.15ms
step:1169/1393 train_time:138104ms step_avg:119.16ms
step:1170/1393 train_time:138234ms step_avg:119.17ms
step:1171/1393 train_time:138360ms step_avg:119.17ms
step:1172/1393 train_time:138485ms step_avg:119.18ms
step:1173/1393 train_time:138610ms step_avg:119.18ms
step:1174/1393 train_time:138736ms step_avg:119.19ms
step:1175/1393 train_time:138860ms step_avg:119.19ms
step:1176/1393 train_time:138986ms step_avg:119.20ms
step:1177/1393 train_time:139112ms step_avg:119.20ms
step:1178/1393 train_time:139237ms step_avg:119.21ms
step:1179/1393 train_time:139364ms step_avg:119.22ms
step:1180/1393 train_time:139490ms step_avg:119.22ms
step:1181/1393 train_time:139616ms step_avg:119.23ms
step:1182/1393 train_time:139741ms step_avg:119.23ms
step:1183/1393 train_time:139865ms step_avg:119.24ms
step:1184/1393 train_time:139991ms step_avg:119.24ms
step:1185/1393 train_time:140122ms step_avg:119.25ms
step:1186/1393 train_time:140248ms step_avg:119.26ms
step:1187/1393 train_time:140373ms step_avg:119.26ms
step:1188/1393 train_time:140498ms step_avg:119.27ms
step:1189/1393 train_time:140623ms step_avg:119.27ms
step:1190/1393 train_time:140747ms step_avg:119.28ms
step:1191/1393 train_time:140873ms step_avg:119.28ms
step:1192/1393 train_time:141000ms step_avg:119.29ms
step:1193/1393 train_time:141127ms step_avg:119.30ms
step:1194/1393 train_time:141252ms step_avg:119.30ms
step:1195/1393 train_time:141377ms step_avg:119.31ms
step:1196/1393 train_time:141504ms step_avg:119.31ms
step:1197/1393 train_time:141634ms step_avg:119.32ms
step:1198/1393 train_time:141757ms step_avg:119.32ms
step:1199/1393 train_time:141884ms step_avg:119.33ms
step:1200/1393 train_time:142008ms step_avg:119.33ms
step:1201/1393 train_time:142132ms step_avg:119.34ms
step:1202/1393 train_time:142258ms step_avg:119.34ms
step:1203/1393 train_time:142383ms step_avg:119.35ms
step:1204/1393 train_time:142508ms step_avg:119.35ms
step:1205/1393 train_time:142634ms step_avg:119.36ms
step:1206/1393 train_time:142759ms step_avg:119.36ms
step:1207/1393 train_time:142886ms step_avg:119.37ms
step:1208/1393 train_time:143012ms step_avg:119.38ms
step:1209/1393 train_time:143137ms step_avg:119.38ms
step:1210/1393 train_time:143262ms step_avg:119.38ms
step:1211/1393 train_time:143387ms step_avg:119.39ms
step:1212/1393 train_time:143515ms step_avg:119.40ms
step:1213/1393 train_time:143639ms step_avg:119.40ms
step:1214/1393 train_time:143766ms step_avg:119.41ms
step:1215/1393 train_time:143891ms step_avg:119.41ms
step:1216/1393 train_time:144015ms step_avg:119.42ms
step:1217/1393 train_time:144140ms step_avg:119.42ms
step:1218/1393 train_time:144265ms step_avg:119.42ms
step:1219/1393 train_time:144390ms step_avg:119.43ms
step:1220/1393 train_time:144520ms step_avg:119.44ms
step:1221/1393 train_time:144645ms step_avg:119.44ms
step:1222/1393 train_time:144772ms step_avg:119.45ms
step:1223/1393 train_time:144898ms step_avg:119.45ms
step:1224/1393 train_time:145024ms step_avg:119.46ms
step:1225/1393 train_time:145149ms step_avg:119.46ms
step:1226/1393 train_time:145274ms step_avg:119.47ms
step:1227/1393 train_time:145400ms step_avg:119.47ms
step:1228/1393 train_time:145525ms step_avg:119.48ms
step:1229/1393 train_time:145650ms step_avg:119.48ms
step:1230/1393 train_time:145774ms step_avg:119.49ms
step:1231/1393 train_time:145898ms step_avg:119.49ms
step:1232/1393 train_time:146023ms step_avg:119.50ms
step:1233/1393 train_time:146150ms step_avg:119.50ms
step:1234/1393 train_time:146275ms step_avg:119.51ms
step:1235/1393 train_time:146399ms step_avg:119.51ms
step:1236/1393 train_time:146528ms step_avg:119.52ms
step:1237/1393 train_time:146652ms step_avg:119.52ms
step:1238/1393 train_time:146778ms step_avg:119.53ms
step:1239/1393 train_time:146905ms step_avg:119.53ms
step:1240/1393 train_time:147034ms step_avg:119.54ms
step:1241/1393 train_time:147158ms step_avg:119.54ms
step:1242/1393 train_time:147284ms step_avg:119.55ms
step:1243/1393 train_time:147412ms step_avg:119.56ms
step:1244/1393 train_time:147541ms step_avg:119.56ms
step:1245/1393 train_time:147666ms step_avg:119.57ms
step:1246/1393 train_time:147790ms step_avg:119.57ms
step:1247/1393 train_time:147915ms step_avg:119.58ms
step:1248/1393 train_time:148041ms step_avg:119.58ms
step:1249/1393 train_time:148166ms step_avg:119.59ms
step:1250/1393 train_time:148292ms step_avg:119.59ms
step:1250/1393 val_loss:3.3138 train_time:148419ms step_avg:119.69ms
step:1251/1393 train_time:148442ms step_avg:119.61ms
step:1252/1393 train_time:148551ms step_avg:119.61ms
step:1253/1393 train_time:148682ms step_avg:119.62ms
step:1254/1393 train_time:148808ms step_avg:119.62ms
step:1255/1393 train_time:148933ms step_avg:119.62ms
step:1256/1393 train_time:149058ms step_avg:119.63ms
step:1257/1393 train_time:149183ms step_avg:119.63ms
step:1258/1393 train_time:149308ms step_avg:119.64ms
step:1259/1393 train_time:149433ms step_avg:119.64ms
step:1260/1393 train_time:149557ms step_avg:119.65ms
step:1261/1393 train_time:149687ms step_avg:119.65ms
step:1262/1393 train_time:149813ms step_avg:119.66ms
step:1263/1393 train_time:149937ms step_avg:119.66ms
step:1264/1393 train_time:150062ms step_avg:119.67ms
step:1265/1393 train_time:150188ms step_avg:119.67ms
step:1266/1393 train_time:150312ms step_avg:119.68ms
step:1267/1393 train_time:150439ms step_avg:119.68ms
step:1268/1393 train_time:150566ms step_avg:119.69ms
step:1269/1393 train_time:150691ms step_avg:119.69ms
step:1270/1393 train_time:150817ms step_avg:119.70ms
step:1271/1393 train_time:150946ms step_avg:119.70ms
step:1272/1393 train_time:151073ms step_avg:119.71ms
step:1273/1393 train_time:151198ms step_avg:119.71ms
step:1274/1393 train_time:151326ms step_avg:119.72ms
step:1275/1393 train_time:151452ms step_avg:119.73ms
step:1276/1393 train_time:151577ms step_avg:119.73ms
step:1277/1393 train_time:151703ms step_avg:119.73ms
step:1278/1393 train_time:151830ms step_avg:119.74ms
step:1279/1393 train_time:151956ms step_avg:119.74ms
step:1280/1393 train_time:152081ms step_avg:119.75ms
step:1281/1393 train_time:152205ms step_avg:119.75ms
step:1282/1393 train_time:152331ms step_avg:119.76ms
step:1283/1393 train_time:152457ms step_avg:119.76ms
step:1284/1393 train_time:152584ms step_avg:119.77ms
step:1285/1393 train_time:152710ms step_avg:119.77ms
step:1286/1393 train_time:152834ms step_avg:119.78ms
step:1287/1393 train_time:152961ms step_avg:119.78ms
step:1288/1393 train_time:153085ms step_avg:119.79ms
step:1289/1393 train_time:153211ms step_avg:119.79ms
step:1290/1393 train_time:153336ms step_avg:119.79ms
step:1291/1393 train_time:153461ms step_avg:119.80ms
step:1292/1393 train_time:153585ms step_avg:119.80ms
step:1293/1393 train_time:153710ms step_avg:119.81ms
step:1294/1393 train_time:153836ms step_avg:119.81ms
step:1295/1393 train_time:153963ms step_avg:119.82ms
step:1296/1393 train_time:154087ms step_avg:119.82ms
step:1297/1393 train_time:154213ms step_avg:119.82ms
step:1298/1393 train_time:154338ms step_avg:119.83ms
step:1299/1393 train_time:154465ms step_avg:119.83ms
step:1300/1393 train_time:154591ms step_avg:119.84ms
step:1301/1393 train_time:154716ms step_avg:119.84ms
step:1302/1393 train_time:154840ms step_avg:119.85ms
step:1303/1393 train_time:154969ms step_avg:119.85ms
step:1304/1393 train_time:155094ms step_avg:119.86ms
step:1305/1393 train_time:155219ms step_avg:119.86ms
step:1306/1393 train_time:155345ms step_avg:119.86ms
step:1307/1393 train_time:155471ms step_avg:119.87ms
step:1308/1393 train_time:155596ms step_avg:119.87ms
step:1309/1393 train_time:155726ms step_avg:119.88ms
step:1310/1393 train_time:155852ms step_avg:119.89ms
step:1311/1393 train_time:155978ms step_avg:119.89ms
step:1312/1393 train_time:156105ms step_avg:119.90ms
step:1313/1393 train_time:156232ms step_avg:119.90ms
step:1314/1393 train_time:156358ms step_avg:119.91ms
step:1315/1393 train_time:156484ms step_avg:119.91ms
step:1316/1393 train_time:156608ms step_avg:119.91ms
step:1317/1393 train_time:156736ms step_avg:119.92ms
step:1318/1393 train_time:156863ms step_avg:119.93ms
step:1319/1393 train_time:156987ms step_avg:119.93ms
step:1320/1393 train_time:157113ms step_avg:119.93ms
step:1321/1393 train_time:157239ms step_avg:119.94ms
step:1322/1393 train_time:157363ms step_avg:119.94ms
step:1323/1393 train_time:157488ms step_avg:119.95ms
step:1324/1393 train_time:157614ms step_avg:119.95ms
step:1325/1393 train_time:157738ms step_avg:119.95ms
step:1326/1393 train_time:157865ms step_avg:119.96ms
step:1327/1393 train_time:157990ms step_avg:119.96ms
step:1328/1393 train_time:158116ms step_avg:119.97ms
step:1329/1393 train_time:158241ms step_avg:119.97ms
step:1330/1393 train_time:158366ms step_avg:119.97ms
step:1331/1393 train_time:158491ms step_avg:119.98ms
step:1332/1393 train_time:158617ms step_avg:119.98ms
step:1333/1393 train_time:158744ms step_avg:119.99ms
step:1334/1393 train_time:158870ms step_avg:119.99ms
step:1335/1393 train_time:158995ms step_avg:120.00ms
step:1336/1393 train_time:159119ms step_avg:120.00ms
step:1337/1393 train_time:159245ms step_avg:120.00ms
step:1338/1393 train_time:159372ms step_avg:120.01ms
step:1339/1393 train_time:159497ms step_avg:120.01ms
step:1340/1393 train_time:159623ms step_avg:120.02ms
step:1341/1393 train_time:159750ms step_avg:120.02ms
step:1342/1393 train_time:159877ms step_avg:120.03ms
step:1343/1393 train_time:160004ms step_avg:120.03ms
step:1344/1393 train_time:160131ms step_avg:120.04ms
step:1345/1393 train_time:160255ms step_avg:120.04ms
step:1346/1393 train_time:160382ms step_avg:120.05ms
step:1347/1393 train_time:160509ms step_avg:120.05ms
step:1348/1393 train_time:160635ms step_avg:120.06ms
step:1349/1393 train_time:160767ms step_avg:120.07ms
step:1350/1393 train_time:160894ms step_avg:120.07ms
step:1351/1393 train_time:161021ms step_avg:120.08ms
step:1352/1393 train_time:161149ms step_avg:120.08ms
step:1353/1393 train_time:161276ms step_avg:120.09ms
step:1354/1393 train_time:161406ms step_avg:120.09ms
step:1355/1393 train_time:161532ms step_avg:120.10ms
step:1356/1393 train_time:161658ms step_avg:120.10ms
step:1357/1393 train_time:161785ms step_avg:120.11ms
step:1358/1393 train_time:161912ms step_avg:120.11ms
step:1359/1393 train_time:162041ms step_avg:120.12ms
step:1360/1393 train_time:162172ms step_avg:120.13ms
step:1361/1393 train_time:162302ms step_avg:120.13ms
step:1362/1393 train_time:162430ms step_avg:120.14ms
step:1363/1393 train_time:162556ms step_avg:120.15ms
step:1364/1393 train_time:162687ms step_avg:120.15ms
step:1365/1393 train_time:162813ms step_avg:120.16ms
step:1366/1393 train_time:162939ms step_avg:120.16ms
step:1367/1393 train_time:163066ms step_avg:120.17ms
step:1368/1393 train_time:163192ms step_avg:120.17ms
step:1369/1393 train_time:163317ms step_avg:120.17ms
step:1370/1393 train_time:163444ms step_avg:120.18ms
step:1371/1393 train_time:163570ms step_avg:120.18ms
step:1372/1393 train_time:163697ms step_avg:120.19ms
step:1373/1393 train_time:163822ms step_avg:120.19ms
step:1374/1393 train_time:163949ms step_avg:120.20ms
step:1375/1393 train_time:164074ms step_avg:120.20ms
step:1375/1393 val_loss:3.2803 train_time:164201ms step_avg:120.29ms
step:1376/1393 train_time:164223ms step_avg:120.22ms
step:1377/1393 train_time:164333ms step_avg:120.21ms
step:1378/1393 train_time:164465ms step_avg:120.22ms
step:1379/1393 train_time:164594ms step_avg:120.23ms
step:1380/1393 train_time:164719ms step_avg:120.23ms
step:1381/1393 train_time:164846ms step_avg:120.24ms
step:1382/1393 train_time:164973ms step_avg:120.24ms
step:1383/1393 train_time:165101ms step_avg:120.25ms
step:1384/1393 train_time:165226ms step_avg:120.25ms
step:1385/1393 train_time:165353ms step_avg:120.26ms
step:1386/1393 train_time:165479ms step_avg:120.26ms
step:1387/1393 train_time:165604ms step_avg:120.26ms
step:1388/1393 train_time:165736ms step_avg:120.27ms
step:1389/1393 train_time:165862ms step_avg:120.28ms
step:1390/1393 train_time:165988ms step_avg:120.28ms
step:1391/1393 train_time:166114ms step_avg:120.29ms
step:1392/1393 train_time:166240ms step_avg:120.29ms
step:1393/1393 train_time:166366ms step_avg:120.29ms
step:1393/1393 val_loss:3.2767 train_time:166492ms step_avg:120.38ms
peak memory allocated: 31573 MiB reserved: 32976 MiB
